{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/Cellar/python/3.6.4_4/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from autoencoder import process_image, process_image_deeply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input-layer (InputLayer)     (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "hidden-layer (Dense)         (None, 150)               45150     \n",
      "_________________________________________________________________\n",
      "output-layer (Dense)         (None, 300)               45300     \n",
      "=================================================================\n",
      "Total params: 90,450\n",
      "Trainable params: 90,450\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/500\n",
      "12800/12800 [==============================] - 1s 94us/step - loss: 0.0016 - val_loss: 7.3621e-04\n",
      "Epoch 2/500\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 5.6161e-04 - val_loss: 4.2837e-04\n",
      "Test MSE: 0.00043282026\n",
      "Total MSE: 0.00043035515\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input-layer (InputLayer)     (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "hidden-layer-1 (Dense)       (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "hidden-layer-2 (Dense)       (None, 150)               45150     \n",
      "_________________________________________________________________\n",
      "hidden-layer-3 (Dense)       (None, 300)               45300     \n",
      "_________________________________________________________________\n",
      "output-layer (Dense)         (None, 300)               90300     \n",
      "=================================================================\n",
      "Total params: 271,050\n",
      "Trainable params: 271,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/500\n",
      "12800/12800 [==============================] - 3s 224us/step - loss: 0.0071 - val_loss: 0.0018\n",
      "Epoch 2/500\n",
      "12800/12800 [==============================] - 2s 160us/step - loss: 0.0016 - val_loss: 0.0013\n",
      "Epoch 3/500\n",
      "12800/12800 [==============================] - 2s 163us/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 4/500\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.0010 - val_loss: 9.3292e-04\n",
      "Epoch 5/500\n",
      "12800/12800 [==============================] - 3s 196us/step - loss: 8.8894e-04 - val_loss: 8.1633e-04\n",
      "Epoch 6/500\n",
      "12800/12800 [==============================] - 2s 166us/step - loss: 7.8432e-04 - val_loss: 7.2701e-04\n",
      "Epoch 7/500\n",
      "12800/12800 [==============================] - 2s 164us/step - loss: 7.0230e-04 - val_loss: 6.5656e-04\n",
      "Epoch 8/500\n",
      "12800/12800 [==============================] - 2s 190us/step - loss: 6.3589e-04 - val_loss: 5.9684e-04\n",
      "Epoch 9/500\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 5.8075e-04 - val_loss: 5.4793e-04\n",
      "Epoch 10/500\n",
      "12800/12800 [==============================] - 3s 195us/step - loss: 5.3398e-04 - val_loss: 5.0609e-04\n",
      "Epoch 11/500\n",
      "12800/12800 [==============================] - 2s 188us/step - loss: 4.9390e-04 - val_loss: 4.6995e-04\n",
      "Test MSE: 0.00047306914\n",
      "Total MSE: 0.00047374726\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input-layer (InputLayer)     (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "hidden-layer (Dense)         (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "output-layer (Dense)         (None, 300)               30300     \n",
      "=================================================================\n",
      "Total params: 60,400\n",
      "Trainable params: 60,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/500\n",
      "12800/12800 [==============================] - 1s 108us/step - loss: 0.0016 - val_loss: 8.4900e-04\n",
      "Epoch 2/500\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 6.3978e-04 - val_loss: 5.3050e-04\n",
      "Epoch 3/500\n",
      "12800/12800 [==============================] - 1s 72us/step - loss: 4.3371e-04 - val_loss: 3.9249e-04\n",
      "Test MSE: 0.00037731911\n",
      "Total MSE: 0.00037523699\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input-layer (InputLayer)     (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "hidden-layer-1 (Dense)       (None, 200)               60200     \n",
      "_________________________________________________________________\n",
      "hidden-layer-2 (Dense)       (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "hidden-layer-3 (Dense)       (None, 200)               20200     \n",
      "_________________________________________________________________\n",
      "output-layer (Dense)         (None, 300)               60300     \n",
      "=================================================================\n",
      "Total params: 160,800\n",
      "Trainable params: 160,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/500\n",
      "12800/12800 [==============================] - 2s 182us/step - loss: 0.0040 - val_loss: 0.0016\n",
      "Epoch 2/500\n",
      "12800/12800 [==============================] - 2s 142us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 3/500\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 0.0011 - val_loss: 9.6907e-04\n",
      "Epoch 4/500\n",
      "12800/12800 [==============================] - 2s 161us/step - loss: 8.9187e-04 - val_loss: 8.2852e-04\n",
      "Epoch 5/500\n",
      "12800/12800 [==============================] - 2s 181us/step - loss: 7.7239e-04 - val_loss: 7.2791e-04\n",
      "Epoch 6/500\n",
      "12800/12800 [==============================] - 2s 156us/step - loss: 6.8226e-04 - val_loss: 6.4910e-04\n",
      "Epoch 7/500\n",
      "12800/12800 [==============================] - 2s 144us/step - loss: 6.1139e-04 - val_loss: 5.8634e-04\n",
      "Epoch 8/500\n",
      "12800/12800 [==============================] - 2s 158us/step - loss: 5.5379e-04 - val_loss: 5.3534e-04\n",
      "Epoch 9/500\n",
      "12800/12800 [==============================] - 2s 146us/step - loss: 5.0601e-04 - val_loss: 4.9198e-04\n",
      "Test MSE: 0.00048926658\n",
      "Total MSE: 0.00048604934\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input-layer (InputLayer)     (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "hidden-layer (Dense)         (None, 75)                22575     \n",
      "_________________________________________________________________\n",
      "output-layer (Dense)         (None, 300)               22800     \n",
      "=================================================================\n",
      "Total params: 45,375\n",
      "Trainable params: 45,375\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/500\n",
      "12800/12800 [==============================] - 1s 117us/step - loss: 0.0017 - val_loss: 9.5361e-04\n",
      "Epoch 2/500\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 7.7949e-04 - val_loss: 6.3853e-04\n",
      "Epoch 3/500\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 5.5915e-04 - val_loss: 4.8581e-04\n",
      "Test MSE: 0.0004925017\n",
      "Total MSE: 0.00048754052\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input-layer (InputLayer)     (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "hidden-layer-1 (Dense)       (None, 150)               45150     \n",
      "_________________________________________________________________\n",
      "hidden-layer-2 (Dense)       (None, 75)                11325     \n",
      "_________________________________________________________________\n",
      "hidden-layer-3 (Dense)       (None, 150)               11400     \n",
      "_________________________________________________________________\n",
      "output-layer (Dense)         (None, 300)               45300     \n",
      "=================================================================\n",
      "Total params: 113,175\n",
      "Trainable params: 113,175\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/500\n",
      "12800/12800 [==============================] - 2s 140us/step - loss: 0.0025 - val_loss: 0.0013\n",
      "Epoch 2/500\n",
      "12800/12800 [==============================] - 1s 109us/step - loss: 0.0011 - val_loss: 9.4183e-04\n",
      "Epoch 3/500\n",
      "12800/12800 [==============================] - 1s 105us/step - loss: 8.3096e-04 - val_loss: 7.5339e-04\n",
      "Epoch 4/500\n",
      "12800/12800 [==============================] - 2s 124us/step - loss: 6.8161e-04 - val_loss: 6.3207e-04\n",
      "Epoch 5/500\n",
      "12800/12800 [==============================] - 2s 132us/step - loss: 5.8022e-04 - val_loss: 5.4560e-04\n",
      "Epoch 6/500\n",
      "12800/12800 [==============================] - 2s 120us/step - loss: 5.0654e-04 - val_loss: 4.8247e-04\n",
      "Test MSE: 0.00048421855\n",
      "Total MSE: 0.00047824575\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input-layer (InputLayer)     (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "hidden-layer (Dense)         (None, 60)                18060     \n",
      "_________________________________________________________________\n",
      "output-layer (Dense)         (None, 300)               18300     \n",
      "=================================================================\n",
      "Total params: 36,360\n",
      "Trainable params: 36,360\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/500\n",
      "12800/12800 [==============================] - 1s 112us/step - loss: 0.0016 - val_loss: 9.3820e-04\n",
      "Epoch 2/500\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 7.8889e-04 - val_loss: 6.5385e-04\n",
      "Epoch 3/500\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 5.8758e-04 - val_loss: 5.1892e-04\n",
      "Epoch 4/500\n",
      "12800/12800 [==============================] - 1s 72us/step - loss: 4.7916e-04 - val_loss: 4.3439e-04\n",
      "Test MSE: 0.00044061111\n",
      "Total MSE: 0.0004372504\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input-layer (InputLayer)     (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "hidden-layer-1 (Dense)       (None, 120)               36120     \n",
      "_________________________________________________________________\n",
      "hidden-layer-2 (Dense)       (None, 60)                7260      \n",
      "_________________________________________________________________\n",
      "hidden-layer-3 (Dense)       (None, 120)               7320      \n",
      "_________________________________________________________________\n",
      "output-layer (Dense)         (None, 300)               36300     \n",
      "=================================================================\n",
      "Total params: 87,000\n",
      "Trainable params: 87,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/500\n",
      "12800/12800 [==============================] - 2s 147us/step - loss: 0.0033 - val_loss: 0.0016\n",
      "Epoch 2/500\n",
      "12800/12800 [==============================] - 2s 117us/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 3/500\n",
      "12800/12800 [==============================] - 1s 104us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 4/500\n",
      "12800/12800 [==============================] - 1s 106us/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 5/500\n",
      "12800/12800 [==============================] - 1s 100us/step - loss: 9.9207e-04 - val_loss: 9.3834e-04\n",
      "Epoch 6/500\n",
      "12800/12800 [==============================] - 1s 97us/step - loss: 9.0438e-04 - val_loss: 8.6039e-04\n",
      "Epoch 7/500\n",
      "12800/12800 [==============================] - 1s 103us/step - loss: 8.3296e-04 - val_loss: 7.9584e-04\n",
      "Epoch 8/500\n",
      "12800/12800 [==============================] - 1s 112us/step - loss: 7.7326e-04 - val_loss: 7.4194e-04\n",
      "Epoch 9/500\n",
      "12800/12800 [==============================] - 1s 101us/step - loss: 7.2283e-04 - val_loss: 6.9517e-04\n",
      "Epoch 10/500\n",
      "12800/12800 [==============================] - 1s 102us/step - loss: 6.7945e-04 - val_loss: 6.5635e-04\n",
      "Epoch 11/500\n",
      "12800/12800 [==============================] - 1s 103us/step - loss: 6.4165e-04 - val_loss: 6.2137e-04\n",
      "Epoch 12/500\n",
      "12800/12800 [==============================] - 2s 137us/step - loss: 6.0849e-04 - val_loss: 5.9003e-04\n",
      "Epoch 13/500\n",
      "12800/12800 [==============================] - 2s 122us/step - loss: 5.7895e-04 - val_loss: 5.6295e-04\n",
      "Epoch 14/500\n",
      "12800/12800 [==============================] - 2s 133us/step - loss: 5.5253e-04 - val_loss: 5.3812e-04\n",
      "Epoch 15/500\n",
      "12800/12800 [==============================] - 1s 104us/step - loss: 5.2870e-04 - val_loss: 5.1589e-04\n",
      "Epoch 16/500\n",
      "12800/12800 [==============================] - 1s 110us/step - loss: 5.0707e-04 - val_loss: 4.9575e-04\n",
      "Test MSE: 0.00049247965\n",
      "Total MSE: 0.00049503882\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input-layer (InputLayer)     (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "hidden-layer (Dense)         (None, 50)                15050     \n",
      "_________________________________________________________________\n",
      "output-layer (Dense)         (None, 300)               15300     \n",
      "=================================================================\n",
      "Total params: 30,350\n",
      "Trainable params: 30,350\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/500\n",
      "12800/12800 [==============================] - 2s 123us/step - loss: 0.0017 - val_loss: 0.0011\n",
      "Epoch 2/500\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 9.1775e-04 - val_loss: 7.7394e-04- lo\n",
      "Epoch 3/500\n",
      "12800/12800 [==============================] - 1s 72us/step - loss: 7.1094e-04 - val_loss: 6.3234e-04\n",
      "Epoch 4/500\n",
      "12800/12800 [==============================] - 1s 95us/step - loss: 5.9489e-04 - val_loss: 5.4475e-04\n",
      "Epoch 5/500\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 5.1755e-04 - val_loss: 4.8010e-04\n",
      "Test MSE: 0.00049300585\n",
      "Total MSE: 0.00048553368\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input-layer (InputLayer)     (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "hidden-layer-1 (Dense)       (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "hidden-layer-2 (Dense)       (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "hidden-layer-3 (Dense)       (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "output-layer (Dense)         (None, 300)               30300     \n",
      "=================================================================\n",
      "Total params: 70,550\n",
      "Trainable params: 70,550\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/500\n",
      "12800/12800 [==============================] - 2s 148us/step - loss: 0.0024 - val_loss: 0.0014\n",
      "Epoch 2/500\n",
      "12800/12800 [==============================] - 1s 113us/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 3/500\n",
      "12800/12800 [==============================] - 2s 132us/step - loss: 9.6243e-04 - val_loss: 8.9262e-04\n",
      "Epoch 4/500\n",
      "12800/12800 [==============================] - 2s 139us/step - loss: 8.2117e-04 - val_loss: 7.7801e-04\n",
      "Epoch 5/500\n",
      "12800/12800 [==============================] - 1s 110us/step - loss: 7.2385e-04 - val_loss: 6.9647e-04\n",
      "Epoch 6/500\n",
      "12800/12800 [==============================] - 1s 94us/step - loss: 6.5258e-04 - val_loss: 6.3419e-04\n",
      "Epoch 7/500\n",
      "12800/12800 [==============================] - 1s 110us/step - loss: 5.9751e-04 - val_loss: 5.8519e-04\n",
      "Epoch 8/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 1s 100us/step - loss: 5.5342e-04 - val_loss: 5.4504e-04\n",
      "Epoch 9/500\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 5.1694e-04 - val_loss: 5.1173e-04\n",
      "Epoch 10/500\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 4.8654e-04 - val_loss: 4.8285e-04\n",
      "Test MSE: 0.00047669721\n",
      "Total MSE: 0.0004742605\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input-layer (InputLayer)     (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "hidden-layer (Dense)         (None, 42)                12642     \n",
      "_________________________________________________________________\n",
      "output-layer (Dense)         (None, 300)               12900     \n",
      "=================================================================\n",
      "Total params: 25,542\n",
      "Trainable params: 25,542\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/500\n",
      "12800/12800 [==============================] - 2s 121us/step - loss: 0.0017 - val_loss: 0.0011\n",
      "Epoch 2/500\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 9.5136e-04 - val_loss: 8.5314e-04\n",
      "Epoch 3/500\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 7.6395e-04 - val_loss: 7.2285e-04\n",
      "Epoch 4/500\n",
      "12800/12800 [==============================] - 1s 111us/step - loss: 6.6431e-04 - val_loss: 6.4599e-04\n",
      "Epoch 5/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 6.0104e-04 - val_loss: 5.9258e-04\n",
      "Epoch 6/500\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 5.5492e-04 - val_loss: 5.5178e-04\n",
      "Epoch 7/500\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 5.1849e-04 - val_loss: 5.1898e-04\n",
      "Epoch 8/500\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 4.8882e-04 - val_loss: 4.9131e-04\n",
      "Test MSE: 0.00048663321\n",
      "Total MSE: 0.00047930828\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input-layer (InputLayer)     (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "hidden-layer-1 (Dense)       (None, 84)                25284     \n",
      "_________________________________________________________________\n",
      "hidden-layer-2 (Dense)       (None, 42)                3570      \n",
      "_________________________________________________________________\n",
      "hidden-layer-3 (Dense)       (None, 84)                3612      \n",
      "_________________________________________________________________\n",
      "output-layer (Dense)         (None, 300)               25500     \n",
      "=================================================================\n",
      "Total params: 57,966\n",
      "Trainable params: 57,966\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/500\n",
      "12800/12800 [==============================] - 2s 151us/step - loss: 0.0024 - val_loss: 0.0014\n",
      "Epoch 2/500\n",
      "12800/12800 [==============================] - 1s 100us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 3/500\n",
      "12800/12800 [==============================] - 1s 93us/step - loss: 0.0011 - val_loss: 9.9871e-04\n",
      "Epoch 4/500\n",
      "12800/12800 [==============================] - 1s 95us/step - loss: 9.2337e-04 - val_loss: 8.8448e-04\n",
      "Epoch 5/500\n",
      "12800/12800 [==============================] - 1s 106us/step - loss: 8.2607e-04 - val_loss: 7.9940e-04\n",
      "Epoch 6/500\n",
      "12800/12800 [==============================] - 1s 101us/step - loss: 7.5159e-04 - val_loss: 7.3353e-04\n",
      "Epoch 7/500\n",
      "12800/12800 [==============================] - 1s 93us/step - loss: 6.9328e-04 - val_loss: 6.8161e-04\n",
      "Epoch 8/500\n",
      "12800/12800 [==============================] - 1s 88us/step - loss: 6.4631e-04 - val_loss: 6.4042e-04\n",
      "Epoch 9/500\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 6.0760e-04 - val_loss: 6.0560e-04\n",
      "Epoch 10/500\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 5.7527e-04 - val_loss: 5.7506e-04\n",
      "Epoch 11/500\n",
      "12800/12800 [==============================] - 1s 96us/step - loss: 5.4745e-04 - val_loss: 5.4948e-04\n",
      "Epoch 12/500\n",
      "12800/12800 [==============================] - 1s 96us/step - loss: 5.2355e-04 - val_loss: 5.2701e-04\n",
      "Epoch 13/500\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 5.0257e-04 - val_loss: 5.0749e-04\n",
      "Epoch 14/500\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 4.8419e-04 - val_loss: 4.9013e-04\n",
      "Test MSE: 0.00047736499\n",
      "Total MSE: 0.00047759058\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input-layer (InputLayer)     (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "hidden-layer (Dense)         (None, 37)                11137     \n",
      "_________________________________________________________________\n",
      "output-layer (Dense)         (None, 300)               11400     \n",
      "=================================================================\n",
      "Total params: 22,537\n",
      "Trainable params: 22,537\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/500\n",
      "12800/12800 [==============================] - 2s 139us/step - loss: 0.0019 - val_loss: 0.0013\n",
      "Epoch 2/500\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 0.0011 - val_loss: 9.8116e-04\n",
      "Epoch 3/500\n",
      "12800/12800 [==============================] - 1s 96us/step - loss: 8.8960e-04 - val_loss: 8.3123e-04\n",
      "Epoch 4/500\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 7.7155e-04 - val_loss: 7.3874e-04\n",
      "Epoch 5/500\n",
      "12800/12800 [==============================] - 1s 72us/step - loss: 6.9415e-04 - val_loss: 6.7309e-04\n",
      "Epoch 6/500\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 6.3920e-04 - val_loss: 6.2618e-04\n",
      "Epoch 7/500\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 5.9801e-04 - val_loss: 5.9018e-04\n",
      "Epoch 8/500\n",
      "12800/12800 [==============================] - 1s 97us/step - loss: 5.6579e-04 - val_loss: 5.6097e-04\n",
      "Epoch 9/500\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 5.3998e-04 - val_loss: 5.3792e-04\n",
      "Epoch 10/500\n",
      "12800/12800 [==============================] - 1s 69us/step - loss: 5.1889e-04 - val_loss: 5.1941e-04\n",
      "Epoch 11/500\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 5.0147e-04 - val_loss: 5.0236e-04\n",
      "Epoch 12/500\n",
      "12800/12800 [==============================] - 1s 68us/step - loss: 4.8668e-04 - val_loss: 4.8900e-04\n",
      "Test MSE: 0.00050627923\n",
      "Total MSE: 0.00048602237\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input-layer (InputLayer)     (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "hidden-layer-1 (Dense)       (None, 74)                22274     \n",
      "_________________________________________________________________\n",
      "hidden-layer-2 (Dense)       (None, 37)                2775      \n",
      "_________________________________________________________________\n",
      "hidden-layer-3 (Dense)       (None, 74)                2812      \n",
      "_________________________________________________________________\n",
      "output-layer (Dense)         (None, 300)               22500     \n",
      "=================================================================\n",
      "Total params: 50,361\n",
      "Trainable params: 50,361\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/500\n",
      "12800/12800 [==============================] - 2s 158us/step - loss: 0.0023 - val_loss: 0.0014\n",
      "Epoch 2/500\n",
      "12800/12800 [==============================] - 1s 108us/step - loss: 0.0013 - val_loss: 0.0011\n",
      "Epoch 3/500\n",
      "12800/12800 [==============================] - 1s 98us/step - loss: 0.0011 - val_loss: 9.7891e-04\n",
      "Epoch 4/500\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 9.3671e-04 - val_loss: 8.7968e-04\n",
      "Epoch 5/500\n",
      "12800/12800 [==============================] - 1s 116us/step - loss: 8.5037e-04 - val_loss: 8.0815e-04\n",
      "Epoch 6/500\n",
      "12800/12800 [==============================] - 1s 112us/step - loss: 7.8650e-04 - val_loss: 7.5513e-04\n",
      "Epoch 7/500\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 7.3625e-04 - val_loss: 7.1114e-04\n",
      "Epoch 8/500\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 6.9496e-04 - val_loss: 6.7436e-04\n",
      "Epoch 9/500\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 6.6066e-04 - val_loss: 6.4517e-04\n",
      "Epoch 10/500\n",
      "12800/12800 [==============================] - 1s 64us/step - loss: 6.3181e-04 - val_loss: 6.1812e-04\n",
      "Epoch 11/500\n",
      "12800/12800 [==============================] - 1s 64us/step - loss: 6.0742e-04 - val_loss: 5.9686e-04\n",
      "Epoch 12/500\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 5.8669e-04 - val_loss: 5.7768e-04\n",
      "Epoch 13/500\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 5.6891e-04 - val_loss: 5.6103e-04\n",
      "Epoch 14/500\n",
      "12800/12800 [==============================] - 1s 64us/step - loss: 5.5344e-04 - val_loss: 5.4673e-04\n",
      "Epoch 15/500\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 5.3975e-04 - val_loss: 5.3498e-04\n",
      "Epoch 16/500\n",
      "12800/12800 [==============================] - 2s 123us/step - loss: 5.2759e-04 - val_loss: 5.2277e-04\n",
      "Epoch 17/500\n",
      "12800/12800 [==============================] - 1s 92us/step - loss: 5.1653e-04 - val_loss: 5.1259e-04\n",
      "Epoch 18/500\n",
      "12800/12800 [==============================] - 1s 93us/step - loss: 5.0656e-04 - val_loss: 5.0319e-04\n",
      "Epoch 19/500\n",
      "12800/12800 [==============================] - 1s 95us/step - loss: 4.9748e-04 - val_loss: 4.9481e-04\n",
      "Test MSE: 0.00049986969\n",
      "Total MSE: 0.00049413458\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input-layer (InputLayer)     (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "hidden-layer (Dense)         (None, 33)                9933      \n",
      "_________________________________________________________________\n",
      "output-layer (Dense)         (None, 300)               10200     \n",
      "=================================================================\n",
      "Total params: 20,133\n",
      "Trainable params: 20,133\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/500\n",
      "12800/12800 [==============================] - 2s 128us/step - loss: 0.0019 - val_loss: 0.0013\n",
      "Epoch 2/500\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 3/500\n",
      "12800/12800 [==============================] - 1s 69us/step - loss: 9.5038e-04 - val_loss: 9.0027e-04\n",
      "Epoch 4/500\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 8.4225e-04 - val_loss: 8.1162e-04\n",
      "Epoch 5/500\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 7.6955e-04 - val_loss: 7.5164e-04\n",
      "Epoch 6/500\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 7.1705e-04 - val_loss: 7.0584e-04\n",
      "Epoch 7/500\n",
      "12800/12800 [==============================] - 1s 72us/step - loss: 6.7668e-04 - val_loss: 6.7017e-04\n",
      "Epoch 8/500\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 6.4477e-04 - val_loss: 6.4055e-04\n",
      "Epoch 9/500\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 6.1867e-04 - val_loss: 6.1663e-04\n",
      "Epoch 10/500\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 5.9698e-04 - val_loss: 5.9689e-04\n",
      "Epoch 11/500\n",
      "12800/12800 [==============================] - 1s 69us/step - loss: 5.7875e-04 - val_loss: 5.7962e-04\n",
      "Epoch 12/500\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 5.6343e-04 - val_loss: 5.6479e-04\n",
      "Epoch 13/500\n",
      "12800/12800 [==============================] - 1s 68us/step - loss: 5.5049e-04 - val_loss: 5.5303e-04\n",
      "Epoch 14/500\n",
      "12800/12800 [==============================] - 1s 69us/step - loss: 5.3944e-04 - val_loss: 5.4275e-04\n",
      "Epoch 15/500\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 5.3027e-04 - val_loss: 5.3377e-04\n",
      "Epoch 16/500\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 5.2227e-04 - val_loss: 5.2627e-04\n",
      "Epoch 17/500\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 5.1565e-04 - val_loss: 5.1991e-04\n",
      "Epoch 18/500\n",
      "12800/12800 [==============================] - 1s 69us/step - loss: 5.0981e-04 - val_loss: 5.1469e-04\n",
      "Epoch 19/500\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 5.0483e-04 - val_loss: 5.0977e-04\n",
      "Epoch 20/500\n",
      "12800/12800 [==============================] - 1s 98us/step - loss: 5.0066e-04 - val_loss: 5.0556e-04\n",
      "Epoch 21/500\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 4.9696e-04 - val_loss: 5.0214e-04\n",
      "Epoch 22/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 4.9373e-04 - val_loss: 4.9919e-04\n",
      "Test MSE: 0.00050205357\n",
      "Total MSE: 0.00049471546\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input-layer (InputLayer)     (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "hidden-layer-1 (Dense)       (None, 66)                19866     \n",
      "_________________________________________________________________\n",
      "hidden-layer-2 (Dense)       (None, 33)                2211      \n",
      "_________________________________________________________________\n",
      "hidden-layer-3 (Dense)       (None, 66)                2244      \n",
      "_________________________________________________________________\n",
      "output-layer (Dense)         (None, 300)               20100     \n",
      "=================================================================\n",
      "Total params: 44,421\n",
      "Trainable params: 44,421\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/500\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.0026 - val_loss: 0.0017\n",
      "Epoch 2/500\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 3/500\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 4/500\n",
      "12800/12800 [==============================] - 1s 93us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 5/500\n",
      "12800/12800 [==============================] - 1s 104us/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 6/500\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 9.6625e-04 - val_loss: 9.5440e-04\n",
      "Epoch 7/500\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 9.1039e-04 - val_loss: 9.0222e-04\n",
      "Epoch 8/500\n",
      "12800/12800 [==============================] - 1s 88us/step - loss: 8.6465e-04 - val_loss: 8.6071e-04\n",
      "Epoch 9/500\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 8.2632e-04 - val_loss: 8.2425e-04\n",
      "Epoch 10/500\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 7.9298e-04 - val_loss: 7.9156e-04\n",
      "Epoch 11/500\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 7.6352e-04 - val_loss: 7.6432e-04\n",
      "Epoch 12/500\n",
      "12800/12800 [==============================] - 1s 96us/step - loss: 7.3743e-04 - val_loss: 7.3804e-04\n",
      "Epoch 13/500\n",
      "12800/12800 [==============================] - 1s 101us/step - loss: 7.1389e-04 - val_loss: 7.1494e-04\n",
      "Epoch 14/500\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 6.9300e-04 - val_loss: 6.9508e-04\n",
      "Epoch 15/500\n",
      "12800/12800 [==============================] - 1s 94us/step - loss: 6.7409e-04 - val_loss: 6.7720e-04\n",
      "Epoch 16/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 1s 82us/step - loss: 6.5702e-04 - val_loss: 6.5976e-04\n",
      "Epoch 17/500\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 6.4159e-04 - val_loss: 6.4494e-04\n",
      "Epoch 18/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 6.2780e-04 - val_loss: 6.3146e-04\n",
      "Epoch 19/500\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 6.1525e-04 - val_loss: 6.1974e-04\n",
      "Epoch 20/500\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 6.0399e-04 - val_loss: 6.0882e-04\n",
      "Epoch 21/500\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 5.9359e-04 - val_loss: 5.9811e-04\n",
      "Epoch 22/500\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 5.8418e-04 - val_loss: 5.8934e-04\n",
      "Epoch 23/500\n",
      "12800/12800 [==============================] - 1s 96us/step - loss: 5.7552e-04 - val_loss: 5.8110e-04\n",
      "Epoch 24/500\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 5.6758e-04 - val_loss: 5.7375e-04\n",
      "Epoch 25/500\n",
      "12800/12800 [==============================] - 1s 96us/step - loss: 5.6016e-04 - val_loss: 5.6647e-04\n",
      "Epoch 26/500\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 5.5322e-04 - val_loss: 5.5970e-04\n",
      "Epoch 27/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 5.4695e-04 - val_loss: 5.5384e-04\n",
      "Epoch 28/500\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 5.4104e-04 - val_loss: 5.4823e-04\n",
      "Epoch 29/500\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 5.3545e-04 - val_loss: 5.4346e-04\n",
      "Epoch 30/500\n",
      "12800/12800 [==============================] - 1s 88us/step - loss: 5.3035e-04 - val_loss: 5.3789e-04\n",
      "Epoch 31/500\n",
      "12800/12800 [==============================] - 1s 105us/step - loss: 5.2552e-04 - val_loss: 5.3356e-04\n",
      "Epoch 32/500\n",
      "12800/12800 [==============================] - 2s 141us/step - loss: 5.2105e-04 - val_loss: 5.2945e-04\n",
      "Epoch 33/500\n",
      "12800/12800 [==============================] - 1s 108us/step - loss: 5.1693e-04 - val_loss: 5.2557e-04\n",
      "Epoch 34/500\n",
      "12800/12800 [==============================] - 1s 113us/step - loss: 5.1312e-04 - val_loss: 5.2222e-04\n",
      "Epoch 35/500\n",
      "12800/12800 [==============================] - 2s 123us/step - loss: 5.0951e-04 - val_loss: 5.1842e-04\n",
      "Epoch 36/500\n",
      "12800/12800 [==============================] - 1s 98us/step - loss: 5.0620e-04 - val_loss: 5.1584e-04\n",
      "Epoch 37/500\n",
      "12800/12800 [==============================] - 1s 103us/step - loss: 5.0316e-04 - val_loss: 5.1298e-04\n",
      "Epoch 38/500\n",
      "12800/12800 [==============================] - 2s 120us/step - loss: 5.0035e-04 - val_loss: 5.1028e-04\n",
      "Epoch 39/500\n",
      "12800/12800 [==============================] - 1s 94us/step - loss: 4.9772e-04 - val_loss: 5.0797e-04\n",
      "Epoch 40/500\n",
      "12800/12800 [==============================] - 1s 97us/step - loss: 4.9536e-04 - val_loss: 5.0591e-04\n",
      "Epoch 41/500\n",
      "12800/12800 [==============================] - 1s 113us/step - loss: 4.9318e-04 - val_loss: 5.0370e-04\n",
      "Epoch 42/500\n",
      "12800/12800 [==============================] - 2s 124us/step - loss: 4.9113e-04 - val_loss: 5.0216e-04\n",
      "Epoch 43/500\n",
      "12800/12800 [==============================] - 2s 125us/step - loss: 4.8931e-04 - val_loss: 5.0023e-04\n",
      "Epoch 44/500\n",
      "12800/12800 [==============================] - 1s 98us/step - loss: 4.8760e-04 - val_loss: 4.9883e-04\n",
      "Test MSE: 0.00049575481\n",
      "Total MSE: 0.00048995377\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input-layer (InputLayer)     (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "hidden-layer (Dense)         (None, 30)                9030      \n",
      "_________________________________________________________________\n",
      "output-layer (Dense)         (None, 300)               9300      \n",
      "=================================================================\n",
      "Total params: 18,330\n",
      "Trainable params: 18,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/500\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.0018 - val_loss: 0.0013\n",
      "Epoch 2/500\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 3/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 9.5375e-04 - val_loss: 9.1781e-04\n",
      "Epoch 4/500\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 8.4641e-04 - val_loss: 8.3001e-04\n",
      "Epoch 5/500\n",
      "12800/12800 [==============================] - 1s 94us/step - loss: 7.7670e-04 - val_loss: 7.7085e-04\n",
      "Epoch 6/500\n",
      "12800/12800 [==============================] - ETA: 0s - loss: 7.2764e-0 - 1s 89us/step - loss: 7.2866e-04 - val_loss: 7.2944e-04\n",
      "Epoch 7/500\n",
      "12800/12800 [==============================] - 1s 110us/step - loss: 6.9393e-04 - val_loss: 6.9957e-04\n",
      "Epoch 8/500\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 6.6748e-04 - val_loss: 6.7622e-04\n",
      "Epoch 9/500\n",
      "12800/12800 [==============================] - 1s 94us/step - loss: 6.4695e-04 - val_loss: 6.5728e-04\n",
      "Epoch 10/500\n",
      "12800/12800 [==============================] - 1s 102us/step - loss: 6.3036e-04 - val_loss: 6.4186e-04\n",
      "Epoch 11/500\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 6.1692e-04 - val_loss: 6.2966e-04\n",
      "Epoch 12/500\n",
      "12800/12800 [==============================] - 1s 99us/step - loss: 6.0571e-04 - val_loss: 6.1893e-04\n",
      "Epoch 13/500\n",
      "12800/12800 [==============================] - 1s 98us/step - loss: 5.9639e-04 - val_loss: 6.1079e-04\n",
      "Epoch 14/500\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 5.8853e-04 - val_loss: 6.0295e-04\n",
      "Epoch 15/500\n",
      "12800/12800 [==============================] - 1s 96us/step - loss: 5.8184e-04 - val_loss: 5.9666e-04\n",
      "Epoch 16/500\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 5.7600e-04 - val_loss: 5.9084e-04\n",
      "Epoch 17/500\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 5.7114e-04 - val_loss: 5.8582e-04\n",
      "Epoch 18/500\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 5.6679e-04 - val_loss: 5.8169e-04\n",
      "Epoch 19/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 5.6303e-04 - val_loss: 5.7843e-04\n",
      "Epoch 20/500\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 5.5968e-04 - val_loss: 5.7490e-04\n",
      "Epoch 21/500\n",
      "12800/12800 [==============================] - 1s 95us/step - loss: 5.5679e-04 - val_loss: 5.7157e-04\n",
      "Epoch 22/500\n",
      "12800/12800 [==============================] - 1s 100us/step - loss: 5.5419e-04 - val_loss: 5.6955e-04\n",
      "Epoch 23/500\n",
      "12800/12800 [==============================] - 1s 104us/step - loss: 5.5191e-04 - val_loss: 5.6689e-04\n",
      "Epoch 24/500\n",
      "12800/12800 [==============================] - 1s 101us/step - loss: 5.4984e-04 - val_loss: 5.6497e-04\n",
      "Epoch 25/500\n",
      "12800/12800 [==============================] - 1s 113us/step - loss: 5.4798e-04 - val_loss: 5.6307e-04\n",
      "Epoch 26/500\n",
      "12800/12800 [==============================] - 1s 99us/step - loss: 5.4628e-04 - val_loss: 5.6128e-04\n",
      "Epoch 27/500\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 5.4480e-04 - val_loss: 5.5990e-04\n",
      "Epoch 28/500\n",
      "12800/12800 [==============================] - 1s 61us/step - loss: 5.4344e-04 - val_loss: 5.5859e-04\n",
      "Epoch 29/500\n",
      "12800/12800 [==============================] - 1s 65us/step - loss: 5.4219e-04 - val_loss: 5.5724e-04\n",
      "Epoch 30/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 5.4106e-04 - val_loss: 5.5651e-04\n",
      "Epoch 31/500\n",
      "12800/12800 [==============================] - 2s 159us/step - loss: 5.4004e-04 - val_loss: 5.5520e-04\n",
      "Epoch 32/500\n",
      "12800/12800 [==============================] - 2s 130us/step - loss: 5.3905e-04 - val_loss: 5.5446e-04\n",
      "Epoch 33/500\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 5.3819e-04 - val_loss: 5.5321e-04\n",
      "Epoch 34/500\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 5.3742e-04 - val_loss: 5.5272e-04\n",
      "Epoch 35/500\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 5.3667e-04 - val_loss: 5.5189e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/500\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 5.3600e-04 - val_loss: 5.5126e-04\n",
      "Epoch 37/500\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 5.3541e-04 - val_loss: 5.5055e-04\n",
      "Epoch 38/500\n",
      "12800/12800 [==============================] - 1s 53us/step - loss: 5.3483e-04 - val_loss: 5.5006e-04\n",
      "Epoch 39/500\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 5.3427e-04 - val_loss: 5.4963e-04\n",
      "Epoch 40/500\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 5.3381e-04 - val_loss: 5.4952e-04\n",
      "Epoch 41/500\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 5.3331e-04 - val_loss: 5.4847e-04\n",
      "Epoch 42/500\n",
      "12800/12800 [==============================] - 1s 52us/step - loss: 5.3293e-04 - val_loss: 5.4804e-04\n",
      "Epoch 43/500\n",
      "12800/12800 [==============================] - 1s 48us/step - loss: 5.3257e-04 - val_loss: 5.4767e-04\n",
      "Epoch 44/500\n",
      "12800/12800 [==============================] - 1s 57us/step - loss: 5.3212e-04 - val_loss: 5.4721e-04\n",
      "Epoch 45/500\n",
      "12800/12800 [==============================] - 1s 53us/step - loss: 5.3178e-04 - val_loss: 5.4683e-04\n",
      "Epoch 46/500\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 5.3145e-04 - val_loss: 5.4661e-04\n",
      "Epoch 47/500\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 5.3121e-04 - val_loss: 5.4635e-04\n",
      "Epoch 48/500\n",
      "12800/12800 [==============================] - 1s 94us/step - loss: 5.3085e-04 - val_loss: 5.4621e-04\n",
      "Epoch 49/500\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 5.3063e-04 - val_loss: 5.4587e-04\n",
      "Epoch 50/500\n",
      "12800/12800 [==============================] - 1s 103us/step - loss: 5.3036e-04 - val_loss: 5.4547e-04\n",
      "Epoch 51/500\n",
      "12800/12800 [==============================] - 1s 65us/step - loss: 5.3017e-04 - val_loss: 5.4549e-04\n",
      "Epoch 52/500\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 5.2992e-04 - val_loss: 5.4515e-04\n",
      "Epoch 53/500\n",
      "12800/12800 [==============================] - 1s 62us/step - loss: 5.2966e-04 - val_loss: 5.4495e-04\n",
      "Epoch 54/500\n",
      "12800/12800 [==============================] - 1s 65us/step - loss: 5.2950e-04 - val_loss: 5.4452e-04\n",
      "Epoch 55/500\n",
      "12800/12800 [==============================] - 1s 61us/step - loss: 5.2926e-04 - val_loss: 5.4444e-04\n",
      "Epoch 56/500\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 5.2916e-04 - val_loss: 5.4415e-04\n",
      "Epoch 57/500\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 5.2893e-04 - val_loss: 5.4407e-04\n",
      "Epoch 58/500\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 5.2878e-04 - val_loss: 5.4394e-04\n",
      "Epoch 59/500\n",
      "12800/12800 [==============================] - 1s 54us/step - loss: 5.2867e-04 - val_loss: 5.4355e-04\n",
      "Epoch 60/500\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 5.2848e-04 - val_loss: 5.4358e-04\n",
      "Epoch 61/500\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 5.2835e-04 - val_loss: 5.4327e-04\n",
      "Epoch 62/500\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 5.2819e-04 - val_loss: 5.4330e-04\n",
      "Epoch 63/500\n",
      "12800/12800 [==============================] - 1s 106us/step - loss: 5.2807e-04 - val_loss: 5.4300e-04\n",
      "Epoch 64/500\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 5.2796e-04 - val_loss: 5.4312e-04\n",
      "Epoch 65/500\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 5.2783e-04 - val_loss: 5.4301e-04\n",
      "Epoch 66/500\n",
      "12800/12800 [==============================] - 1s 54us/step - loss: 5.2777e-04 - val_loss: 5.4288e-04\n",
      "Epoch 67/500\n",
      "12800/12800 [==============================] - 1s 56us/step - loss: 5.2760e-04 - val_loss: 5.4267e-04\n",
      "Epoch 68/500\n",
      "12800/12800 [==============================] - 1s 56us/step - loss: 5.2753e-04 - val_loss: 5.4241e-04\n",
      "Epoch 69/500\n",
      "12800/12800 [==============================] - 1s 53us/step - loss: 5.2741e-04 - val_loss: 5.4253e-04\n",
      "Epoch 70/500\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 5.2731e-04 - val_loss: 5.4229e-04\n",
      "Epoch 71/500\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 5.2722e-04 - val_loss: 5.4223e-04\n",
      "Epoch 72/500\n",
      "12800/12800 [==============================] - 1s 64us/step - loss: 5.2715e-04 - val_loss: 5.4205e-04\n",
      "Epoch 73/500\n",
      "12800/12800 [==============================] - 1s 63us/step - loss: 5.2698e-04 - val_loss: 5.4211e-04\n",
      "Epoch 74/500\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 5.2696e-04 - val_loss: 5.4218e-04\n",
      "Epoch 75/500\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 5.2684e-04 - val_loss: 5.4180e-04\n",
      "Epoch 76/500\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 5.2678e-04 - val_loss: 5.4170e-04\n",
      "Epoch 77/500\n",
      "12800/12800 [==============================] - 1s 54us/step - loss: 5.2670e-04 - val_loss: 5.4190e-04\n",
      "Epoch 78/500\n",
      "12800/12800 [==============================] - 1s 62us/step - loss: 5.2667e-04 - val_loss: 5.4141e-04\n",
      "Epoch 79/500\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 5.2656e-04 - val_loss: 5.4164e-04\n",
      "Epoch 80/500\n",
      "12800/12800 [==============================] - 1s 63us/step - loss: 5.2652e-04 - val_loss: 5.4151e-04\n",
      "Epoch 81/500\n",
      "12800/12800 [==============================] - 1s 61us/step - loss: 5.2647e-04 - val_loss: 5.4125e-04\n",
      "Epoch 82/500\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 5.2635e-04 - val_loss: 5.4158e-04\n",
      "Epoch 83/500\n",
      "12800/12800 [==============================] - 1s 62us/step - loss: 5.2630e-04 - val_loss: 5.4123e-04\n",
      "Epoch 84/500\n",
      "12800/12800 [==============================] - 1s 57us/step - loss: 5.2623e-04 - val_loss: 5.4105e-04\n",
      "Epoch 85/500\n",
      "12800/12800 [==============================] - 1s 64us/step - loss: 5.2618e-04 - val_loss: 5.4110e-04\n",
      "Epoch 86/500\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 5.2611e-04 - val_loss: 5.4115e-04\n",
      "Epoch 87/500\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 5.2607e-04 - val_loss: 5.4100e-04\n",
      "Epoch 88/500\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 5.2605e-04 - val_loss: 5.4088e-04\n",
      "Epoch 89/500\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 5.2596e-04 - val_loss: 5.4078e-04\n",
      "Epoch 90/500\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 5.2591e-04 - val_loss: 5.4093e-04\n",
      "Epoch 91/500\n",
      "12800/12800 [==============================] - 1s 53us/step - loss: 5.2587e-04 - val_loss: 5.4074e-04\n",
      "Epoch 92/500\n",
      "12800/12800 [==============================] - 1s 53us/step - loss: 5.2582e-04 - val_loss: 5.4073e-04\n",
      "Epoch 93/500\n",
      "12800/12800 [==============================] - 1s 63us/step - loss: 5.2573e-04 - val_loss: 5.4080e-04\n",
      "Epoch 94/500\n",
      "12800/12800 [==============================] - 1s 56us/step - loss: 5.2569e-04 - val_loss: 5.4062e-04\n",
      "Epoch 95/500\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 5.2568e-04 - val_loss: 5.4047e-04\n",
      "Epoch 96/500\n",
      "12800/12800 [==============================] - 1s 65us/step - loss: 5.2563e-04 - val_loss: 5.4046e-04\n",
      "Epoch 97/500\n",
      "12800/12800 [==============================] - 1s 68us/step - loss: 5.2558e-04 - val_loss: 5.4036e-04\n",
      "Epoch 98/500\n",
      "12800/12800 [==============================] - 1s 63us/step - loss: 5.2551e-04 - val_loss: 5.4031e-04\n",
      "Epoch 99/500\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 5.2547e-04 - val_loss: 5.4029e-04\n",
      "Epoch 100/500\n",
      "12800/12800 [==============================] - 1s 54us/step - loss: 5.2543e-04 - val_loss: 5.4027e-04\n",
      "Epoch 101/500\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 5.2540e-04 - val_loss: 5.4027e-04\n",
      "Epoch 102/500\n",
      "12800/12800 [==============================] - 1s 56us/step - loss: 5.2535e-04 - val_loss: 5.4009e-04\n",
      "Epoch 103/500\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 5.2533e-04 - val_loss: 5.4008e-04\n",
      "Epoch 104/500\n",
      "12800/12800 [==============================] - 1s 56us/step - loss: 5.2526e-04 - val_loss: 5.4006e-04\n",
      "Epoch 105/500\n",
      "12800/12800 [==============================] - 1s 62us/step - loss: 5.2523e-04 - val_loss: 5.4018e-04\n",
      "Epoch 106/500\n",
      "12800/12800 [==============================] - 1s 53us/step - loss: 5.2519e-04 - val_loss: 5.3971e-04\n",
      "Epoch 107/500\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 5.2519e-04 - val_loss: 5.3973e-04\n",
      "Epoch 108/500\n",
      "12800/12800 [==============================] - 1s 52us/step - loss: 5.2515e-04 - val_loss: 5.3994e-04\n",
      "Epoch 109/500\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 5.2510e-04 - val_loss: 5.3995e-04\n",
      "Epoch 110/500\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 5.2508e-04 - val_loss: 5.3993e-04\n",
      "Epoch 111/500\n",
      "12800/12800 [==============================] - 1s 63us/step - loss: 5.2502e-04 - val_loss: 5.3974e-04\n",
      "Epoch 112/500\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 5.2502e-04 - val_loss: 5.3965e-04\n",
      "Epoch 113/500\n",
      "12800/12800 [==============================] - 1s 52us/step - loss: 5.2501e-04 - val_loss: 5.3970e-04\n",
      "Epoch 114/500\n",
      "12800/12800 [==============================] - 1s 57us/step - loss: 5.2494e-04 - val_loss: 5.3959e-04\n",
      "Epoch 115/500\n",
      "12800/12800 [==============================] - 1s 53us/step - loss: 5.2488e-04 - val_loss: 5.3981e-04\n",
      "Epoch 116/500\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 5.2490e-04 - val_loss: 5.3973e-04\n",
      "Epoch 117/500\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 5.2481e-04 - val_loss: 5.3978e-04\n",
      "Epoch 118/500\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 5.2481e-04 - val_loss: 5.3962e-04\n",
      "Epoch 119/500\n",
      "12800/12800 [==============================] - 1s 53us/step - loss: 5.2478e-04 - val_loss: 5.3944e-04\n",
      "Epoch 120/500\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 5.2476e-04 - val_loss: 5.3944e-04\n",
      "Epoch 121/500\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 5.2473e-04 - val_loss: 5.3961e-04\n",
      "Epoch 122/500\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 5.2469e-04 - val_loss: 5.3938e-04\n",
      "Epoch 123/500\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 5.2468e-04 - val_loss: 5.3947e-04\n",
      "Epoch 124/500\n",
      "12800/12800 [==============================] - 1s 53us/step - loss: 5.2461e-04 - val_loss: 5.3953e-04\n",
      "Epoch 125/500\n",
      "12800/12800 [==============================] - 1s 54us/step - loss: 5.2463e-04 - val_loss: 5.3945e-04\n",
      "Epoch 126/500\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 5.2461e-04 - val_loss: 5.3933e-04\n",
      "Epoch 127/500\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 5.2457e-04 - val_loss: 5.3926e-04\n",
      "Epoch 128/500\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 5.2454e-04 - val_loss: 5.3910e-04\n",
      "Epoch 129/500\n",
      "12800/12800 [==============================] - 1s 52us/step - loss: 5.2455e-04 - val_loss: 5.3916e-04\n",
      "Epoch 130/500\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 5.2453e-04 - val_loss: 5.3910e-04\n",
      "Epoch 131/500\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 5.2447e-04 - val_loss: 5.3903e-04\n",
      "Epoch 132/500\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 5.2446e-04 - val_loss: 5.3898e-04\n",
      "Epoch 133/500\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 5.2440e-04 - val_loss: 5.3918e-04\n",
      "Epoch 134/500\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 5.2442e-04 - val_loss: 5.3897e-04\n",
      "Epoch 135/500\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 5.2437e-04 - val_loss: 5.3907e-04\n",
      "Epoch 136/500\n",
      "12800/12800 [==============================] - 1s 48us/step - loss: 5.2437e-04 - val_loss: 5.3880e-04\n",
      "Epoch 137/500\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 5.2434e-04 - val_loss: 5.3884e-04\n",
      "Epoch 138/500\n",
      "12800/12800 [==============================] - 1s 53us/step - loss: 5.2434e-04 - val_loss: 5.3878e-04\n",
      "Epoch 139/500\n",
      "12800/12800 [==============================] - 1s 53us/step - loss: 5.2433e-04 - val_loss: 5.3876e-04\n",
      "Epoch 140/500\n",
      "12800/12800 [==============================] - 1s 54us/step - loss: 5.2427e-04 - val_loss: 5.3891e-04\n",
      "Epoch 141/500\n",
      "12800/12800 [==============================] - 1s 56us/step - loss: 5.2426e-04 - val_loss: 5.3895e-04\n",
      "Epoch 142/500\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 5.2425e-04 - val_loss: 5.3871e-04\n",
      "Epoch 143/500\n",
      "12800/12800 [==============================] - 1s 72us/step - loss: 5.2421e-04 - val_loss: 5.3891e-04\n",
      "Epoch 144/500\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 5.2421e-04 - val_loss: 5.3879e-04\n",
      "Epoch 145/500\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 5.2417e-04 - val_loss: 5.3881e-04\n",
      "Epoch 146/500\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 5.2415e-04 - val_loss: 5.3881e-04\n",
      "Epoch 147/500\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 5.2414e-04 - val_loss: 5.3876e-04\n",
      "Epoch 148/500\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 5.2412e-04 - val_loss: 5.3862e-04\n",
      "Epoch 149/500\n",
      "12800/12800 [==============================] - 1s 52us/step - loss: 5.2409e-04 - val_loss: 5.3866e-04\n",
      "Epoch 150/500\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 5.2408e-04 - val_loss: 5.3859e-04\n",
      "Epoch 151/500\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 5.2407e-04 - val_loss: 5.3876e-04\n",
      "Epoch 152/500\n",
      "12800/12800 [==============================] - 1s 52us/step - loss: 5.2407e-04 - val_loss: 5.3867e-04\n",
      "Epoch 153/500\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 5.2404e-04 - val_loss: 5.3850e-04\n",
      "Epoch 154/500\n",
      "12800/12800 [==============================] - 1s 62us/step - loss: 5.2401e-04 - val_loss: 5.3857e-04\n",
      "Epoch 155/500\n",
      "12800/12800 [==============================] - 1s 53us/step - loss: 5.2401e-04 - val_loss: 5.3842e-04\n",
      "Epoch 156/500\n",
      "12800/12800 [==============================] - 1s 56us/step - loss: 5.2399e-04 - val_loss: 5.3839e-04\n",
      "Epoch 157/500\n",
      "12800/12800 [==============================] - 1s 53us/step - loss: 5.2398e-04 - val_loss: 5.3850e-04\n",
      "Epoch 158/500\n",
      "12800/12800 [==============================] - 1s 52us/step - loss: 5.2398e-04 - val_loss: 5.3845e-04\n",
      "Epoch 159/500\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 5.2395e-04 - val_loss: 5.3826e-04\n",
      "Epoch 160/500\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 5.2390e-04 - val_loss: 5.3845e-04\n",
      "Epoch 161/500\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 5.2393e-04 - val_loss: 5.3842e-04\n",
      "Epoch 162/500\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 5.2387e-04 - val_loss: 5.3837e-04\n",
      "Epoch 163/500\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 5.2384e-04 - val_loss: 5.3832e-04\n",
      "Epoch 164/500\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 5.2388e-04 - val_loss: 5.3841e-04\n",
      "Epoch 165/500\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 5.2387e-04 - val_loss: 5.3837e-04\n",
      "Test MSE: 0.0005299931\n",
      "Total MSE: 0.00052696975\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input-layer (InputLayer)     (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "hidden-layer-1 (Dense)       (None, 60)                18060     \n",
      "_________________________________________________________________\n",
      "hidden-layer-2 (Dense)       (None, 30)                1830      \n",
      "_________________________________________________________________\n",
      "hidden-layer-3 (Dense)       (None, 60)                1860      \n",
      "_________________________________________________________________\n",
      "output-layer (Dense)         (None, 300)               18300     \n",
      "=================================================================\n",
      "Total params: 40,050\n",
      "Trainable params: 40,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/500\n",
      "12800/12800 [==============================] - 2s 190us/step - loss: 0.0023 - val_loss: 0.0015\n",
      "Epoch 2/500\n",
      "12800/12800 [==============================] - 2s 129us/step - loss: 0.0014 - val_loss: 0.0012\n",
      "Epoch 3/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 1s 106us/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 4/500\n",
      "12800/12800 [==============================] - 1s 102us/step - loss: 0.0010 - val_loss: 9.9661e-04\n",
      "Epoch 5/500\n",
      "12800/12800 [==============================] - 1s 108us/step - loss: 9.5974e-04 - val_loss: 9.2479e-04\n",
      "Epoch 6/500\n",
      "12800/12800 [==============================] - 1s 115us/step - loss: 8.9524e-04 - val_loss: 8.6970e-04\n",
      "Epoch 7/500\n",
      "12800/12800 [==============================] - 2s 135us/step - loss: 8.4485e-04 - val_loss: 8.2607e-04\n",
      "Epoch 8/500\n",
      "12800/12800 [==============================] - 1s 106us/step - loss: 8.0478e-04 - val_loss: 7.9029e-04\n",
      "Epoch 9/500\n",
      "12800/12800 [==============================] - 1s 105us/step - loss: 7.7173e-04 - val_loss: 7.6256e-04\n",
      "Epoch 10/500\n",
      "12800/12800 [==============================] - 1s 98us/step - loss: 7.4453e-04 - val_loss: 7.3741e-04\n",
      "Epoch 11/500\n",
      "12800/12800 [==============================] - 1s 112us/step - loss: 7.2171e-04 - val_loss: 7.1664e-04\n",
      "Epoch 12/500\n",
      "12800/12800 [==============================] - 1s 96us/step - loss: 7.0171e-04 - val_loss: 6.9861e-04\n",
      "Epoch 13/500\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 6.8440e-04 - val_loss: 6.8268e-04\n",
      "Epoch 14/500\n",
      "12800/12800 [==============================] - 1s 88us/step - loss: 6.6902e-04 - val_loss: 6.6863e-04\n",
      "Epoch 15/500\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 6.5504e-04 - val_loss: 6.5554e-04\n",
      "Epoch 16/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 6.4239e-04 - val_loss: 6.4422e-04\n",
      "Epoch 17/500\n",
      "12800/12800 [==============================] - 1s 115us/step - loss: 6.3092e-04 - val_loss: 6.3325e-04\n",
      "Epoch 18/500\n",
      "12800/12800 [==============================] - 1s 111us/step - loss: 6.2033e-04 - val_loss: 6.2335e-04\n",
      "Epoch 19/500\n",
      "12800/12800 [==============================] - 1s 105us/step - loss: 6.1091e-04 - val_loss: 6.1452e-04\n",
      "Epoch 20/500\n",
      "12800/12800 [==============================] - 2s 130us/step - loss: 6.0239e-04 - val_loss: 6.0707e-04\n",
      "Epoch 21/500\n",
      "12800/12800 [==============================] - 1s 107us/step - loss: 5.9478e-04 - val_loss: 5.9981e-04\n",
      "Epoch 22/500\n",
      "12800/12800 [==============================] - 1s 97us/step - loss: 5.8823e-04 - val_loss: 5.9399e-04\n",
      "Epoch 23/500\n",
      "12800/12800 [==============================] - 1s 94us/step - loss: 5.8235e-04 - val_loss: 5.8844e-04\n",
      "Epoch 24/500\n",
      "12800/12800 [==============================] - 1s 95us/step - loss: 5.7723e-04 - val_loss: 5.8369e-04\n",
      "Epoch 25/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 5.7274e-04 - val_loss: 5.7959e-04\n",
      "Epoch 26/500\n",
      "12800/12800 [==============================] - 1s 94us/step - loss: 5.6891e-04 - val_loss: 5.7622e-04\n",
      "Epoch 27/500\n",
      "12800/12800 [==============================] - 1s 94us/step - loss: 5.6544e-04 - val_loss: 5.7337e-04\n",
      "Epoch 28/500\n",
      "12800/12800 [==============================] - 1s 97us/step - loss: 5.6267e-04 - val_loss: 5.7040e-04\n",
      "Epoch 29/500\n",
      "12800/12800 [==============================] - 1s 105us/step - loss: 5.5998e-04 - val_loss: 5.6839e-04\n",
      "Epoch 30/500\n",
      "12800/12800 [==============================] - 1s 98us/step - loss: 5.5776e-04 - val_loss: 5.6594e-04\n",
      "Epoch 31/500\n",
      "12800/12800 [==============================] - 1s 100us/step - loss: 5.5567e-04 - val_loss: 5.6424e-04\n",
      "Epoch 32/500\n",
      "12800/12800 [==============================] - 1s 96us/step - loss: 5.5397e-04 - val_loss: 5.6219e-04\n",
      "Epoch 33/500\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 5.5236e-04 - val_loss: 5.6110e-04\n",
      "Epoch 34/500\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 5.5091e-04 - val_loss: 5.5973e-04\n",
      "Epoch 35/500\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 5.4959e-04 - val_loss: 5.5858e-04\n",
      "Epoch 36/500\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 5.4848e-04 - val_loss: 5.5744e-04\n",
      "Epoch 37/500\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 5.4746e-04 - val_loss: 5.5647e-04\n",
      "Epoch 38/500\n",
      "12800/12800 [==============================] - 1s 112us/step - loss: 5.4643e-04 - val_loss: 5.5556e-04\n",
      "Epoch 39/500\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 5.4559e-04 - val_loss: 5.5444e-04\n",
      "Epoch 40/500\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 5.4479e-04 - val_loss: 5.5351e-04\n",
      "Epoch 41/500\n",
      "12800/12800 [==============================] - 1s 88us/step - loss: 5.4405e-04 - val_loss: 5.5299e-04\n",
      "Epoch 42/500\n",
      "12800/12800 [==============================] - 1s 95us/step - loss: 5.4329e-04 - val_loss: 5.5212e-04\n",
      "Epoch 43/500\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 5.4261e-04 - val_loss: 5.5169e-04\n",
      "Epoch 44/500\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 5.4202e-04 - val_loss: 5.5113e-04\n",
      "Epoch 45/500\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 5.4148e-04 - val_loss: 5.5061e-04\n",
      "Epoch 46/500\n",
      "12800/12800 [==============================] - 1s 110us/step - loss: 5.4094e-04 - val_loss: 5.5010e-04\n",
      "Epoch 47/500\n",
      "12800/12800 [==============================] - 1s 109us/step - loss: 5.4042e-04 - val_loss: 5.4911e-04\n",
      "Epoch 48/500\n",
      "12800/12800 [==============================] - 1s 99us/step - loss: 5.3988e-04 - val_loss: 5.4893e-04\n",
      "Epoch 49/500\n",
      "12800/12800 [==============================] - 1s 88us/step - loss: 5.3944e-04 - val_loss: 5.4847e-04\n",
      "Epoch 50/500\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 5.3900e-04 - val_loss: 5.4822e-04\n",
      "Epoch 51/500\n",
      "12800/12800 [==============================] - 1s 93us/step - loss: 5.3860e-04 - val_loss: 5.4783e-04\n",
      "Epoch 52/500\n",
      "12800/12800 [==============================] - 1s 92us/step - loss: 5.3817e-04 - val_loss: 5.4705e-04\n",
      "Epoch 53/500\n",
      "12800/12800 [==============================] - 1s 94us/step - loss: 5.3786e-04 - val_loss: 5.4665e-04\n",
      "Epoch 54/500\n",
      "12800/12800 [==============================] - 2s 121us/step - loss: 5.3747e-04 - val_loss: 5.4647e-04\n",
      "Epoch 55/500\n",
      "12800/12800 [==============================] - 1s 101us/step - loss: 5.3716e-04 - val_loss: 5.4660e-04\n",
      "Epoch 56/500\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 5.3677e-04 - val_loss: 5.4591e-04\n",
      "Epoch 57/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 5.3651e-04 - val_loss: 5.4577e-04\n",
      "Epoch 58/500\n",
      "12800/12800 [==============================] - 1s 110us/step - loss: 5.3621e-04 - val_loss: 5.4497e-04\n",
      "Epoch 59/500\n",
      "12800/12800 [==============================] - 1s 102us/step - loss: 5.3592e-04 - val_loss: 5.4487e-04\n",
      "Epoch 60/500\n",
      "12800/12800 [==============================] - 1s 93us/step - loss: 5.3563e-04 - val_loss: 5.4453e-04\n",
      "Epoch 61/500\n",
      "12800/12800 [==============================] - 1s 108us/step - loss: 5.3537e-04 - val_loss: 5.4431e-04\n",
      "Epoch 62/500\n",
      "12800/12800 [==============================] - 1s 101us/step - loss: 5.3514e-04 - val_loss: 5.4407e-04\n",
      "Epoch 63/500\n",
      "12800/12800 [==============================] - 1s 97us/step - loss: 5.3486e-04 - val_loss: 5.4378e-04\n",
      "Epoch 64/500\n",
      "12800/12800 [==============================] - 2s 126us/step - loss: 5.3460e-04 - val_loss: 5.4386e-04\n",
      "Epoch 65/500\n",
      "12800/12800 [==============================] - 1s 111us/step - loss: 5.3443e-04 - val_loss: 5.4329e-04\n",
      "Epoch 66/500\n",
      "12800/12800 [==============================] - 1s 98us/step - loss: 5.3413e-04 - val_loss: 5.4293e-04\n",
      "Epoch 67/500\n",
      "12800/12800 [==============================] - 1s 92us/step - loss: 5.3388e-04 - val_loss: 5.4323e-04\n",
      "Epoch 68/500\n",
      "12800/12800 [==============================] - 1s 94us/step - loss: 5.3369e-04 - val_loss: 5.4314e-04\n",
      "Epoch 69/500\n",
      "12800/12800 [==============================] - 1s 96us/step - loss: 5.3358e-04 - val_loss: 5.4253e-04\n",
      "Epoch 70/500\n",
      "12800/12800 [==============================] - 1s 93us/step - loss: 5.3335e-04 - val_loss: 5.4231e-04\n",
      "Epoch 71/500\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 5.3317e-04 - val_loss: 5.4205e-04\n",
      "Epoch 72/500\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 5.3289e-04 - val_loss: 5.4221e-04\n",
      "Epoch 73/500\n",
      "12800/12800 [==============================] - 1s 96us/step - loss: 5.3279e-04 - val_loss: 5.4174e-04\n",
      "Epoch 74/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 5.3260e-04 - val_loss: 5.4156e-04\n",
      "Epoch 75/500\n",
      "12800/12800 [==============================] - 1s 105us/step - loss: 5.3238e-04 - val_loss: 5.4156e-04\n",
      "Epoch 76/500\n",
      "12800/12800 [==============================] - 1s 114us/step - loss: 5.3225e-04 - val_loss: 5.4113e-04\n",
      "Epoch 77/500\n",
      "12800/12800 [==============================] - 1s 110us/step - loss: 5.3214e-04 - val_loss: 5.4118e-04\n",
      "Epoch 78/500\n",
      "12800/12800 [==============================] - 1s 94us/step - loss: 5.3189e-04 - val_loss: 5.4093e-04\n",
      "Epoch 79/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 5.3183e-04 - val_loss: 5.4073e-04\n",
      "Epoch 80/500\n",
      "12800/12800 [==============================] - 1s 114us/step - loss: 5.3164e-04 - val_loss: 5.4038e-04\n",
      "Epoch 81/500\n",
      "12800/12800 [==============================] - 2s 142us/step - loss: 5.3147e-04 - val_loss: 5.4065e-04\n",
      "Epoch 82/500\n",
      "12800/12800 [==============================] - 2s 135us/step - loss: 5.3134e-04 - val_loss: 5.4017e-04\n",
      "Epoch 83/500\n",
      "12800/12800 [==============================] - 1s 96us/step - loss: 5.3119e-04 - val_loss: 5.4011e-04\n",
      "Epoch 84/500\n",
      "12800/12800 [==============================] - 1s 96us/step - loss: 5.3105e-04 - val_loss: 5.4016e-04\n",
      "Epoch 85/500\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 5.3094e-04 - val_loss: 5.3990e-04\n",
      "Epoch 86/500\n",
      "12800/12800 [==============================] - 1s 95us/step - loss: 5.3081e-04 - val_loss: 5.3988e-04\n",
      "Epoch 87/500\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 5.3069e-04 - val_loss: 5.3973e-04\n",
      "Epoch 88/500\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 5.3051e-04 - val_loss: 5.3974e-04\n",
      "Epoch 89/500\n",
      "12800/12800 [==============================] - 1s 97us/step - loss: 5.3042e-04 - val_loss: 5.3934e-04\n",
      "Epoch 90/500\n",
      "12800/12800 [==============================] - 1s 114us/step - loss: 5.3035e-04 - val_loss: 5.3908e-04\n",
      "Epoch 91/500\n",
      "12800/12800 [==============================] - 1s 94us/step - loss: 5.3019e-04 - val_loss: 5.3905e-04\n",
      "Epoch 92/500\n",
      "12800/12800 [==============================] - ETA: 0s - loss: 5.3003e-0 - 1s 104us/step - loss: 5.3010e-04 - val_loss: 5.3918e-04\n",
      "Epoch 93/500\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 5.2996e-04 - val_loss: 5.3881e-04\n",
      "Epoch 94/500\n",
      "12800/12800 [==============================] - 1s 96us/step - loss: 5.2986e-04 - val_loss: 5.3882e-04\n",
      "Epoch 95/500\n",
      "12800/12800 [==============================] - 1s 96us/step - loss: 5.2975e-04 - val_loss: 5.3879e-04\n",
      "Epoch 96/500\n",
      "12800/12800 [==============================] - 2s 126us/step - loss: 5.2967e-04 - val_loss: 5.3857e-04\n",
      "Epoch 97/500\n",
      "12800/12800 [==============================] - 1s 97us/step - loss: 5.2958e-04 - val_loss: 5.3856e-04\n",
      "Epoch 98/500\n",
      "12800/12800 [==============================] - 2s 123us/step - loss: 5.2946e-04 - val_loss: 5.3828e-04\n",
      "Epoch 99/500\n",
      "12800/12800 [==============================] - 1s 101us/step - loss: 5.2937e-04 - val_loss: 5.3822e-04\n",
      "Epoch 100/500\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 5.2926e-04 - val_loss: 5.3818e-04\n",
      "Epoch 101/500\n",
      "12800/12800 [==============================] - 1s 57us/step - loss: 5.2915e-04 - val_loss: 5.3811e-04\n",
      "Epoch 102/500\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 5.2904e-04 - val_loss: 5.3816e-04\n",
      "Epoch 103/500\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 5.2895e-04 - val_loss: 5.3781e-04\n",
      "Epoch 104/500\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 5.2891e-04 - val_loss: 5.3772e-04\n",
      "Epoch 105/500\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 5.2879e-04 - val_loss: 5.3788e-04\n",
      "Epoch 106/500\n",
      "12800/12800 [==============================] - 1s 57us/step - loss: 5.2878e-04 - val_loss: 5.3774e-04\n",
      "Epoch 107/500\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 5.2865e-04 - val_loss: 5.3769e-04\n",
      "Epoch 108/500\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 5.2852e-04 - val_loss: 5.3758e-04\n",
      "Epoch 109/500\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 5.2848e-04 - val_loss: 5.3757e-04\n",
      "Epoch 110/500\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 5.2841e-04 - val_loss: 5.3748e-04\n",
      "Epoch 111/500\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 5.2832e-04 - val_loss: 5.3755e-04\n",
      "Epoch 112/500\n",
      "12800/12800 [==============================] - 1s 62us/step - loss: 5.2826e-04 - val_loss: 5.3738e-04\n",
      "Epoch 113/500\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 5.2816e-04 - val_loss: 5.3744e-04\n",
      "Epoch 114/500\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 5.2808e-04 - val_loss: 5.3732e-04\n",
      "Epoch 115/500\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 5.2803e-04 - val_loss: 5.3717e-04\n",
      "Epoch 116/500\n",
      "12800/12800 [==============================] - 1s 63us/step - loss: 5.2795e-04 - val_loss: 5.3726e-04\n",
      "Epoch 117/500\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 5.2790e-04 - val_loss: 5.3695e-04\n",
      "Epoch 118/500\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 5.2785e-04 - val_loss: 5.3678e-04\n",
      "Epoch 119/500\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 5.2778e-04 - val_loss: 5.3673e-04\n",
      "Epoch 120/500\n",
      "12800/12800 [==============================] - 2s 135us/step - loss: 5.2773e-04 - val_loss: 5.3669e-04\n",
      "Epoch 121/500\n",
      "12800/12800 [==============================] - 2s 147us/step - loss: 5.2762e-04 - val_loss: 5.3669e-04\n",
      "Epoch 122/500\n",
      "12800/12800 [==============================] - 2s 132us/step - loss: 5.2758e-04 - val_loss: 5.3660e-04\n",
      "Epoch 123/500\n",
      "12800/12800 [==============================] - 1s 100us/step - loss: 5.2748e-04 - val_loss: 5.3656e-04\n",
      "Epoch 124/500\n",
      "12800/12800 [==============================] - 1s 104us/step - loss: 5.2743e-04 - val_loss: 5.3639e-04\n",
      "Epoch 125/500\n",
      "12800/12800 [==============================] - 2s 119us/step - loss: 5.2736e-04 - val_loss: 5.3638e-04\n",
      "Epoch 126/500\n",
      "12800/12800 [==============================] - 1s 109us/step - loss: 5.2731e-04 - val_loss: 5.3634e-04\n",
      "Epoch 127/500\n",
      "12800/12800 [==============================] - 2s 119us/step - loss: 5.2729e-04 - val_loss: 5.3645e-04\n",
      "Epoch 128/500\n",
      "12800/12800 [==============================] - 1s 110us/step - loss: 5.2721e-04 - val_loss: 5.3628e-04\n",
      "Epoch 129/500\n",
      "12800/12800 [==============================] - 1s 102us/step - loss: 5.2711e-04 - val_loss: 5.3634e-04\n",
      "Epoch 130/500\n",
      "12800/12800 [==============================] - 1s 113us/step - loss: 5.2709e-04 - val_loss: 5.3605e-04\n",
      "Epoch 131/500\n",
      "12800/12800 [==============================] - 2s 123us/step - loss: 5.2705e-04 - val_loss: 5.3631e-04\n",
      "Epoch 132/500\n",
      "12800/12800 [==============================] - 1s 117us/step - loss: 5.2702e-04 - val_loss: 5.3630e-04\n",
      "Epoch 133/500\n",
      "12800/12800 [==============================] - 1s 110us/step - loss: 5.2692e-04 - val_loss: 5.3610e-04\n",
      "Epoch 134/500\n",
      "12800/12800 [==============================] - 1s 96us/step - loss: 5.2688e-04 - val_loss: 5.3583e-04\n",
      "Epoch 135/500\n",
      "12800/12800 [==============================] - 1s 97us/step - loss: 5.2685e-04 - val_loss: 5.3578e-04\n",
      "Epoch 136/500\n",
      "12800/12800 [==============================] - 1s 94us/step - loss: 5.2680e-04 - val_loss: 5.3600e-04\n",
      "Epoch 137/500\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 5.2672e-04 - val_loss: 5.3588e-04\n",
      "Epoch 138/500\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 5.2671e-04 - val_loss: 5.3590e-04\n",
      "Epoch 139/500\n",
      "12800/12800 [==============================] - 1s 92us/step - loss: 5.2664e-04 - val_loss: 5.3600e-04\n",
      "Epoch 140/500\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 5.2661e-04 - val_loss: 5.3569e-04\n",
      "Epoch 141/500\n",
      "12800/12800 [==============================] - 1s 97us/step - loss: 5.2658e-04 - val_loss: 5.3587e-04\n",
      "Epoch 142/500\n",
      "12800/12800 [==============================] - 1s 111us/step - loss: 5.2654e-04 - val_loss: 5.3572e-04\n",
      "Epoch 143/500\n",
      "12800/12800 [==============================] - 1s 97us/step - loss: 5.2648e-04 - val_loss: 5.3549e-04\n",
      "Epoch 144/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 1s 94us/step - loss: 5.2641e-04 - val_loss: 5.3534e-04\n",
      "Epoch 145/500\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 5.2638e-04 - val_loss: 5.3542e-04\n",
      "Epoch 146/500\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 5.2636e-04 - val_loss: 5.3542e-04\n",
      "Epoch 147/500\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 5.2630e-04 - val_loss: 5.3531e-04\n",
      "Epoch 148/500\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 5.2624e-04 - val_loss: 5.3540e-04\n",
      "Epoch 149/500\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 5.2621e-04 - val_loss: 5.3533e-04\n",
      "Epoch 150/500\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 5.2618e-04 - val_loss: 5.3546e-04\n",
      "Epoch 151/500\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 5.2619e-04 - val_loss: 5.3538e-04\n",
      "Epoch 152/500\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 5.2606e-04 - val_loss: 5.3531e-04\n",
      "Epoch 153/500\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 5.2608e-04 - val_loss: 5.3528e-04\n",
      "Epoch 154/500\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 5.2604e-04 - val_loss: 5.3518e-04\n",
      "Epoch 155/500\n",
      "12800/12800 [==============================] - 1s 72us/step - loss: 5.2600e-04 - val_loss: 5.3519e-04\n",
      "Epoch 156/500\n",
      "12800/12800 [==============================] - 1s 72us/step - loss: 5.2594e-04 - val_loss: 5.3520e-04\n",
      "Epoch 157/500\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 5.2591e-04 - val_loss: 5.3507e-04\n",
      "Epoch 158/500\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 5.2589e-04 - val_loss: 5.3524e-04\n",
      "Epoch 159/500\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 5.2586e-04 - val_loss: 5.3514e-04\n",
      "Epoch 160/500\n",
      "12800/12800 [==============================] - 1s 72us/step - loss: 5.2580e-04 - val_loss: 5.3497e-04\n",
      "Epoch 161/500\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 5.2576e-04 - val_loss: 5.3502e-04\n",
      "Epoch 162/500\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 5.2582e-04 - val_loss: 5.3482e-04\n",
      "Epoch 163/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 5.2570e-04 - val_loss: 5.3507e-04\n",
      "Epoch 164/500\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 5.2568e-04 - val_loss: 5.3486e-04\n",
      "Epoch 165/500\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 5.2563e-04 - val_loss: 5.3495e-04\n",
      "Epoch 166/500\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 5.2563e-04 - val_loss: 5.3483e-04\n",
      "Epoch 167/500\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 5.2559e-04 - val_loss: 5.3482e-04\n",
      "Epoch 168/500\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 5.2558e-04 - val_loss: 5.3473e-04\n",
      "Epoch 169/500\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 5.2558e-04 - val_loss: 5.3475e-04\n",
      "Epoch 170/500\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 5.2552e-04 - val_loss: 5.3458e-04\n",
      "Epoch 171/500\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 5.2545e-04 - val_loss: 5.3476e-04\n",
      "Epoch 172/500\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 5.2545e-04 - val_loss: 5.3471e-04\n",
      "Epoch 173/500\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 5.2542e-04 - val_loss: 5.3481e-04\n",
      "Epoch 174/500\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 5.2541e-04 - val_loss: 5.3485e-04\n",
      "Epoch 175/500\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 5.2537e-04 - val_loss: 5.3460e-04\n",
      "Epoch 176/500\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 5.2534e-04 - val_loss: 5.3464e-04\n",
      "Test MSE: 0.00053251541\n",
      "Total MSE: 0.00052772473\n"
     ]
    }
   ],
   "source": [
    "z_epochs = {}\n",
    "z_epochs_deep = {}\n",
    "for z in range(2, 11):\n",
    "    z_epochs[z] = process_image('a.jpg', z, 5e-4)\n",
    "    z_epochs[z] = process_image_deeply('a.jpg', z, 5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'z_epochs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-baf251ad04b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Comression rate z'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mylabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Epochs count'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_epochs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mz_epochs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-o'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Epochs count'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'z_epochs' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x468 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['font.size'] = 15\n",
    "plt.rcParams['font.weight'] = 100\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(15, 6.5)\n",
    "\n",
    "ax.set(xlabel='Comression rate z', ylabel='Epochs count')\n",
    "ax.plot(z_epochs.keys(), [len(e['loss']) for e in z_epochs.values()], '-o', label='Epochs count', linewidth=2)\n",
    "\n",
    "ax.grid()\n",
    "ax.legend(loc='upper center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: {'val_loss': [0.0017552691407036036,\n",
       "   0.001336450955714099,\n",
       "   0.0010947001149179414,\n",
       "   0.000932919248007238,\n",
       "   0.0008163274260004983,\n",
       "   0.000727010543923825,\n",
       "   0.0006565554399276152,\n",
       "   0.0005968422375735826,\n",
       "   0.0005479333066614344,\n",
       "   0.0005060932619380765,\n",
       "   0.0004699480315321125],\n",
       "  'loss': [0.007061152483220212,\n",
       "   0.0015567110729170963,\n",
       "   0.00123290992793045,\n",
       "   0.001029756852076389,\n",
       "   0.0008889430829731282,\n",
       "   0.0007843219723145012,\n",
       "   0.0007023027791001369,\n",
       "   0.0006358870599797228,\n",
       "   0.0005807526578428223,\n",
       "   0.0005339792128506815,\n",
       "   0.0004939002857281594]},\n",
       " 3: {'val_loss': [0.0015579486300703138,\n",
       "   0.001181529380264692,\n",
       "   0.0009690728120040149,\n",
       "   0.0008285223145503551,\n",
       "   0.000727911215799395,\n",
       "   0.0006491011218167841,\n",
       "   0.0005863434707862325,\n",
       "   0.0005353357811691239,\n",
       "   0.0004919764085207134],\n",
       "  'loss': [0.003988753063604236,\n",
       "   0.0013445055815100204,\n",
       "   0.0010634577593009453,\n",
       "   0.0008918706160329748,\n",
       "   0.0007723885890300152,\n",
       "   0.0006822593278775457,\n",
       "   0.0006113916650792817,\n",
       "   0.0005537922927032924,\n",
       "   0.0005060072591732024]},\n",
       " 4: {'val_loss': [0.0012895489711081609,\n",
       "   0.000941830271622166,\n",
       "   0.0007533936470281333,\n",
       "   0.0006320719036739319,\n",
       "   0.0005456009806948714,\n",
       "   0.000482466813409701],\n",
       "  'loss': [0.002450065267039463,\n",
       "   0.0010795527663140091,\n",
       "   0.0008309614917379804,\n",
       "   0.0006816113634704379,\n",
       "   0.000580218068062095,\n",
       "   0.0005065394019766245]},\n",
       " 5: {'val_loss': [0.0016483995609451086,\n",
       "   0.0013507094531087205,\n",
       "   0.0011633641610387714,\n",
       "   0.0010379404248669743,\n",
       "   0.0009383414476178587,\n",
       "   0.0008603861188748852,\n",
       "   0.0007958431774750352,\n",
       "   0.0007419378432678059,\n",
       "   0.000695167352387216,\n",
       "   0.0006563472870038822,\n",
       "   0.0006213697529165074,\n",
       "   0.000590034970373381,\n",
       "   0.0005629489221610129,\n",
       "   0.0005381242418661714,\n",
       "   0.000515888005902525,\n",
       "   0.0004957496467977762],\n",
       "  'loss': [0.0033452581267920324,\n",
       "   0.001490476698090788,\n",
       "   0.001257408721576212,\n",
       "   0.0011036388168577105,\n",
       "   0.0009920677985064685,\n",
       "   0.0009043849482259247,\n",
       "   0.0008329620484437328,\n",
       "   0.0007732559023133945,\n",
       "   0.0007228298788686516,\n",
       "   0.0006794548832840519,\n",
       "   0.0006416528613044647,\n",
       "   0.0006084878725232557,\n",
       "   0.0005789480398379964,\n",
       "   0.0005525271657097619,\n",
       "   0.0005286996377981268,\n",
       "   0.0005070667610561941]},\n",
       " 6: {'val_loss': [0.001389823331264779,\n",
       "   0.0010666652180952951,\n",
       "   0.0008926197263645009,\n",
       "   0.0007780081749660894,\n",
       "   0.000696469901886303,\n",
       "   0.0006341878784587606,\n",
       "   0.000585185078671202,\n",
       "   0.0005450374461361207,\n",
       "   0.0005117250891635194,\n",
       "   0.000482845377700869],\n",
       "  'loss': [0.002361183568427805,\n",
       "   0.0011967584647936747,\n",
       "   0.0009624297871778253,\n",
       "   0.000821165942834341,\n",
       "   0.0007238464702095371,\n",
       "   0.0006525765472906642,\n",
       "   0.0005975133241008735,\n",
       "   0.0005534233328216942,\n",
       "   0.0005169383432075847,\n",
       "   0.0004865410958882421]},\n",
       " 7: {'val_loss': [0.0014336964534595608,\n",
       "   0.0011579110438469798,\n",
       "   0.0009987140615703538,\n",
       "   0.0008844782487722113,\n",
       "   0.0007993963826447725,\n",
       "   0.0007335328019689769,\n",
       "   0.0006816055678064004,\n",
       "   0.0006404161092359573,\n",
       "   0.0006055995268980041,\n",
       "   0.0005750634794821962,\n",
       "   0.0005494835230638273,\n",
       "   0.0005270055113942362,\n",
       "   0.0005074942225473933,\n",
       "   0.00049012667092029],\n",
       "  'loss': [0.0023641307165962644,\n",
       "   0.0012638713070191442,\n",
       "   0.0010562743362970651,\n",
       "   0.0009233679468161427,\n",
       "   0.0008260665211128071,\n",
       "   0.0007515906197659206,\n",
       "   0.0006932820742804324,\n",
       "   0.0006463107904710341,\n",
       "   0.0006076007118826965,\n",
       "   0.0005752720562304603,\n",
       "   0.0005474502981087426,\n",
       "   0.0005235477147653001,\n",
       "   0.0005025716912496137,\n",
       "   0.00048418810903967825]},\n",
       " 8: {'val_loss': [0.001419184819678776,\n",
       "   0.0011350396455964073,\n",
       "   0.0009789145149989053,\n",
       "   0.0008796832070220262,\n",
       "   0.000808147878269665,\n",
       "   0.000755127074662596,\n",
       "   0.0007111398107372224,\n",
       "   0.0006743578996974975,\n",
       "   0.0006451651241513901,\n",
       "   0.0006181191976065747,\n",
       "   0.0005968581768684089,\n",
       "   0.0005776771769160405,\n",
       "   0.000561027453513816,\n",
       "   0.0005467317419243045,\n",
       "   0.0005349762411788106,\n",
       "   0.0005227745720185339,\n",
       "   0.0005125926315668039,\n",
       "   0.0005031949569820426,\n",
       "   0.0004948144344962202],\n",
       "  'loss': [0.0022726046232855878,\n",
       "   0.0012816880736500026,\n",
       "   0.0010651146047166548,\n",
       "   0.0009367097305948846,\n",
       "   0.0008503703774476889,\n",
       "   0.0007865042772027664,\n",
       "   0.0007362521079630823,\n",
       "   0.0006949634126067394,\n",
       "   0.0006606641566031613,\n",
       "   0.0006318115285830572,\n",
       "   0.0006074151681968943,\n",
       "   0.0005866884876013501,\n",
       "   0.0005689081197488121,\n",
       "   0.0005534448762045941,\n",
       "   0.0005397535551310284,\n",
       "   0.0005275935830286471,\n",
       "   0.0005165279041830218,\n",
       "   0.0005065595940686763,\n",
       "   0.0004974756392766721]},\n",
       " 9: {'val_loss': [0.0016785999992862343,\n",
       "   0.0013921437918907032,\n",
       "   0.0012169435556279495,\n",
       "   0.0010987628943985328,\n",
       "   0.0010151795065030456,\n",
       "   0.0009544046496739611,\n",
       "   0.0009022171335527673,\n",
       "   0.0008607050572754815,\n",
       "   0.0008242546045221388,\n",
       "   0.0007915579376276583,\n",
       "   0.0007643203233601525,\n",
       "   0.0007380449282936752,\n",
       "   0.0007149413513252512,\n",
       "   0.0006950836538453586,\n",
       "   0.0006771992644644343,\n",
       "   0.0006597567274002358,\n",
       "   0.000644939235644415,\n",
       "   0.0006314595806179568,\n",
       "   0.0006197402265388518,\n",
       "   0.0006088176133926027,\n",
       "   0.0005981145784608088,\n",
       "   0.0005893435378675349,\n",
       "   0.0005811025277944282,\n",
       "   0.0005737516059889458,\n",
       "   0.0005664686142699793,\n",
       "   0.0005597006881725975,\n",
       "   0.0005538387157139369,\n",
       "   0.0005482284023310057,\n",
       "   0.0005434603014145978,\n",
       "   0.000537887952523306,\n",
       "   0.0005335569495218806,\n",
       "   0.0005294479435542598,\n",
       "   0.0005255678176763468,\n",
       "   0.0005222208000486716,\n",
       "   0.0005184159730561077,\n",
       "   0.0005158449229202233,\n",
       "   0.0005129773801309057,\n",
       "   0.0005102781983441673,\n",
       "   0.0005079656018642709,\n",
       "   0.0005059094779426232,\n",
       "   0.0005037024454213678,\n",
       "   0.000502156634174753,\n",
       "   0.0005002283191424795,\n",
       "   0.0004988268497982063],\n",
       "  'loss': [0.0026394399569835514,\n",
       "   0.0014990819311060477,\n",
       "   0.0012772669314290398,\n",
       "   0.0011365045861748513,\n",
       "   0.0010383475488924888,\n",
       "   0.0009662506885069888,\n",
       "   0.0009103922802023589,\n",
       "   0.00086465223270352,\n",
       "   0.0008263211853045504,\n",
       "   0.0007929783234430943,\n",
       "   0.0007635165370447794,\n",
       "   0.0007374259013158735,\n",
       "   0.0007138898971606977,\n",
       "   0.0006930010978976498,\n",
       "   0.0006740945095225471,\n",
       "   0.0006570235435356153,\n",
       "   0.0006415933599782875,\n",
       "   0.0006278013672999805,\n",
       "   0.0006152514686255017,\n",
       "   0.0006039907944796141,\n",
       "   0.0005935940148629015,\n",
       "   0.0005841812243306776,\n",
       "   0.0005755219578713877,\n",
       "   0.0005675798992160708,\n",
       "   0.0005601596261112718,\n",
       "   0.0005532181875605601,\n",
       "   0.0005469539597834227,\n",
       "   0.0005410397576633841,\n",
       "   0.0005354499847453554,\n",
       "   0.0005303544074558886,\n",
       "   0.0005255245435546386,\n",
       "   0.0005210516697115963,\n",
       "   0.0005169319186097709,\n",
       "   0.0005131191465625307,\n",
       "   0.0005095147043903125,\n",
       "   0.0005061997726443224,\n",
       "   0.0005031572978623444,\n",
       "   0.0005003528593078954,\n",
       "   0.0004977235902333632,\n",
       "   0.0004953595867118565,\n",
       "   0.0004931822649086826,\n",
       "   0.000491133838731912,\n",
       "   0.0004893072048434987,\n",
       "   0.00048760362107714173]},\n",
       " 10: {'val_loss': [0.0015040038025472312,\n",
       "   0.0012324170727515593,\n",
       "   0.0010932204901473598,\n",
       "   0.0009966131101828068,\n",
       "   0.0009247937379404902,\n",
       "   0.0008696959627559408,\n",
       "   0.0008260747080203146,\n",
       "   0.0007902908255346119,\n",
       "   0.0007625634525902569,\n",
       "   0.0007374137575970963,\n",
       "   0.0007166367187164724,\n",
       "   0.000698612320702523,\n",
       "   0.0006826781906420365,\n",
       "   0.0006686334434198216,\n",
       "   0.0006555423626559787,\n",
       "   0.0006442161259474233,\n",
       "   0.0006332518553244881,\n",
       "   0.0006233482071547769,\n",
       "   0.00061451618821593,\n",
       "   0.0006070661862031556,\n",
       "   0.0005998061964055523,\n",
       "   0.0005939902510726825,\n",
       "   0.0005884438438806683,\n",
       "   0.0005836911970982328,\n",
       "   0.0005795878020580858,\n",
       "   0.0005762157772551291,\n",
       "   0.0005733719881391153,\n",
       "   0.0005704026090097614,\n",
       "   0.0005683875945396721,\n",
       "   0.000565936102066189,\n",
       "   0.000564236099307891,\n",
       "   0.0005621887155575678,\n",
       "   0.0005611005504033528,\n",
       "   0.0005597318758373148,\n",
       "   0.0005585809581680223,\n",
       "   0.0005574407396488823,\n",
       "   0.0005564740745467134,\n",
       "   0.0005555573792662471,\n",
       "   0.0005544399278005585,\n",
       "   0.0005535132298246027,\n",
       "   0.0005529864897835068,\n",
       "   0.0005521245778072625,\n",
       "   0.0005516910483129322,\n",
       "   0.0005511296883923932,\n",
       "   0.0005506074134609663,\n",
       "   0.0005501006959821097,\n",
       "   0.0005491057943436317,\n",
       "   0.0005489312636200339,\n",
       "   0.0005484711218741722,\n",
       "   0.0005482209514593706,\n",
       "   0.0005478285191929899,\n",
       "   0.0005470519134541974,\n",
       "   0.0005466504470678046,\n",
       "   0.0005464735405985266,\n",
       "   0.0005465971230296418,\n",
       "   0.000545911577064544,\n",
       "   0.0005457662837579846,\n",
       "   0.0005449709473759868,\n",
       "   0.0005448686311137862,\n",
       "   0.0005445274757221342,\n",
       "   0.0005443091911729425,\n",
       "   0.0005440682964399457,\n",
       "   0.0005437768471892923,\n",
       "   0.0005438595911255107,\n",
       "   0.0005432910250965506,\n",
       "   0.0005429263346013613,\n",
       "   0.0005432312944321893,\n",
       "   0.0005431444392888806,\n",
       "   0.0005425262998323887,\n",
       "   0.0005423095548758284,\n",
       "   0.0005420509920804761,\n",
       "   0.0005422097651171499,\n",
       "   0.0005417439716984518,\n",
       "   0.0005415607133181766,\n",
       "   0.0005415621108841151,\n",
       "   0.0005411273983190767,\n",
       "   0.0005411754921078682,\n",
       "   0.0005409314917051234,\n",
       "   0.000540730859793257,\n",
       "   0.0005403760293847881,\n",
       "   0.0005406499697710388,\n",
       "   0.0005401662655640393,\n",
       "   0.0005401082077878528,\n",
       "   0.0005401640152558684,\n",
       "   0.0005399046320235356,\n",
       "   0.0005398846699972637,\n",
       "   0.0005397322808858007,\n",
       "   0.000539742378750816,\n",
       "   0.0005393442921922542,\n",
       "   0.0005390778274158947,\n",
       "   0.000539045579789672,\n",
       "   0.000539177319733426,\n",
       "   0.0005388114406378008,\n",
       "   0.0005388235216378234,\n",
       "   0.0005387932999292389,\n",
       "   0.00053856552724028,\n",
       "   0.0005385601881425828,\n",
       "   0.0005382828618166968,\n",
       "   0.0005382214166456833,\n",
       "   0.0005381774337729439,\n",
       "   0.0005381085220142268,\n",
       "   0.0005381620995467529,\n",
       "   0.0005378117927466519,\n",
       "   0.0005377163752564229,\n",
       "   0.0005378814187133685,\n",
       "   0.0005377410046639852,\n",
       "   0.0005376850493485108,\n",
       "   0.0005375774664571509,\n",
       "   0.0005375658319098875,\n",
       "   0.0005374815833056345,\n",
       "   0.0005375547375297174,\n",
       "   0.0005373790391604416,\n",
       "   0.0005374404220492579,\n",
       "   0.0005373231103294529,\n",
       "   0.0005371668236330151,\n",
       "   0.0005372565632569603,\n",
       "   0.0005369511793833226,\n",
       "   0.0005367786015267483,\n",
       "   0.0005367277300683781,\n",
       "   0.0005366917070932687,\n",
       "   0.0005366878191125579,\n",
       "   0.000536603023938369,\n",
       "   0.0005365565590909682,\n",
       "   0.0005363905552076176,\n",
       "   0.0005363786497036927,\n",
       "   0.000536337113881018,\n",
       "   0.0005364491252112202,\n",
       "   0.0005362840541056357,\n",
       "   0.0005363437684718519,\n",
       "   0.0005360527164884843,\n",
       "   0.0005363104672869667,\n",
       "   0.0005362954648444429,\n",
       "   0.0005360955413198098,\n",
       "   0.0005358295695623383,\n",
       "   0.000535783605009783,\n",
       "   0.0005360025804839097,\n",
       "   0.0005358849241747521,\n",
       "   0.0005359013713314198,\n",
       "   0.000536003727465868,\n",
       "   0.0005356898705940694,\n",
       "   0.0005358671024441719,\n",
       "   0.0005357205419568345,\n",
       "   0.00053549294534605,\n",
       "   0.0005353433417622,\n",
       "   0.0005354150183848105,\n",
       "   0.0005354172957595438,\n",
       "   0.0005353116683545523,\n",
       "   0.0005354010363225825,\n",
       "   0.0005353305937023834,\n",
       "   0.0005354618109413423,\n",
       "   0.0005353835434652865,\n",
       "   0.0005353082029614598,\n",
       "   0.0005352759716333821,\n",
       "   0.0005351826208061538,\n",
       "   0.0005351863589021377,\n",
       "   0.000535199380828999,\n",
       "   0.0005350700768758543,\n",
       "   0.0005352425322053023,\n",
       "   0.0005351438096840866,\n",
       "   0.0005349713994655758,\n",
       "   0.0005350249315961265,\n",
       "   0.0005348229149240069,\n",
       "   0.0005350738466950133,\n",
       "   0.0005348566701286472,\n",
       "   0.0005349451885558664,\n",
       "   0.0005348255261196754,\n",
       "   0.0005348166529438458,\n",
       "   0.0005347326371702366,\n",
       "   0.0005347520581563003,\n",
       "   0.000534584132838063,\n",
       "   0.0005347620093380101,\n",
       "   0.0005347138061188162,\n",
       "   0.0005348104867152869,\n",
       "   0.0005348483225679956,\n",
       "   0.0005346044429461471,\n",
       "   0.0005346413803636096],\n",
       "  'loss': [0.0023111940472153947,\n",
       "   0.0013576850350364112,\n",
       "   0.0011635014205239713,\n",
       "   0.001045175170584116,\n",
       "   0.0009597378353646491,\n",
       "   0.0008952364191645757,\n",
       "   0.0008448457439226331,\n",
       "   0.0008047810899734031,\n",
       "   0.000771734234149335,\n",
       "   0.0007445293511409545,\n",
       "   0.00072170580009697,\n",
       "   0.0007017081930098357,\n",
       "   0.0006844008433108683,\n",
       "   0.0006690223261830397,\n",
       "   0.0006550393519864883,\n",
       "   0.0006423888696008361,\n",
       "   0.0006309199061797699,\n",
       "   0.0006203298030595761,\n",
       "   0.0006109143054345622,\n",
       "   0.0006023876892868429,\n",
       "   0.0005947809864301234,\n",
       "   0.0005882340976677369,\n",
       "   0.0005823467357549817,\n",
       "   0.0005772325069847284,\n",
       "   0.0005727369892701972,\n",
       "   0.0005689148778037634,\n",
       "   0.0005654400971980066,\n",
       "   0.0005626711848890409,\n",
       "   0.0005599765895021847,\n",
       "   0.0005577607369195903,\n",
       "   0.0005556692076788749,\n",
       "   0.0005539713691541693,\n",
       "   0.0005523589855874889,\n",
       "   0.0005509099925257033,\n",
       "   0.0005495881802926306,\n",
       "   0.0005484841024008347,\n",
       "   0.0005474625997885596,\n",
       "   0.000546432565242867,\n",
       "   0.0005455938729573973,\n",
       "   0.0005447884933528258,\n",
       "   0.0005440472553891596,\n",
       "   0.0005432947228837293,\n",
       "   0.0005426053301926004,\n",
       "   0.0005420180968940258,\n",
       "   0.0005414764680608641,\n",
       "   0.0005409417748887791,\n",
       "   0.0005404213687870651,\n",
       "   0.000539881146105472,\n",
       "   0.0005394435934431385,\n",
       "   0.0005389977434970206,\n",
       "   0.0005385951131029287,\n",
       "   0.000538168306229636,\n",
       "   0.0005378563938575098,\n",
       "   0.0005374652062164387,\n",
       "   0.0005371605011896463,\n",
       "   0.0005367707059485838,\n",
       "   0.0005365079669718398,\n",
       "   0.0005362123608938418,\n",
       "   0.0005359234250499867,\n",
       "   0.000535632872779388,\n",
       "   0.0005353715015371563,\n",
       "   0.0005351413103926461,\n",
       "   0.0005348639412113698,\n",
       "   0.0005346039691357874,\n",
       "   0.0005344301585137145,\n",
       "   0.0005341291777585865,\n",
       "   0.0005338779569137842,\n",
       "   0.0005336885510041612,\n",
       "   0.000533582760981517,\n",
       "   0.0005333488797623432,\n",
       "   0.0005331650392326992,\n",
       "   0.0005328917068254668,\n",
       "   0.0005327937450056197,\n",
       "   0.0005325973252911354,\n",
       "   0.0005323838353069732,\n",
       "   0.0005322546089155367,\n",
       "   0.0005321410395845305,\n",
       "   0.0005318937452102546,\n",
       "   0.0005318287182308267,\n",
       "   0.0005316385104379151,\n",
       "   0.0005314700707094743,\n",
       "   0.0005313390849914868,\n",
       "   0.0005311883570539067,\n",
       "   0.0005310494299919811,\n",
       "   0.0005309392305935034,\n",
       "   0.0005308108267490752,\n",
       "   0.0005306938783178339,\n",
       "   0.0005305074455827707,\n",
       "   0.0005304192463518121,\n",
       "   0.0005303528013610048,\n",
       "   0.0005301934716408141,\n",
       "   0.0005301049241097644,\n",
       "   0.0005299601014849032,\n",
       "   0.0005298595398926409,\n",
       "   0.0005297521747706924,\n",
       "   0.0005296729670226341,\n",
       "   0.0005295760802255245,\n",
       "   0.0005294610747660044,\n",
       "   0.0005293670592072886,\n",
       "   0.000529260836992762,\n",
       "   0.0005291516709257848,\n",
       "   0.0005290373497700785,\n",
       "   0.0005289518667996163,\n",
       "   0.000528912517693243,\n",
       "   0.0005287944815063383,\n",
       "   0.0005287764168315334,\n",
       "   0.0005286501389491604,\n",
       "   0.0005285158288461389,\n",
       "   0.0005284794168983353,\n",
       "   0.000528407311430783,\n",
       "   0.0005283169741596794,\n",
       "   0.0005282561255444307,\n",
       "   0.0005281632117112167,\n",
       "   0.0005280767492513405,\n",
       "   0.0005280261050211266,\n",
       "   0.0005279533726570662,\n",
       "   0.0005279033422266366,\n",
       "   0.000527846719705849,\n",
       "   0.0005277762069454184,\n",
       "   0.0005277264191681752,\n",
       "   0.0005276154199964367,\n",
       "   0.0005275782142416574,\n",
       "   0.0005274766372167505,\n",
       "   0.0005274296686548041,\n",
       "   0.000527356921666069,\n",
       "   0.000527314039572957,\n",
       "   0.0005272865970619023,\n",
       "   0.0005272070365754189,\n",
       "   0.000527106627705507,\n",
       "   0.0005270857723371591,\n",
       "   0.0005270501349150436,\n",
       "   0.0005270228325389326,\n",
       "   0.0005269180269533535,\n",
       "   0.0005268779902689858,\n",
       "   0.0005268464491382474,\n",
       "   0.0005268021437223069,\n",
       "   0.0005267246427683858,\n",
       "   0.0005267114271555329,\n",
       "   0.0005266360282257665,\n",
       "   0.0005266075159306638,\n",
       "   0.0005265753804269479,\n",
       "   0.0005265393271838548,\n",
       "   0.0005264781689766095,\n",
       "   0.0005264106854883721,\n",
       "   0.0005263768223812803,\n",
       "   0.0005263575147546362,\n",
       "   0.0005263027607725235,\n",
       "   0.0005262429407594027,\n",
       "   0.0005262106855661842,\n",
       "   0.000526175646300544,\n",
       "   0.0005261876231088536,\n",
       "   0.0005260575312422589,\n",
       "   0.0005260791894397698,\n",
       "   0.0005260392218042398,\n",
       "   0.0005260044974420453,\n",
       "   0.0005259408060373971,\n",
       "   0.000525912033408531,\n",
       "   0.0005258906949893572,\n",
       "   0.0005258555008185794,\n",
       "   0.000525801337134908,\n",
       "   0.00052576028254407,\n",
       "   0.0005258159071672708,\n",
       "   0.0005257002433063462,\n",
       "   0.0005256786635436583,\n",
       "   0.0005256293706042925,\n",
       "   0.0005256322226341581,\n",
       "   0.0005255895537266042,\n",
       "   0.0005255825327185449,\n",
       "   0.0005255764220783021,\n",
       "   0.0005255218504316872,\n",
       "   0.0005254521780443611,\n",
       "   0.0005254489329672652,\n",
       "   0.0005254155494912994,\n",
       "   0.0005254139091266552,\n",
       "   0.0005253722142515471,\n",
       "   0.0005253377358167199]}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0002\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input-layer (InputLayer)     (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "hidden-layer (Dense)         (None, 75)                22575     \n",
      "_________________________________________________________________\n",
      "output-layer (Dense)         (None, 300)               22800     \n",
      "=================================================================\n",
      "Total params: 45,375\n",
      "Trainable params: 45,375\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/500\n",
      "12800/12800 [==============================] - 2s 150us/step - loss: 0.0017 - val_loss: 9.7809e-04\n",
      "Epoch 2/500\n",
      "12800/12800 [==============================] - 1s 72us/step - loss: 8.0798e-04 - val_loss: 6.5922e-04\n",
      "Epoch 3/500\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 5.8197e-04 - val_loss: 5.0718e-04\n",
      "Epoch 4/500\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 4.6008e-04 - val_loss: 4.1378e-04\n",
      "Epoch 5/500\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 3.8245e-04 - val_loss: 3.5089e-04\n",
      "Epoch 6/500\n",
      "12800/12800 [==============================] - 1s 72us/step - loss: 3.2913e-04 - val_loss: 3.0847e-04\n",
      "Epoch 7/500\n",
      "12800/12800 [==============================] - 1s 72us/step - loss: 2.9088e-04 - val_loss: 2.7472e-04\n",
      "Epoch 8/500\n",
      "12800/12800 [==============================] - 1s 72us/step - loss: 2.6233e-04 - val_loss: 2.4999e-04\n",
      "Epoch 9/500\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 2.4016e-04 - val_loss: 2.3063e-04\n",
      "Epoch 10/500\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 2.2260e-04 - val_loss: 2.1531e-04\n",
      "Epoch 11/500\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 2.0830e-04 - val_loss: 2.0176e-04\n",
      "Epoch 12/500\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 1.9635e-04 - val_loss: 1.9052e-04\n",
      "Test MSE: 0.00019366361\n",
      "Total MSE: 0.00019080349\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input-layer (InputLayer)     (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "hidden-layer-1 (Dense)       (None, 150)               45150     \n",
      "_________________________________________________________________\n",
      "hidden-layer-2 (Dense)       (None, 75)                11325     \n",
      "_________________________________________________________________\n",
      "hidden-layer-3 (Dense)       (None, 150)               11400     \n",
      "_________________________________________________________________\n",
      "output-layer (Dense)         (None, 300)               45300     \n",
      "=================================================================\n",
      "Total params: 113,175\n",
      "Trainable params: 113,175\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/500\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.0029 - val_loss: 0.0014\n",
      "Epoch 2/500\n",
      "12800/12800 [==============================] - 1s 97us/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 3/500\n",
      "12800/12800 [==============================] - 1s 96us/step - loss: 9.7447e-04 - val_loss: 8.7532e-04\n",
      "Epoch 4/500\n",
      "12800/12800 [==============================] - 1s 113us/step - loss: 8.1425e-04 - val_loss: 7.4577e-04\n",
      "Epoch 5/500\n",
      "12800/12800 [==============================] - 1s 114us/step - loss: 7.0080e-04 - val_loss: 6.5088e-04\n",
      "Epoch 6/500\n",
      "12800/12800 [==============================] - 1s 101us/step - loss: 6.1505e-04 - val_loss: 5.7683e-04\n",
      "Epoch 7/500\n",
      "12800/12800 [==============================] - 1s 104us/step - loss: 5.4825e-04 - val_loss: 5.1805e-04\n",
      "Epoch 8/500\n",
      "12800/12800 [==============================] - 1s 114us/step - loss: 4.9472e-04 - val_loss: 4.7197e-04\n",
      "Epoch 9/500\n",
      "12800/12800 [==============================] - 1s 101us/step - loss: 4.5129e-04 - val_loss: 4.3188e-04\n",
      "Epoch 10/500\n",
      "12800/12800 [==============================] - 1s 112us/step - loss: 4.1536e-04 - val_loss: 4.0043e-04\n",
      "Epoch 11/500\n",
      "12800/12800 [==============================] - 2s 120us/step - loss: 3.8539e-04 - val_loss: 3.7224e-04\n",
      "Epoch 12/500\n",
      "12800/12800 [==============================] - 1s 115us/step - loss: 3.5992e-04 - val_loss: 3.4930e-04\n",
      "Epoch 13/500\n",
      "12800/12800 [==============================] - 2s 121us/step - loss: 3.3815e-04 - val_loss: 3.2902e-04\n",
      "Epoch 14/500\n",
      "12800/12800 [==============================] - 1s 108us/step - loss: 3.1923e-04 - val_loss: 3.1179e-04\n",
      "Epoch 15/500\n",
      "12800/12800 [==============================] - 1s 103us/step - loss: 3.0275e-04 - val_loss: 2.9626e-04\n",
      "Epoch 16/500\n",
      "12800/12800 [==============================] - 1s 108us/step - loss: 2.8820e-04 - val_loss: 2.8253e-04\n",
      "Epoch 17/500\n",
      "12800/12800 [==============================] - 1s 117us/step - loss: 2.7533e-04 - val_loss: 2.7046e-04\n",
      "Epoch 18/500\n",
      "12800/12800 [==============================] - 1s 112us/step - loss: 2.6382e-04 - val_loss: 2.5959e-04\n",
      "Epoch 19/500\n",
      "12800/12800 [==============================] - 1s 112us/step - loss: 2.5350e-04 - val_loss: 2.5035e-04\n",
      "Epoch 20/500\n",
      "12800/12800 [==============================] - 2s 120us/step - loss: 2.4417e-04 - val_loss: 2.4155e-04\n",
      "Epoch 21/500\n",
      "12800/12800 [==============================] - 1s 115us/step - loss: 2.3570e-04 - val_loss: 2.3333e-04\n",
      "Epoch 22/500\n",
      "12800/12800 [==============================] - 1s 104us/step - loss: 2.2801e-04 - val_loss: 2.2656e-04\n",
      "Epoch 23/500\n",
      "12800/12800 [==============================] - 1s 110us/step - loss: 2.2096e-04 - val_loss: 2.1935e-04\n",
      "Epoch 24/500\n",
      "12800/12800 [==============================] - 2s 120us/step - loss: 2.1444e-04 - val_loss: 2.1322e-04\n",
      "Epoch 25/500\n",
      "12800/12800 [==============================] - 2s 132us/step - loss: 2.0846e-04 - val_loss: 2.0771e-04\n",
      "Epoch 26/500\n",
      "12800/12800 [==============================] - 2s 122us/step - loss: 2.0288e-04 - val_loss: 2.0227e-04\n",
      "Epoch 27/500\n",
      "12800/12800 [==============================] - 2s 181us/step - loss: 1.9769e-04 - val_loss: 1.9717e-04\n",
      "Test MSE: 0.0002003628\n",
      "Total MSE: 0.00019610822\n",
      "0.00030000000000000003\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input-layer (InputLayer)     (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "hidden-layer (Dense)         (None, 75)                22575     \n",
      "_________________________________________________________________\n",
      "output-layer (Dense)         (None, 300)               22800     \n",
      "=================================================================\n",
      "Total params: 45,375\n",
      "Trainable params: 45,375\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/500\n",
      "12800/12800 [==============================] - 2s 138us/step - loss: 0.0016 - val_loss: 9.3709e-04\n",
      "Epoch 2/500\n",
      "12800/12800 [==============================] - 1s 100us/step - loss: 7.4960e-04 - val_loss: 6.2359e-04\n",
      "Epoch 3/500\n",
      "12800/12800 [==============================] - 1s 98us/step - loss: 5.3526e-04 - val_loss: 4.7700e-04\n",
      "Epoch 4/500\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 4.2237e-04 - val_loss: 3.8866e-04\n",
      "Epoch 5/500\n",
      "12800/12800 [==============================] - 1s 97us/step - loss: 3.5178e-04 - val_loss: 3.3064e-04\n",
      "Epoch 6/500\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 3.0374e-04 - val_loss: 2.9014e-04\n",
      "Test MSE: 0.00029251185\n",
      "Total MSE: 0.00028611921\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input-layer (InputLayer)     (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "hidden-layer-1 (Dense)       (None, 150)               45150     \n",
      "_________________________________________________________________\n",
      "hidden-layer-2 (Dense)       (None, 75)                11325     \n",
      "_________________________________________________________________\n",
      "hidden-layer-3 (Dense)       (None, 150)               11400     \n",
      "_________________________________________________________________\n",
      "output-layer (Dense)         (None, 300)               45300     \n",
      "=================================================================\n",
      "Total params: 113,175\n",
      "Trainable params: 113,175\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/500\n",
      "12800/12800 [==============================] - 3s 268us/step - loss: 0.0034 - val_loss: 0.0016\n",
      "Epoch 2/500\n",
      "12800/12800 [==============================] - 1s 111us/step - loss: 0.0014 - val_loss: 0.0013\n",
      "Epoch 3/500\n",
      "12800/12800 [==============================] - 1s 114us/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 4/500\n",
      "12800/12800 [==============================] - 2s 119us/step - loss: 9.8506e-04 - val_loss: 9.2282e-04\n",
      "Epoch 5/500\n",
      "12800/12800 [==============================] - 1s 110us/step - loss: 8.6783e-04 - val_loss: 8.2282e-04\n",
      "Epoch 6/500\n",
      "12800/12800 [==============================] - 1s 106us/step - loss: 7.7748e-04 - val_loss: 7.4194e-04\n",
      "Epoch 7/500\n",
      "12800/12800 [==============================] - 1s 111us/step - loss: 7.0534e-04 - val_loss: 6.7681e-04\n",
      "Epoch 8/500\n",
      "12800/12800 [==============================] - 1s 108us/step - loss: 6.4637e-04 - val_loss: 6.2348e-04\n",
      "Epoch 9/500\n",
      "12800/12800 [==============================] - 1s 113us/step - loss: 5.9724e-04 - val_loss: 5.7902e-04\n",
      "Epoch 10/500\n",
      "12800/12800 [==============================] - 1s 109us/step - loss: 5.5567e-04 - val_loss: 5.4110e-04\n",
      "Epoch 11/500\n",
      "12800/12800 [==============================] - 1s 117us/step - loss: 5.1996e-04 - val_loss: 5.0822e-04\n",
      "Epoch 12/500\n",
      "12800/12800 [==============================] - 2s 126us/step - loss: 4.8888e-04 - val_loss: 4.7868e-04\n",
      "Epoch 13/500\n",
      "12800/12800 [==============================] - 1s 110us/step - loss: 4.6154e-04 - val_loss: 4.5336e-04\n",
      "Epoch 14/500\n",
      "12800/12800 [==============================] - 1s 115us/step - loss: 4.3730e-04 - val_loss: 4.3070e-04\n",
      "Epoch 15/500\n",
      "12800/12800 [==============================] - 1s 111us/step - loss: 4.1582e-04 - val_loss: 4.0993e-04\n",
      "Epoch 16/500\n",
      "12800/12800 [==============================] - 1s 107us/step - loss: 3.9641e-04 - val_loss: 3.9261e-04\n",
      "Epoch 17/500\n",
      "12800/12800 [==============================] - 1s 110us/step - loss: 3.7899e-04 - val_loss: 3.7580e-04\n",
      "Epoch 18/500\n",
      "12800/12800 [==============================] - 2s 124us/step - loss: 3.6318e-04 - val_loss: 3.6054e-04\n",
      "Epoch 19/500\n",
      "12800/12800 [==============================] - 2s 128us/step - loss: 3.4880e-04 - val_loss: 3.4690e-04\n",
      "Epoch 20/500\n",
      "12800/12800 [==============================] - 1s 116us/step - loss: 3.3566e-04 - val_loss: 3.3437e-04\n",
      "Epoch 21/500\n",
      "12800/12800 [==============================] - 2s 119us/step - loss: 3.2364e-04 - val_loss: 3.2253e-04\n",
      "Epoch 22/500\n",
      "12800/12800 [==============================] - 1s 112us/step - loss: 3.1257e-04 - val_loss: 3.1185e-04\n",
      "Epoch 23/500\n",
      "12800/12800 [==============================] - 1s 112us/step - loss: 3.0236e-04 - val_loss: 3.0208e-04\n",
      "Epoch 24/500\n",
      "12800/12800 [==============================] - 1s 115us/step - loss: 2.9290e-04 - val_loss: 2.9290e-04\n",
      "Test MSE: 0.00028221758\n",
      "Total MSE: 0.00028740131\n",
      "0.0004\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input-layer (InputLayer)     (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "hidden-layer (Dense)         (None, 75)                22575     \n",
      "_________________________________________________________________\n",
      "output-layer (Dense)         (None, 300)               22800     \n",
      "=================================================================\n",
      "Total params: 45,375\n",
      "Trainable params: 45,375\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/500\n",
      "12800/12800 [==============================] - 2s 157us/step - loss: 0.0015 - val_loss: 8.4384e-04\n",
      "Epoch 2/500\n",
      "12800/12800 [==============================] - 1s 100us/step - loss: 6.7905e-04 - val_loss: 5.5985e-04\n",
      "Epoch 3/500\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 4.8199e-04 - val_loss: 4.2179e-04\n",
      "Epoch 4/500\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 3.7727e-04 - val_loss: 3.4190e-04\n",
      "Test MSE: 0.0003494569\n",
      "Total MSE: 0.00034104601\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input-layer (InputLayer)     (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "hidden-layer-1 (Dense)       (None, 150)               45150     \n",
      "_________________________________________________________________\n",
      "hidden-layer-2 (Dense)       (None, 75)                11325     \n",
      "_________________________________________________________________\n",
      "hidden-layer-3 (Dense)       (None, 150)               11400     \n",
      "_________________________________________________________________\n",
      "output-layer (Dense)         (None, 300)               45300     \n",
      "=================================================================\n",
      "Total params: 113,175\n",
      "Trainable params: 113,175\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/500\n",
      "12800/12800 [==============================] - 2s 180us/step - loss: 0.0024 - val_loss: 0.0013\n",
      "Epoch 2/500\n",
      "12800/12800 [==============================] - 2s 136us/step - loss: 0.0011 - val_loss: 9.3733e-04\n",
      "Epoch 3/500\n",
      "12800/12800 [==============================] - 2s 130us/step - loss: 8.1806e-04 - val_loss: 7.4984e-04\n",
      "Epoch 4/500\n",
      "12800/12800 [==============================] - 1s 113us/step - loss: 6.6900e-04 - val_loss: 6.2486e-04\n",
      "Epoch 5/500\n",
      "12800/12800 [==============================] - 1s 112us/step - loss: 5.6795e-04 - val_loss: 5.3929e-04\n",
      "Epoch 6/500\n",
      "12800/12800 [==============================] - 1s 110us/step - loss: 4.9505e-04 - val_loss: 4.7555e-04\n",
      "Epoch 7/500\n",
      "12800/12800 [==============================] - 1s 107us/step - loss: 4.3996e-04 - val_loss: 4.2654e-04\n",
      "Epoch 8/500\n",
      "12800/12800 [==============================] - 1s 116us/step - loss: 3.9676e-04 - val_loss: 3.8641e-04\n",
      "Test MSE: 0.00039504847\n",
      "Total MSE: 0.0003815204\n",
      "0.0005\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input-layer (InputLayer)     (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "hidden-layer (Dense)         (None, 75)                22575     \n",
      "_________________________________________________________________\n",
      "output-layer (Dense)         (None, 300)               22800     \n",
      "=================================================================\n",
      "Total params: 45,375\n",
      "Trainable params: 45,375\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/500\n",
      "12800/12800 [==============================] - 2s 153us/step - loss: 0.0016 - val_loss: 9.0933e-04\n",
      "Epoch 2/500\n",
      "12800/12800 [==============================] - 1s 88us/step - loss: 7.0731e-04 - val_loss: 6.0559e-04\n",
      "Epoch 3/500\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 5.0096e-04 - val_loss: 4.5883e-04\n",
      "Test MSE: 0.00042691619\n",
      "Total MSE: 0.00043700005\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input-layer (InputLayer)     (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "hidden-layer-1 (Dense)       (None, 150)               45150     \n",
      "_________________________________________________________________\n",
      "hidden-layer-2 (Dense)       (None, 75)                11325     \n",
      "_________________________________________________________________\n",
      "hidden-layer-3 (Dense)       (None, 150)               11400     \n",
      "_________________________________________________________________\n",
      "output-layer (Dense)         (None, 300)               45300     \n",
      "=================================================================\n",
      "Total params: 113,175\n",
      "Trainable params: 113,175\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/500\n",
      "12800/12800 [==============================] - 4s 300us/step - loss: 0.0033 - val_loss: 0.0015\n",
      "Epoch 2/500\n",
      "12800/12800 [==============================] - 2s 140us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 3/500\n",
      "12800/12800 [==============================] - 2s 161us/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 4/500\n",
      "12800/12800 [==============================] - 2s 123us/step - loss: 9.2237e-04 - val_loss: 8.7937e-04\n",
      "Epoch 5/500\n",
      "12800/12800 [==============================] - 1s 116us/step - loss: 8.0932e-04 - val_loss: 7.8264e-04\n",
      "Epoch 6/500\n",
      "12800/12800 [==============================] - 2s 120us/step - loss: 7.2525e-04 - val_loss: 7.0821e-04\n",
      "Epoch 7/500\n",
      "12800/12800 [==============================] - 2s 123us/step - loss: 6.5935e-04 - val_loss: 6.4647e-04\n",
      "Epoch 8/500\n",
      "12800/12800 [==============================] - 2s 120us/step - loss: 6.0545e-04 - val_loss: 5.9663e-04\n",
      "Epoch 9/500\n",
      "12800/12800 [==============================] - 1s 116us/step - loss: 5.6051e-04 - val_loss: 5.5390e-04\n",
      "Epoch 10/500\n",
      "12800/12800 [==============================] - 2s 124us/step - loss: 5.2234e-04 - val_loss: 5.1734e-04\n",
      "Epoch 11/500\n",
      "12800/12800 [==============================] - 2s 137us/step - loss: 4.8916e-04 - val_loss: 4.8606e-04\n",
      "Test MSE: 0.0004910276\n",
      "Total MSE: 0.00047886615\n",
      "0.0006000000000000001\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input-layer (InputLayer)     (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "hidden-layer (Dense)         (None, 75)                22575     \n",
      "_________________________________________________________________\n",
      "output-layer (Dense)         (None, 300)               22800     \n",
      "=================================================================\n",
      "Total params: 45,375\n",
      "Trainable params: 45,375\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/500\n",
      "12800/12800 [==============================] - 2s 184us/step - loss: 0.0016 - val_loss: 9.2948e-04\n",
      "Epoch 2/500\n",
      "12800/12800 [==============================] - 1s 116us/step - loss: 7.2678e-04 - val_loss: 6.1217e-04\n",
      "Epoch 3/500\n",
      "12800/12800 [==============================] - 2s 130us/step - loss: 5.1563e-04 - val_loss: 4.6532e-04\n",
      "Test MSE: 0.000456381\n",
      "Total MSE: 0.00045253325\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input-layer (InputLayer)     (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "hidden-layer-1 (Dense)       (None, 150)               45150     \n",
      "_________________________________________________________________\n",
      "hidden-layer-2 (Dense)       (None, 75)                11325     \n",
      "_________________________________________________________________\n",
      "hidden-layer-3 (Dense)       (None, 150)               11400     \n",
      "_________________________________________________________________\n",
      "output-layer (Dense)         (None, 300)               45300     \n",
      "=================================================================\n",
      "Total params: 113,175\n",
      "Trainable params: 113,175\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/500\n",
      "12800/12800 [==============================] - 3s 205us/step - loss: 0.0029 - val_loss: 0.0014\n",
      "Epoch 2/500\n",
      "12800/12800 [==============================] - 2s 136us/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 3/500\n",
      "12800/12800 [==============================] - 2s 121us/step - loss: 9.7726e-04 - val_loss: 9.0424e-04\n",
      "Epoch 4/500\n",
      "12800/12800 [==============================] - 2s 124us/step - loss: 8.2319e-04 - val_loss: 7.7704e-04\n",
      "Epoch 5/500\n",
      "12800/12800 [==============================] - 1s 117us/step - loss: 7.1578e-04 - val_loss: 6.8451e-04\n",
      "Epoch 6/500\n",
      "12800/12800 [==============================] - 1s 112us/step - loss: 6.3529e-04 - val_loss: 6.1338e-04\n",
      "Epoch 7/500\n",
      "12800/12800 [==============================] - 1s 115us/step - loss: 5.7244e-04 - val_loss: 5.5687e-04\n",
      "Test MSE: 0.00056073495\n",
      "Total MSE: 0.00054937002\n",
      "0.0007\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input-layer (InputLayer)     (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "hidden-layer (Dense)         (None, 75)                22575     \n",
      "_________________________________________________________________\n",
      "output-layer (Dense)         (None, 300)               22800     \n",
      "=================================================================\n",
      "Total params: 45,375\n",
      "Trainable params: 45,375\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/500\n",
      "12800/12800 [==============================] - 2s 163us/step - loss: 0.0018 - val_loss: 0.0011\n",
      "Epoch 2/500\n",
      "12800/12800 [==============================] - 1s 94us/step - loss: 8.5131e-04 - val_loss: 7.2621e-04\n",
      "Epoch 3/500\n",
      "12800/12800 [==============================] - 1s 98us/step - loss: 6.2540e-04 - val_loss: 5.6340e-04\n",
      "Test MSE: 0.00057601852\n",
      "Total MSE: 0.00055784954\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input-layer (InputLayer)     (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "hidden-layer-1 (Dense)       (None, 150)               45150     \n",
      "_________________________________________________________________\n",
      "hidden-layer-2 (Dense)       (None, 75)                11325     \n",
      "_________________________________________________________________\n",
      "hidden-layer-3 (Dense)       (None, 150)               11400     \n",
      "_________________________________________________________________\n",
      "output-layer (Dense)         (None, 300)               45300     \n",
      "=================================================================\n",
      "Total params: 113,175\n",
      "Trainable params: 113,175\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/500\n",
      "12800/12800 [==============================] - 2s 183us/step - loss: 0.0026 - val_loss: 0.0013\n",
      "Epoch 2/500\n",
      "12800/12800 [==============================] - 2s 124us/step - loss: 0.0011 - val_loss: 9.7494e-04\n",
      "Epoch 3/500\n",
      "12800/12800 [==============================] - 1s 116us/step - loss: 8.5994e-04 - val_loss: 7.9107e-04\n",
      "Epoch 4/500\n",
      "12800/12800 [==============================] - 1s 111us/step - loss: 7.1156e-04 - val_loss: 6.7064e-04\n",
      "Test MSE: 0.00065907294\n",
      "Total MSE: 0.00065688146\n",
      "0.0008\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input-layer (InputLayer)     (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "hidden-layer (Dense)         (None, 75)                22575     \n",
      "_________________________________________________________________\n",
      "output-layer (Dense)         (None, 300)               22800     \n",
      "=================================================================\n",
      "Total params: 45,375\n",
      "Trainable params: 45,375\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/500\n",
      "12800/12800 [==============================] - 2s 147us/step - loss: 0.0017 - val_loss: 9.6567e-04\n",
      "Epoch 2/500\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 7.8605e-04 - val_loss: 6.5124e-04\n",
      "Test MSE: 0.00066673312\n",
      "Total MSE: 0.00065205336\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input-layer (InputLayer)     (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "hidden-layer-1 (Dense)       (None, 150)               45150     \n",
      "_________________________________________________________________\n",
      "hidden-layer-2 (Dense)       (None, 75)                11325     \n",
      "_________________________________________________________________\n",
      "hidden-layer-3 (Dense)       (None, 150)               11400     \n",
      "_________________________________________________________________\n",
      "output-layer (Dense)         (None, 300)               45300     \n",
      "=================================================================\n",
      "Total params: 113,175\n",
      "Trainable params: 113,175\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/500\n",
      "12800/12800 [==============================] - 3s 234us/step - loss: 0.0027 - val_loss: 0.0014\n",
      "Epoch 2/500\n",
      "12800/12800 [==============================] - 2s 133us/step - loss: 0.0012 - val_loss: 9.9866e-04\n",
      "Epoch 3/500\n",
      "12800/12800 [==============================] - 1s 116us/step - loss: 8.9429e-04 - val_loss: 8.1642e-04\n",
      "Epoch 4/500\n",
      "12800/12800 [==============================] - 2s 118us/step - loss: 7.4741e-04 - val_loss: 7.0018e-04\n",
      "Test MSE: 0.00069753023\n",
      "Total MSE: 0.00069327226\n",
      "0.0009000000000000001\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input-layer (InputLayer)     (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "hidden-layer (Dense)         (None, 75)                22575     \n",
      "_________________________________________________________________\n",
      "output-layer (Dense)         (None, 300)               22800     \n",
      "=================================================================\n",
      "Total params: 45,375\n",
      "Trainable params: 45,375\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/500\n",
      "12800/12800 [==============================] - 2s 183us/step - loss: 0.0016 - val_loss: 8.8983e-04\n",
      "Test MSE: 0.00089376707\n",
      "Total MSE: 0.00088480855\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input-layer (InputLayer)     (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "hidden-layer-1 (Dense)       (None, 150)               45150     \n",
      "_________________________________________________________________\n",
      "hidden-layer-2 (Dense)       (None, 75)                11325     \n",
      "_________________________________________________________________\n",
      "hidden-layer-3 (Dense)       (None, 150)               11400     \n",
      "_________________________________________________________________\n",
      "output-layer (Dense)         (None, 300)               45300     \n",
      "=================================================================\n",
      "Total params: 113,175\n",
      "Trainable params: 113,175\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/500\n",
      "12800/12800 [==============================] - 3s 256us/step - loss: 0.0037 - val_loss: 0.0016\n",
      "Epoch 2/500\n",
      "12800/12800 [==============================] - 2s 145us/step - loss: 0.0015 - val_loss: 0.0013\n",
      "Epoch 3/500\n",
      "12800/12800 [==============================] - 2s 155us/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 4/500\n",
      "12800/12800 [==============================] - 2s 173us/step - loss: 0.0011 - val_loss: 9.9495e-04\n",
      "Epoch 5/500\n",
      "12800/12800 [==============================] - 2s 126us/step - loss: 9.4888e-04 - val_loss: 8.9354e-04\n",
      "Test MSE: 0.00089692493\n",
      "Total MSE: 0.00089653394\n"
     ]
    }
   ],
   "source": [
    "e_epochs = {}\n",
    "e_epochs_deep = {}\n",
    "for e in np.arange(2, 10) * (10 ** -4):\n",
    "    print(e)\n",
    "    e_epochs[e] = process_image('a.jpg', 4, e)\n",
    "    e_epochs_deep[e] = process_image_deeply('a.jpg', 4, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x468 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['font.size'] = 15\n",
    "plt.rcParams['font.weight'] = 100\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(15, 6.5)\n",
    "\n",
    "ax.set(xlabel='Target accuracy e', ylabel='Epochs count')\n",
    "ax.plot(e_epochs.keys(), [len(e['loss']) for e in e_epochs.values()], '-o', label='Epochs count', linewidth=2)\n",
    "\n",
    "ax.grid()\n",
    "ax.legend(loc='upper center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, 6, 4, 3, 3, 3, 2, 1]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(e['loss']) for e in e_epochs.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input-layer (InputLayer)     (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "hidden-layer (Dense)         (None, 75)                22575     \n",
      "_________________________________________________________________\n",
      "output-layer (Dense)         (None, 300)               22800     \n",
      "=================================================================\n",
      "Total params: 45,375\n",
      "Trainable params: 45,375\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/500\n",
      "12800/12800 [==============================] - 3s 244us/step - loss: 0.5371 - val_loss: 0.0055\n",
      "Epoch 2/500\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 0.0046 - val_loss: 0.0038\n",
      "Epoch 3/500\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 0.0035 - val_loss: 0.0031\n",
      "Epoch 4/500\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 0.0030 - val_loss: 0.0028\n",
      "Epoch 5/500\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 0.0027 - val_loss: 0.0026\n",
      "Epoch 6/500\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 0.0025 - val_loss: 0.0024\n",
      "Epoch 7/500\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 0.0024 - val_loss: 0.0023\n",
      "Epoch 8/500\n",
      "12800/12800 [==============================] - 1s 93us/step - loss: 0.0023 - val_loss: 0.0022\n",
      "Epoch 9/500\n",
      "12800/12800 [==============================] - 1s 99us/step - loss: 0.0022 - val_loss: 0.0021\n",
      "Epoch 10/500\n",
      "12800/12800 [==============================] - 1s 96us/step - loss: 0.0021 - val_loss: 0.0020\n",
      "Epoch 11/500\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 12/500\n",
      "12800/12800 [==============================] - 1s 97us/step - loss: 0.0020 - val_loss: 0.0019\n",
      "Epoch 13/500\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 14/500\n",
      "12800/12800 [==============================] - 1s 94us/step - loss: 0.0019 - val_loss: 0.0018\n",
      "Epoch 15/500\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 16/500\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 17/500\n",
      "12800/12800 [==============================] - 1s 93us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 18/500\n",
      "12800/12800 [==============================] - 1s 108us/step - loss: 0.0017 - val_loss: 0.0016\n",
      "Epoch 19/500\n",
      "12800/12800 [==============================] - 1s 99us/step - loss: 0.0017 - val_loss: 0.0016\n",
      "Epoch 20/500\n",
      "12800/12800 [==============================] - 1s 105us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 21/500\n",
      "12800/12800 [==============================] - 1s 92us/step - loss: 0.0016 - val_loss: 0.0015\n",
      "Epoch 22/500\n",
      "12800/12800 [==============================] - 1s 93us/step - loss: 0.0016 - val_loss: 0.0015\n",
      "Epoch 23/500\n",
      "12800/12800 [==============================] - 1s 96us/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 24/500\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 25/500\n",
      "12800/12800 [==============================] - 1s 88us/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 26/500\n",
      "12800/12800 [==============================] - 1s 95us/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 27/500\n",
      "12800/12800 [==============================] - 1s 96us/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 28/500\n",
      "12800/12800 [==============================] - 1s 88us/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 29/500\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 30/500\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 0.0014 - val_loss: 0.0013\n",
      "Epoch 31/500\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 0.0014 - val_loss: 0.0013\n",
      "Epoch 32/500\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 33/500\n",
      "12800/12800 [==============================] - 1s 88us/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 34/500\n",
      "12800/12800 [==============================] - 1s 95us/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 35/500\n",
      "12800/12800 [==============================] - 1s 100us/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 36/500\n",
      "12800/12800 [==============================] - 1s 115us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 37/500\n",
      "12800/12800 [==============================] - 1s 96us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 38/500\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 39/500\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 40/500\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 41/500\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 42/500\n",
      "12800/12800 [==============================] - 1s 93us/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 43/500\n",
      "12800/12800 [==============================] - 1s 95us/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 44/500\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 45/500\n",
      "12800/12800 [==============================] - 1s 107us/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 46/500\n",
      "12800/12800 [==============================] - 1s 94us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 47/500\n",
      "12800/12800 [==============================] - 1s 95us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 48/500\n",
      "12800/12800 [==============================] - 1s 94us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 49/500\n",
      "12800/12800 [==============================] - 1s 94us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 50/500\n",
      "12800/12800 [==============================] - 1s 95us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 51/500\n",
      "12800/12800 [==============================] - 1s 93us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 52/500\n",
      "12800/12800 [==============================] - 1s 94us/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 53/500\n",
      "12800/12800 [==============================] - 1s 96us/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 54/500\n",
      "12800/12800 [==============================] - 1s 106us/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 55/500\n",
      "12800/12800 [==============================] - 1s 98us/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 56/500\n",
      "12800/12800 [==============================] - 1s 99us/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 57/500\n",
      "12800/12800 [==============================] - 1s 94us/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 58/500\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 0.0010 - val_loss: 9.9411e-04\n",
      "Epoch 59/500\n",
      "12800/12800 [==============================] - 1s 95us/step - loss: 0.0010 - val_loss: 9.8430e-04\n",
      "Epoch 60/500\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 0.0010 - val_loss: 9.7745e-04\n",
      "Epoch 61/500\n",
      "12800/12800 [==============================] - 1s 92us/step - loss: 9.9526e-04 - val_loss: 9.6923e-04\n",
      "Epoch 62/500\n",
      "12800/12800 [==============================] - 1s 99us/step - loss: 9.8713e-04 - val_loss: 9.6159e-04\n",
      "Epoch 63/500\n",
      "12800/12800 [==============================] - 1s 100us/step - loss: 9.7946e-04 - val_loss: 9.5441e-04\n",
      "Epoch 64/500\n",
      "12800/12800 [==============================] - 1s 94us/step - loss: 9.7196e-04 - val_loss: 9.4717e-04\n",
      "Epoch 65/500\n",
      "12800/12800 [==============================] - 1s 93us/step - loss: 9.6462e-04 - val_loss: 9.4076e-04\n",
      "Epoch 66/500\n",
      "12800/12800 [==============================] - 1s 100us/step - loss: 9.5711e-04 - val_loss: 9.3226e-04\n",
      "Epoch 67/500\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 9.5004e-04 - val_loss: 9.2704e-04\n",
      "Epoch 68/500\n",
      "12800/12800 [==============================] - 1s 94us/step - loss: 9.4306e-04 - val_loss: 9.2065e-04\n",
      "Epoch 69/500\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 9.3635e-04 - val_loss: 9.1246e-04\n",
      "Epoch 70/500\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 9.2937e-04 - val_loss: 9.0652e-04\n",
      "Epoch 71/500\n",
      "12800/12800 [==============================] - 1s 113us/step - loss: 9.2282e-04 - val_loss: 8.9976e-04\n",
      "Epoch 72/500\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 9.1634e-04 - val_loss: 8.9285e-04\n",
      "Epoch 73/500\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 9.0989e-04 - val_loss: 8.8855e-04\n",
      "Epoch 74/500\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 9.0369e-04 - val_loss: 8.8297e-04\n",
      "Epoch 75/500\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 8.9731e-04 - val_loss: 8.7763e-04\n",
      "Epoch 76/500\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 8.9131e-04 - val_loss: 8.6842e-04\n",
      "Epoch 77/500\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 8.8527e-04 - val_loss: 8.6481e-04\n",
      "Epoch 78/500\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 8.7956e-04 - val_loss: 8.5913e-04\n",
      "Epoch 79/500\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 8.7378e-04 - val_loss: 8.5445e-04\n",
      "Epoch 80/500\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 8.6810e-04 - val_loss: 8.4802e-04\n",
      "Epoch 81/500\n",
      "12800/12800 [==============================] - 1s 93us/step - loss: 8.6248e-04 - val_loss: 8.4167e-04\n",
      "Epoch 82/500\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 8.5703e-04 - val_loss: 8.3759e-04\n",
      "Epoch 83/500\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 8.5153e-04 - val_loss: 8.3148e-04\n",
      "Epoch 84/500\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 8.4600e-04 - val_loss: 8.2748e-04\n",
      "Epoch 85/500\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 8.4063e-04 - val_loss: 8.2180e-04\n",
      "Epoch 86/500\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 8.3577e-04 - val_loss: 8.1625e-04\n",
      "Epoch 87/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 8.3028e-04 - val_loss: 8.1158e-04\n",
      "Epoch 88/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 8.2561e-04 - val_loss: 8.0618e-04\n",
      "Epoch 89/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 8.2049e-04 - val_loss: 8.0184e-04\n",
      "Epoch 90/500\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 8.1556e-04 - val_loss: 7.9635e-04\n",
      "Epoch 91/500\n",
      "12800/12800 [==============================] - 1s 93us/step - loss: 8.1051e-04 - val_loss: 7.9419e-04\n",
      "Epoch 92/500\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 8.0601e-04 - val_loss: 7.8796e-04\n",
      "Epoch 93/500\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 8.0119e-04 - val_loss: 7.8414e-04\n",
      "Epoch 94/500\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 7.9637e-04 - val_loss: 7.7937e-04\n",
      "Epoch 95/500\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 7.9201e-04 - val_loss: 7.7554e-04\n",
      "Epoch 96/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 7.8735e-04 - val_loss: 7.7002e-04\n",
      "Epoch 97/500\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 7.8283e-04 - val_loss: 7.6599e-04\n",
      "Epoch 98/500\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 7.7853e-04 - val_loss: 7.6110e-04\n",
      "Epoch 99/500\n",
      "12800/12800 [==============================] - 1s 92us/step - loss: 7.7416e-04 - val_loss: 7.5766e-04\n",
      "Epoch 100/500\n",
      "12800/12800 [==============================] - 1s 93us/step - loss: 7.6972e-04 - val_loss: 7.5426e-04\n",
      "Epoch 101/500\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 7.6552e-04 - val_loss: 7.4908e-04\n",
      "Epoch 102/500\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 7.6114e-04 - val_loss: 7.4487e-04\n",
      "Epoch 103/500\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 7.5712e-04 - val_loss: 7.4104e-04\n",
      "Epoch 104/500\n",
      "12800/12800 [==============================] - 1s 93us/step - loss: 7.5279e-04 - val_loss: 7.3706e-04\n",
      "Epoch 105/500\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 7.4869e-04 - val_loss: 7.3280e-04\n",
      "Epoch 106/500\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 7.4466e-04 - val_loss: 7.2806e-04\n",
      "Epoch 107/500\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 7.4068e-04 - val_loss: 7.2457e-04\n",
      "Epoch 108/500\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 7.3662e-04 - val_loss: 7.2175e-04\n",
      "Epoch 109/500\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 7.3272e-04 - val_loss: 7.1768e-04\n",
      "Epoch 110/500\n",
      "12800/12800 [==============================] - 1s 100us/step - loss: 7.2888e-04 - val_loss: 7.1331e-04\n",
      "Epoch 111/500\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 7.2497e-04 - val_loss: 7.0943e-04\n",
      "Epoch 112/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 7.2119e-04 - val_loss: 7.0846e-04\n",
      "Epoch 113/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 7.1758e-04 - val_loss: 7.0359e-04\n",
      "Epoch 114/500\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 7.1381e-04 - val_loss: 6.9994e-04\n",
      "Epoch 115/500\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 7.1003e-04 - val_loss: 6.9634e-04\n",
      "Epoch 116/500\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 7.0646e-04 - val_loss: 6.9163e-04\n",
      "Epoch 117/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 7.0278e-04 - val_loss: 6.8972e-04\n",
      "Epoch 118/500\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 6.9916e-04 - val_loss: 6.8559e-04\n",
      "Epoch 119/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 6.9568e-04 - val_loss: 6.8141e-04\n",
      "Epoch 120/500\n",
      "12800/12800 [==============================] - 1s 101us/step - loss: 6.9226e-04 - val_loss: 6.7794e-04\n",
      "Epoch 121/500\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 6.8859e-04 - val_loss: 6.7661e-04\n",
      "Epoch 122/500\n",
      "12800/12800 [==============================] - 1s 93us/step - loss: 6.8520e-04 - val_loss: 6.7104e-04\n",
      "Epoch 123/500\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 6.8177e-04 - val_loss: 6.6938e-04\n",
      "Epoch 124/500\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 6.7844e-04 - val_loss: 6.6527e-04\n",
      "Epoch 125/500\n",
      "12800/12800 [==============================] - 1s 88us/step - loss: 6.7514e-04 - val_loss: 6.6186e-04\n",
      "Epoch 126/500\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 6.7184e-04 - val_loss: 6.5807e-04\n",
      "Epoch 127/500\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 6.6853e-04 - val_loss: 6.5814e-04\n",
      "Epoch 128/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 6.6523e-04 - val_loss: 6.5388e-04\n",
      "Epoch 129/500\n",
      "12800/12800 [==============================] - 1s 93us/step - loss: 6.6223e-04 - val_loss: 6.4860e-04\n",
      "Epoch 130/500\n",
      "12800/12800 [==============================] - 1s 94us/step - loss: 6.5904e-04 - val_loss: 6.4699e-04\n",
      "Epoch 131/500\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 6.5589e-04 - val_loss: 6.4317e-04\n",
      "Epoch 132/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 6.5279e-04 - val_loss: 6.4167e-04\n",
      "Epoch 133/500\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 6.4945e-04 - val_loss: 6.3793e-04\n",
      "Epoch 134/500\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 6.4646e-04 - val_loss: 6.3472e-04\n",
      "Epoch 135/500\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 6.4358e-04 - val_loss: 6.3209e-04\n",
      "Epoch 136/500\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 6.4053e-04 - val_loss: 6.2877e-04\n",
      "Epoch 137/500\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 6.3751e-04 - val_loss: 6.2581e-04\n",
      "Epoch 138/500\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 6.3444e-04 - val_loss: 6.2334e-04\n",
      "Epoch 139/500\n",
      "12800/12800 [==============================] - 1s 101us/step - loss: 6.3161e-04 - val_loss: 6.2005e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 140/500\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 6.2862e-04 - val_loss: 6.1824e-04\n",
      "Epoch 141/500\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 6.2571e-04 - val_loss: 6.1424e-04\n",
      "Epoch 142/500\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 6.2284e-04 - val_loss: 6.1199e-04\n",
      "Epoch 143/500\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 6.2016e-04 - val_loss: 6.0920e-04\n",
      "Epoch 144/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 6.1726e-04 - val_loss: 6.0671e-04\n",
      "Epoch 145/500\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 6.1438e-04 - val_loss: 6.0302e-04\n",
      "Epoch 146/500\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 6.1167e-04 - val_loss: 6.0138e-04\n",
      "Epoch 147/500\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 6.0894e-04 - val_loss: 5.9760e-04\n",
      "Epoch 148/500\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 6.0623e-04 - val_loss: 5.9563e-04\n",
      "Epoch 149/500\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 6.0349e-04 - val_loss: 5.9367e-04\n",
      "Epoch 150/500\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 6.0093e-04 - val_loss: 5.8959e-04\n",
      "Epoch 151/500\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 5.9817e-04 - val_loss: 5.8723e-04\n",
      "Epoch 152/500\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 5.9550e-04 - val_loss: 5.8497e-04\n",
      "Epoch 153/500\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 5.9279e-04 - val_loss: 5.8171e-04\n",
      "Epoch 154/500\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 5.9042e-04 - val_loss: 5.8024e-04\n",
      "Epoch 155/500\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 5.8780e-04 - val_loss: 5.7702e-04\n",
      "Epoch 156/500\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 5.8526e-04 - val_loss: 5.7595e-04\n",
      "Epoch 157/500\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 5.8277e-04 - val_loss: 5.7267e-04\n",
      "Epoch 158/500\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 5.8018e-04 - val_loss: 5.6991e-04\n",
      "Epoch 159/500\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 5.7771e-04 - val_loss: 5.6874e-04\n",
      "Epoch 160/500\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 5.7520e-04 - val_loss: 5.6721e-04\n",
      "Epoch 161/500\n",
      "12800/12800 [==============================] - 1s 88us/step - loss: 5.7281e-04 - val_loss: 5.6344e-04\n",
      "Epoch 162/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 5.7056e-04 - val_loss: 5.6142e-04\n",
      "Epoch 163/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 5.6803e-04 - val_loss: 5.5981e-04\n",
      "Epoch 164/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 5.6558e-04 - val_loss: 5.5686e-04\n",
      "Epoch 165/500\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 5.6330e-04 - val_loss: 5.5516e-04\n",
      "Epoch 166/500\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 5.6099e-04 - val_loss: 5.5137e-04\n",
      "Epoch 167/500\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 5.5868e-04 - val_loss: 5.4909e-04\n",
      "Epoch 168/500\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 5.5628e-04 - val_loss: 5.4710e-04\n",
      "Epoch 169/500\n",
      "12800/12800 [==============================] - 1s 93us/step - loss: 5.5398e-04 - val_loss: 5.4558e-04\n",
      "Epoch 170/500\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 5.5164e-04 - val_loss: 5.4288e-04\n",
      "Epoch 171/500\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 5.4939e-04 - val_loss: 5.4164e-04\n",
      "Epoch 172/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 5.4725e-04 - val_loss: 5.3917e-04\n",
      "Epoch 173/500\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 5.4499e-04 - val_loss: 5.3731e-04\n",
      "Epoch 174/500\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 5.4282e-04 - val_loss: 5.3533e-04\n",
      "Epoch 175/500\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 5.4062e-04 - val_loss: 5.3259e-04\n",
      "Epoch 176/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 5.3842e-04 - val_loss: 5.3004e-04\n",
      "Epoch 177/500\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 5.3636e-04 - val_loss: 5.2759e-04\n",
      "Epoch 178/500\n",
      "12800/12800 [==============================] - 1s 93us/step - loss: 5.3416e-04 - val_loss: 5.2578e-04\n",
      "Epoch 179/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 5.3202e-04 - val_loss: 5.2465e-04\n",
      "Epoch 180/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 5.2989e-04 - val_loss: 5.2251e-04\n",
      "Epoch 181/500\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 5.2777e-04 - val_loss: 5.1981e-04\n",
      "Epoch 182/500\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 5.2581e-04 - val_loss: 5.1801e-04\n",
      "Epoch 183/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 5.2376e-04 - val_loss: 5.1616e-04\n",
      "Epoch 184/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 5.2165e-04 - val_loss: 5.1400e-04\n",
      "Epoch 185/500\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 5.1952e-04 - val_loss: 5.1214e-04\n",
      "Epoch 186/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 5.1767e-04 - val_loss: 5.0965e-04\n",
      "Epoch 187/500\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 5.1559e-04 - val_loss: 5.0781e-04\n",
      "Epoch 188/500\n",
      "12800/12800 [==============================] - 1s 98us/step - loss: 5.1369e-04 - val_loss: 5.0615e-04\n",
      "Epoch 189/500\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 5.1167e-04 - val_loss: 5.0513e-04\n",
      "Epoch 190/500\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 5.0974e-04 - val_loss: 5.0214e-04\n",
      "Epoch 191/500\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 5.0779e-04 - val_loss: 5.0066e-04\n",
      "Epoch 192/500\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 5.0587e-04 - val_loss: 4.9908e-04\n",
      "Test MSE: 0.00052189674\n",
      "Total MSE: 0.00050732527\n",
      "0.01\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input-layer (InputLayer)     (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "hidden-layer (Dense)         (None, 75)                22575     \n",
      "_________________________________________________________________\n",
      "output-layer (Dense)         (None, 300)               22800     \n",
      "=================================================================\n",
      "Total params: 45,375\n",
      "Trainable params: 45,375\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/500\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.0016 - val_loss: 9.1418e-04\n",
      "Epoch 2/500\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 7.3514e-04 - val_loss: 6.1269e-04\n",
      "Epoch 3/500\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 5.2820e-04 - val_loss: 4.7108e-04\n",
      "Test MSE: 0.00047141367\n",
      "Total MSE: 0.00046475343\n",
      "0.001\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input-layer (InputLayer)     (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "hidden-layer (Dense)         (None, 75)                22575     \n",
      "_________________________________________________________________\n",
      "output-layer (Dense)         (None, 300)               22800     \n",
      "=================================================================\n",
      "Total params: 45,375\n",
      "Trainable params: 45,375\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/500\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.0039 - val_loss: 0.0027\n",
      "Epoch 2/500\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 0.0023 - val_loss: 0.0022\n",
      "Epoch 3/500\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 4/500\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 5/500\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 6/500\n",
      "12800/12800 [==============================] - 1s 97us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 7/500\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 8/500\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 9/500\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 10/500\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 11/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 12/500\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 13/500\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 14/500\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 15/500\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 16/500\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 17/500\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 18/500\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 19/500\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 20/500\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 21/500\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 22/500\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 23/500\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 9.8434e-04 - val_loss: 0.0010\n",
      "Epoch 24/500\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 9.6698e-04 - val_loss: 9.9798e-04\n",
      "Epoch 25/500\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 9.5055e-04 - val_loss: 9.8129e-04\n",
      "Epoch 26/500\n",
      "12800/12800 [==============================] - 1s 88us/step - loss: 9.3492e-04 - val_loss: 9.6554e-04\n",
      "Epoch 27/500\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 9.2007e-04 - val_loss: 9.5045e-04\n",
      "Epoch 28/500\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 9.0591e-04 - val_loss: 9.3603e-04\n",
      "Epoch 29/500\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 8.9237e-04 - val_loss: 9.2240e-04\n",
      "Epoch 30/500\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 8.7943e-04 - val_loss: 9.0917e-04\n",
      "Epoch 31/500\n",
      "12800/12800 [==============================] - 1s 88us/step - loss: 8.6701e-04 - val_loss: 8.9637e-04\n",
      "Epoch 32/500\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 8.5507e-04 - val_loss: 8.8459e-04\n",
      "Epoch 33/500\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 8.4361e-04 - val_loss: 8.7284e-04\n",
      "Epoch 34/500\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 8.3259e-04 - val_loss: 8.6150e-04\n",
      "Epoch 35/500\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 8.2192e-04 - val_loss: 8.5065e-04\n",
      "Epoch 36/500\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 8.1165e-04 - val_loss: 8.4018e-04\n",
      "Epoch 37/500\n",
      "12800/12800 [==============================] - 1s 101us/step - loss: 8.0174e-04 - val_loss: 8.3014e-04\n",
      "Epoch 38/500\n",
      "12800/12800 [==============================] - 1s 94us/step - loss: 7.9212e-04 - val_loss: 8.2016e-04\n",
      "Epoch 39/500\n",
      "12800/12800 [==============================] - 1s 93us/step - loss: 7.8284e-04 - val_loss: 8.1076e-04\n",
      "Epoch 40/500\n",
      "12800/12800 [==============================] - 2s 122us/step - loss: 7.7381e-04 - val_loss: 8.0161e-04\n",
      "Epoch 41/500\n",
      "12800/12800 [==============================] - 1s 88us/step - loss: 7.6510e-04 - val_loss: 7.9261e-04\n",
      "Epoch 42/500\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 7.5662e-04 - val_loss: 7.8404e-04\n",
      "Epoch 43/500\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 7.4836e-04 - val_loss: 7.7571e-04\n",
      "Epoch 44/500\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 7.4036e-04 - val_loss: 7.6746e-04\n",
      "Epoch 45/500\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 7.3258e-04 - val_loss: 7.5944e-04\n",
      "Epoch 46/500\n",
      "12800/12800 [==============================] - 1s 95us/step - loss: 7.2501e-04 - val_loss: 7.5173e-04\n",
      "Epoch 47/500\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 7.1764e-04 - val_loss: 7.4409e-04\n",
      "Epoch 48/500\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 7.1044e-04 - val_loss: 7.3679e-04\n",
      "Epoch 49/500\n",
      "12800/12800 [==============================] - 1s 94us/step - loss: 7.0344e-04 - val_loss: 7.2957e-04\n",
      "Epoch 50/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 6.9661e-04 - val_loss: 7.2260e-04\n",
      "Epoch 51/500\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 6.8993e-04 - val_loss: 7.1575e-04\n",
      "Epoch 52/500\n",
      "12800/12800 [==============================] - 1s 97us/step - loss: 6.8343e-04 - val_loss: 7.0914e-04\n",
      "Epoch 53/500\n",
      "12800/12800 [==============================] - 1s 100us/step - loss: 6.7708e-04 - val_loss: 7.0272e-04\n",
      "Epoch 54/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 6.7088e-04 - val_loss: 6.9621e-04\n",
      "Epoch 55/500\n",
      "12800/12800 [==============================] - 1s 116us/step - loss: 6.6482e-04 - val_loss: 6.8999e-04\n",
      "Epoch 56/500\n",
      "12800/12800 [==============================] - 2s 128us/step - loss: 6.5889e-04 - val_loss: 6.8391e-04\n",
      "Epoch 57/500\n",
      "12800/12800 [==============================] - 1s 115us/step - loss: 6.5310e-04 - val_loss: 6.7800e-04\n",
      "Epoch 58/500\n",
      "12800/12800 [==============================] - 1s 95us/step - loss: 6.4743e-04 - val_loss: 6.7219e-04\n",
      "Epoch 59/500\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 6.4187e-04 - val_loss: 6.6650e-04\n",
      "Epoch 60/500\n",
      "12800/12800 [==============================] - 2s 121us/step - loss: 6.3644e-04 - val_loss: 6.6091e-04\n",
      "Epoch 61/500\n",
      "12800/12800 [==============================] - 2s 132us/step - loss: 6.3114e-04 - val_loss: 6.5540e-04\n",
      "Epoch 62/500\n",
      "12800/12800 [==============================] - 2s 141us/step - loss: 6.2593e-04 - val_loss: 6.5003e-04\n",
      "Epoch 63/500\n",
      "12800/12800 [==============================] - 2s 124us/step - loss: 6.2082e-04 - val_loss: 6.4492e-04\n",
      "Epoch 64/500\n",
      "12800/12800 [==============================] - 1s 103us/step - loss: 6.1582e-04 - val_loss: 6.3968e-04\n",
      "Epoch 65/500\n",
      "12800/12800 [==============================] - 1s 103us/step - loss: 6.1093e-04 - val_loss: 6.3475e-04\n",
      "Epoch 66/500\n",
      "12800/12800 [==============================] - 1s 108us/step - loss: 6.0611e-04 - val_loss: 6.2976e-04\n",
      "Epoch 67/500\n",
      "12800/12800 [==============================] - 1s 101us/step - loss: 6.0143e-04 - val_loss: 6.2490e-04\n",
      "Epoch 68/500\n",
      "12800/12800 [==============================] - 1s 95us/step - loss: 5.9679e-04 - val_loss: 6.2015e-04\n",
      "Epoch 69/500\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 5.9227e-04 - val_loss: 6.1552e-04\n",
      "Epoch 70/500\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 5.8783e-04 - val_loss: 6.1096e-04\n",
      "Epoch 71/500\n",
      "12800/12800 [==============================] - 1s 107us/step - loss: 5.8347e-04 - val_loss: 6.0638e-04\n",
      "Epoch 72/500\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 5.7919e-04 - val_loss: 6.0204e-04\n",
      "Epoch 73/500\n",
      "12800/12800 [==============================] - 1s 88us/step - loss: 5.7497e-04 - val_loss: 5.9770e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/500\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 5.7086e-04 - val_loss: 5.9347e-04\n",
      "Epoch 75/500\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 5.6680e-04 - val_loss: 5.8926e-04\n",
      "Epoch 76/500\n",
      "12800/12800 [==============================] - 1s 88us/step - loss: 5.6281e-04 - val_loss: 5.8521e-04\n",
      "Epoch 77/500\n",
      "12800/12800 [==============================] - 1s 94us/step - loss: 5.5890e-04 - val_loss: 5.8118e-04\n",
      "Epoch 78/500\n",
      "12800/12800 [==============================] - 1s 102us/step - loss: 5.5505e-04 - val_loss: 5.7716e-04\n",
      "Epoch 79/500\n",
      "12800/12800 [==============================] - 2s 125us/step - loss: 5.5127e-04 - val_loss: 5.7326e-04\n",
      "Epoch 80/500\n",
      "12800/12800 [==============================] - 2s 166us/step - loss: 5.4754e-04 - val_loss: 5.6946e-04\n",
      "Epoch 81/500\n",
      "12800/12800 [==============================] - 1s 110us/step - loss: 5.4390e-04 - val_loss: 5.6567e-04\n",
      "Epoch 82/500\n",
      "12800/12800 [==============================] - 1s 96us/step - loss: 5.4030e-04 - val_loss: 5.6196e-04\n",
      "Epoch 83/500\n",
      "12800/12800 [==============================] - 1s 115us/step - loss: 5.3676e-04 - val_loss: 5.5838e-04\n",
      "Epoch 84/500\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 5.3329e-04 - val_loss: 5.5474e-04\n",
      "Epoch 85/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 5.2985e-04 - val_loss: 5.5119e-04\n",
      "Epoch 86/500\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 5.2648e-04 - val_loss: 5.4780e-04\n",
      "Epoch 87/500\n",
      "12800/12800 [==============================] - 2s 136us/step - loss: 5.2317e-04 - val_loss: 5.4428e-04\n",
      "Epoch 88/500\n",
      "12800/12800 [==============================] - 2s 162us/step - loss: 5.1990e-04 - val_loss: 5.4089e-04\n",
      "Epoch 89/500\n",
      "12800/12800 [==============================] - 2s 184us/step - loss: 5.1670e-04 - val_loss: 5.3758e-04\n",
      "Epoch 90/500\n",
      "12800/12800 [==============================] - 1s 111us/step - loss: 5.1353e-04 - val_loss: 5.3437e-04\n",
      "Epoch 91/500\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 5.1041e-04 - val_loss: 5.3109e-04\n",
      "Epoch 92/500\n",
      "12800/12800 [==============================] - 1s 92us/step - loss: 5.0734e-04 - val_loss: 5.2799e-04\n",
      "Epoch 93/500\n",
      "12800/12800 [==============================] - ETA: 0s - loss: 5.0455e-04- ETA: 0s -  - 1s 109us/step - loss: 5.0432e-04 - val_loss: 5.2483e-04\n",
      "Epoch 94/500\n",
      "12800/12800 [==============================] - 2s 126us/step - loss: 5.0133e-04 - val_loss: 5.2177e-04\n",
      "Epoch 95/500\n",
      "12800/12800 [==============================] - 1s 93us/step - loss: 4.9841e-04 - val_loss: 5.1874e-04\n",
      "Epoch 96/500\n",
      "12800/12800 [==============================] - 2s 133us/step - loss: 4.9551e-04 - val_loss: 5.1578e-04\n",
      "Epoch 97/500\n",
      "12800/12800 [==============================] - 2s 182us/step - loss: 4.9266e-04 - val_loss: 5.1279e-04\n",
      "Epoch 98/500\n",
      "12800/12800 [==============================] - 2s 165us/step - loss: 4.8985e-04 - val_loss: 5.0993e-04\n",
      "Epoch 99/500\n",
      "12800/12800 [==============================] - 1s 108us/step - loss: 4.8709e-04 - val_loss: 5.0708e-04\n",
      "Epoch 100/500\n",
      "12800/12800 [==============================] - 1s 88us/step - loss: 4.8434e-04 - val_loss: 5.0422e-04\n",
      "Epoch 101/500\n",
      "12800/12800 [==============================] - 1s 105us/step - loss: 4.8166e-04 - val_loss: 5.0144e-04\n",
      "Epoch 102/500\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 4.7900e-04 - val_loss: 4.9871e-04\n",
      "Test MSE: 0.00048168934\n",
      "Total MSE: 0.00048173127\n",
      "0.0001\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input-layer (InputLayer)     (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "hidden-layer (Dense)         (None, 75)                22575     \n",
      "_________________________________________________________________\n",
      "output-layer (Dense)         (None, 300)               22800     \n",
      "=================================================================\n",
      "Total params: 45,375\n",
      "Trainable params: 45,375\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/500\n",
      "12800/12800 [==============================] - 3s 215us/step - loss: 0.0123 - val_loss: 0.0107\n",
      "Epoch 2/500\n",
      "12800/12800 [==============================] - 1s 101us/step - loss: 0.0099 - val_loss: 0.0092\n",
      "Epoch 3/500\n",
      "12800/12800 [==============================] - 1s 106us/step - loss: 0.0087 - val_loss: 0.0082\n",
      "Epoch 4/500\n",
      "12800/12800 [==============================] - 1s 111us/step - loss: 0.0078 - val_loss: 0.0074\n",
      "Epoch 5/500\n",
      "12800/12800 [==============================] - 1s 112us/step - loss: 0.0071 - val_loss: 0.0068\n",
      "Epoch 6/500\n",
      "12800/12800 [==============================] - 1s 97us/step - loss: 0.0066 - val_loss: 0.0063\n",
      "Epoch 7/500\n",
      "12800/12800 [==============================] - 1s 88us/step - loss: 0.0061 - val_loss: 0.0059\n",
      "Epoch 8/500\n",
      "12800/12800 [==============================] - 1s 110us/step - loss: 0.0058 - val_loss: 0.0056\n",
      "Epoch 9/500\n",
      "12800/12800 [==============================] - 1s 110us/step - loss: 0.0055 - val_loss: 0.0053\n",
      "Epoch 10/500\n",
      "12800/12800 [==============================] - 2s 126us/step - loss: 0.0052 - val_loss: 0.0051\n",
      "Epoch 11/500\n",
      "12800/12800 [==============================] - 1s 99us/step - loss: 0.0050 - val_loss: 0.0049\n",
      "Epoch 12/500\n",
      "12800/12800 [==============================] - 1s 97us/step - loss: 0.0048 - val_loss: 0.0047\n",
      "Epoch 13/500\n",
      "12800/12800 [==============================] - 1s 102us/step - loss: 0.0046 - val_loss: 0.0045\n",
      "Epoch 14/500\n",
      "12800/12800 [==============================] - 1s 96us/step - loss: 0.0045 - val_loss: 0.0044\n",
      "Epoch 15/500\n",
      "12800/12800 [==============================] - 1s 117us/step - loss: 0.0044 - val_loss: 0.0043\n",
      "Epoch 16/500\n",
      "12800/12800 [==============================] - 1s 112us/step - loss: 0.0042 - val_loss: 0.0042\n",
      "Epoch 17/500\n",
      "12800/12800 [==============================] - 1s 108us/step - loss: 0.0041 - val_loss: 0.0041\n",
      "Epoch 18/500\n",
      "12800/12800 [==============================] - 1s 113us/step - loss: 0.0041 - val_loss: 0.0040\n",
      "Epoch 19/500\n",
      "12800/12800 [==============================] - 1s 105us/step - loss: 0.0040 - val_loss: 0.0039\n",
      "Epoch 20/500\n",
      "12800/12800 [==============================] - 2s 121us/step - loss: 0.0039 - val_loss: 0.0038\n",
      "Epoch 21/500\n",
      "12800/12800 [==============================] - 2s 132us/step - loss: 0.0038 - val_loss: 0.0038\n",
      "Epoch 22/500\n",
      "12800/12800 [==============================] - 2s 120us/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 23/500\n",
      "12800/12800 [==============================] - 1s 101us/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 24/500\n",
      "12800/12800 [==============================] - 1s 98us/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 25/500\n",
      "12800/12800 [==============================] - 1s 110us/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 26/500\n",
      "12800/12800 [==============================] - 1s 96us/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 27/500\n",
      "12800/12800 [==============================] - 1s 103us/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 28/500\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 29/500\n",
      "12800/12800 [==============================] - 1s 97us/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 30/500\n",
      "12800/12800 [==============================] - 2s 122us/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 31/500\n",
      "12800/12800 [==============================] - 1s 110us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 32/500\n",
      "12800/12800 [==============================] - 2s 133us/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 33/500\n",
      "12800/12800 [==============================] - 1s 112us/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 34/500\n",
      "12800/12800 [==============================] - 1s 109us/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 35/500\n",
      "12800/12800 [==============================] - 1s 98us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 36/500\n",
      "12800/12800 [==============================] - 1s 99us/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 37/500\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 38/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 39/500\n",
      "12800/12800 [==============================] - 1s 88us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 40/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 41/500\n",
      "12800/12800 [==============================] - 1s 88us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 42/500\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 43/500\n",
      "12800/12800 [==============================] - 1s 97us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 44/500\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 45/500\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 46/500\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 47/500\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 48/500\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 49/500\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 50/500\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 51/500\n",
      "12800/12800 [==============================] - 1s 96us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 52/500\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 53/500\n",
      "12800/12800 [==============================] - 1s 93us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 54/500\n",
      "12800/12800 [==============================] - 1s 96us/step - loss: 0.0029 - val_loss: 0.0028\n",
      "Epoch 55/500\n",
      "12800/12800 [==============================] - 1s 95us/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 56/500\n",
      "12800/12800 [==============================] - 1s 96us/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 57/500\n",
      "12800/12800 [==============================] - 1s 100us/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 58/500\n",
      "12800/12800 [==============================] - 1s 108us/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 59/500\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 60/500\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 61/500\n",
      "12800/12800 [==============================] - 1s 93us/step - loss: 0.0028 - val_loss: 0.0027\n",
      "Epoch 62/500\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 63/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 64/500\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 65/500\n",
      "12800/12800 [==============================] - 1s 92us/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 66/500\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 67/500\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 68/500\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 69/500\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 0.0027 - val_loss: 0.0026\n",
      "Epoch 70/500\n",
      "12800/12800 [==============================] - 1s 97us/step - loss: 0.0027 - val_loss: 0.0026\n",
      "Epoch 71/500\n",
      "12800/12800 [==============================] - 1s 88us/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 72/500\n",
      "12800/12800 [==============================] - 1s 93us/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 73/500\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 74/500\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 75/500\n",
      "12800/12800 [==============================] - 1s 93us/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 76/500\n",
      "12800/12800 [==============================] - 1s 93us/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 77/500\n",
      "12800/12800 [==============================] - 1s 92us/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 78/500\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 79/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 0.0026 - val_loss: 0.0025\n",
      "Epoch 80/500\n",
      "12800/12800 [==============================] - 1s 110us/step - loss: 0.0026 - val_loss: 0.0025\n",
      "Epoch 81/500\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 82/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 83/500\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 84/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 85/500\n",
      "12800/12800 [==============================] - 1s 92us/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 86/500\n",
      "12800/12800 [==============================] - 1s 88us/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 87/500\n",
      "12800/12800 [==============================] - 1s 93us/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 88/500\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 89/500\n",
      "12800/12800 [==============================] - 1s 100us/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 90/500\n",
      "12800/12800 [==============================] - 1s 100us/step - loss: 0.0025 - val_loss: 0.0024\n",
      "Epoch 91/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 0.0025 - val_loss: 0.0024\n",
      "Epoch 92/500\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 0.0025 - val_loss: 0.0024\n",
      "Epoch 93/500\n",
      "12800/12800 [==============================] - 1s 96us/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 94/500\n",
      "12800/12800 [==============================] - 1s 93us/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 95/500\n",
      "12800/12800 [==============================] - 1s 95us/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 96/500\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 97/500\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 98/500\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 99/500\n",
      "12800/12800 [==============================] - 2s 119us/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 100/500\n",
      "12800/12800 [==============================] - 1s 98us/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 101/500\n",
      "12800/12800 [==============================] - 1s 108us/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 102/500\n",
      "12800/12800 [==============================] - 1s 96us/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 103/500\n",
      "12800/12800 [==============================] - 1s 94us/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 104/500\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 0.0024 - val_loss: 0.0023\n",
      "Epoch 105/500\n",
      "12800/12800 [==============================] - 1s 98us/step - loss: 0.0024 - val_loss: 0.0023\n",
      "Epoch 106/500\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 0.0024 - val_loss: 0.0023\n",
      "Epoch 107/500\n",
      "12800/12800 [==============================] - 2s 132us/step - loss: 0.0024 - val_loss: 0.0023\n",
      "Epoch 108/500\n",
      "12800/12800 [==============================] - 1s 96us/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 109/500\n",
      "12800/12800 [==============================] - 1s 104us/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 110/500\n",
      "12800/12800 [==============================] - 1s 102us/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 111/500\n",
      "12800/12800 [==============================] - 1s 94us/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 112/500\n",
      "12800/12800 [==============================] - 1s 114us/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 113/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 1s 90us/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 114/500\n",
      "12800/12800 [==============================] - 1s 95us/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 115/500\n",
      "12800/12800 [==============================] - 1s 101us/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 116/500\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 117/500\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 118/500\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 119/500\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 120/500\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 121/500\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 122/500\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 0.0023 - val_loss: 0.0022\n",
      "Epoch 123/500\n",
      "12800/12800 [==============================] - 1s 113us/step - loss: 0.0023 - val_loss: 0.0022\n",
      "Epoch 124/500\n",
      "12800/12800 [==============================] - 2s 131us/step - loss: 0.0023 - val_loss: 0.0022\n",
      "Epoch 125/500\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 0.0023 - val_loss: 0.0022\n",
      "Epoch 126/500\n",
      "12800/12800 [==============================] - 1s 104us/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 127/500\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 128/500\n",
      "12800/12800 [==============================] - 1s 116us/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 129/500\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 130/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 131/500\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 132/500\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 133/500\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 134/500\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 135/500\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 136/500\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 137/500\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 138/500\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 139/500\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 140/500\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 141/500\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 142/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 143/500\n",
      "12800/12800 [==============================] - 1s 104us/step - loss: 0.0022 - val_loss: 0.0021\n",
      "Epoch 144/500\n",
      "12800/12800 [==============================] - 1s 107us/step - loss: 0.0022 - val_loss: 0.0021\n",
      "Epoch 145/500\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 0.0022 - val_loss: 0.0021\n",
      "Epoch 146/500\n",
      "12800/12800 [==============================] - 1s 92us/step - loss: 0.0022 - val_loss: 0.0021\n",
      "Epoch 147/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 0.0022 - val_loss: 0.0021\n",
      "Epoch 148/500\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 149/500\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 150/500\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 151/500\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 152/500\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 153/500\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 154/500\n",
      "12800/12800 [==============================] - 1s 94us/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 155/500\n",
      "12800/12800 [==============================] - 1s 96us/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 156/500\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 157/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 158/500\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 159/500\n",
      "12800/12800 [==============================] - 1s 88us/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 160/500\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 161/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 162/500\n",
      "12800/12800 [==============================] - 2s 148us/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 163/500\n",
      "12800/12800 [==============================] - 1s 102us/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 164/500\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 165/500\n",
      "12800/12800 [==============================] - 1s 93us/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 166/500\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 167/500\n",
      "12800/12800 [==============================] - 1s 95us/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 168/500\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 169/500\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 170/500\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 171/500\n",
      "12800/12800 [==============================] - 1s 103us/step - loss: 0.0021 - val_loss: 0.0020\n",
      "Epoch 172/500\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 0.0021 - val_loss: 0.0020\n",
      "Epoch 173/500\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 0.0021 - val_loss: 0.0020\n",
      "Epoch 174/500\n",
      "12800/12800 [==============================] - 1s 94us/step - loss: 0.0021 - val_loss: 0.0020\n",
      "Epoch 175/500\n",
      "12800/12800 [==============================] - 1s 93us/step - loss: 0.0021 - val_loss: 0.0020\n",
      "Epoch 176/500\n",
      "12800/12800 [==============================] - 1s 94us/step - loss: 0.0021 - val_loss: 0.0020\n",
      "Epoch 177/500\n",
      "12800/12800 [==============================] - 1s 93us/step - loss: 0.0021 - val_loss: 0.0020\n",
      "Epoch 178/500\n",
      "12800/12800 [==============================] - 1s 97us/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 179/500\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 180/500\n",
      "12800/12800 [==============================] - 1s 95us/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 181/500\n",
      "12800/12800 [==============================] - 1s 95us/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 182/500\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 183/500\n",
      "12800/12800 [==============================] - 1s 105us/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 184/500\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 185/500\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 186/500\n",
      "12800/12800 [==============================] - 1s 95us/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 187/500\n",
      "12800/12800 [==============================] - 1s 94us/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 188/500\n",
      "12800/12800 [==============================] - 1s 94us/step - loss: 0.0020 - val_loss: 0.0020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 189/500\n",
      "12800/12800 [==============================] - 1s 97us/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 190/500\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 191/500\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 192/500\n",
      "12800/12800 [==============================] - 1s 96us/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 193/500\n",
      "12800/12800 [==============================] - 1s 103us/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 194/500\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 195/500\n",
      "12800/12800 [==============================] - 1s 93us/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 196/500\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 197/500\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 198/500\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 199/500\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 200/500\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 201/500\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 202/500\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 203/500\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 204/500\n",
      "12800/12800 [==============================] - 1s 107us/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 205/500\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 206/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 207/500\n",
      "12800/12800 [==============================] - 1s 92us/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 208/500\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 0.0020 - val_loss: 0.0019\n",
      "Epoch 209/500\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 0.0020 - val_loss: 0.0019\n",
      "Epoch 210/500\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 0.0020 - val_loss: 0.0019\n",
      "Epoch 211/500\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 0.0020 - val_loss: 0.0019\n",
      "Epoch 212/500\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 0.0020 - val_loss: 0.0019\n",
      "Epoch 213/500\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 0.0020 - val_loss: 0.0019\n",
      "Epoch 214/500\n",
      "12800/12800 [==============================] - 1s 94us/step - loss: 0.0020 - val_loss: 0.0019\n",
      "Epoch 215/500\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 0.0020 - val_loss: 0.0019\n",
      "Epoch 216/500\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 217/500\n",
      "12800/12800 [==============================] - 1s 104us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 218/500\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 219/500\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 220/500\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 221/500\n",
      "12800/12800 [==============================] - 1s 105us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 222/500\n",
      "12800/12800 [==============================] - 1s 98us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 223/500\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 224/500\n",
      "12800/12800 [==============================] - 1s 92us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 225/500\n",
      "12800/12800 [==============================] - 2s 120us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 226/500\n",
      "12800/12800 [==============================] - 1s 95us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 227/500\n",
      "12800/12800 [==============================] - 1s 114us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 228/500\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 229/500\n",
      "12800/12800 [==============================] - 1s 96us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 230/500\n",
      "12800/12800 [==============================] - 2s 132us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 231/500\n",
      "12800/12800 [==============================] - 1s 101us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 232/500\n",
      "12800/12800 [==============================] - 1s 95us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 233/500\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 234/500\n",
      "12800/12800 [==============================] - 1s 110us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 235/500\n",
      "12800/12800 [==============================] - 1s 111us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 236/500\n",
      "12800/12800 [==============================] - 2s 137us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 237/500\n",
      "12800/12800 [==============================] - 2s 124us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 238/500\n",
      "12800/12800 [==============================] - 1s 111us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 239/500\n",
      "12800/12800 [==============================] - 1s 99us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 240/500\n",
      "12800/12800 [==============================] - 1s 114us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 241/500\n",
      "12800/12800 [==============================] - 2s 128us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 242/500\n",
      "12800/12800 [==============================] - 2s 122us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 243/500\n",
      "12800/12800 [==============================] - 1s 108us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 244/500\n",
      "12800/12800 [==============================] - 1s 102us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 245/500\n",
      "12800/12800 [==============================] - 1s 104us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 246/500\n",
      "12800/12800 [==============================] - 1s 116us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 247/500\n",
      "12800/12800 [==============================] - 2s 126us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 248/500\n",
      "12800/12800 [==============================] - 2s 123us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 249/500\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 250/500\n",
      "12800/12800 [==============================] - 1s 100us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 251/500\n",
      "12800/12800 [==============================] - 2s 133us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 252/500\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 253/500\n",
      "12800/12800 [==============================] - 1s 105us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 254/500\n",
      "12800/12800 [==============================] - 1s 92us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 255/500\n",
      "12800/12800 [==============================] - 1s 93us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 256/500\n",
      "12800/12800 [==============================] - 1s 111us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 257/500\n",
      "12800/12800 [==============================] - 1s 97us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 258/500\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 259/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 0.0019 - val_loss: 0.0018\n",
      "Epoch 260/500\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 0.0019 - val_loss: 0.0018\n",
      "Epoch 261/500\n",
      "12800/12800 [==============================] - 2s 134us/step - loss: 0.0019 - val_loss: 0.0018\n",
      "Epoch 262/500\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.0019 - val_loss: 0.0018\n",
      "Epoch 263/500\n",
      "12800/12800 [==============================] - 1s 101us/step - loss: 0.0019 - val_loss: 0.0018\n",
      "Epoch 264/500\n",
      "12800/12800 [==============================] - 3s 232us/step - loss: 0.0019 - val_loss: 0.0018\n",
      "Epoch 265/500\n",
      "12800/12800 [==============================] - 2s 163us/step - loss: 0.0019 - val_loss: 0.0018\n",
      "Epoch 266/500\n",
      "12800/12800 [==============================] - 2s 125us/step - loss: 0.0019 - val_loss: 0.0018\n",
      "Epoch 267/500\n",
      "12800/12800 [==============================] - 2s 188us/step - loss: 0.0019 - val_loss: 0.0018\n",
      "Epoch 268/500\n",
      "12800/12800 [==============================] - 2s 179us/step - loss: 0.0019 - val_loss: 0.0018\n",
      "Epoch 269/500\n",
      "12800/12800 [==============================] - 2s 135us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 270/500\n",
      "12800/12800 [==============================] - 2s 122us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 271/500\n",
      "12800/12800 [==============================] - 2s 136us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 272/500\n",
      "12800/12800 [==============================] - 1s 102us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 273/500\n",
      "12800/12800 [==============================] - 2s 129us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 274/500\n",
      "12800/12800 [==============================] - 2s 183us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 275/500\n",
      "12800/12800 [==============================] - 2s 122us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 276/500\n",
      "12800/12800 [==============================] - 2s 122us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 277/500\n",
      "12800/12800 [==============================] - 1s 92us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 278/500\n",
      "12800/12800 [==============================] - 1s 92us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 279/500\n",
      "12800/12800 [==============================] - 2s 139us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 280/500\n",
      "12800/12800 [==============================] - 1s 97us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 281/500\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 282/500\n",
      "12800/12800 [==============================] - 1s 94us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 283/500\n",
      "12800/12800 [==============================] - 1s 95us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 284/500\n",
      "12800/12800 [==============================] - 1s 93us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 285/500\n",
      "12800/12800 [==============================] - 1s 97us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 286/500\n",
      "12800/12800 [==============================] - 1s 109us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 287/500\n",
      "12800/12800 [==============================] - 1s 95us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 288/500\n",
      "12800/12800 [==============================] - 1s 97us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 289/500\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 290/500\n",
      "12800/12800 [==============================] - 2s 157us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 291/500\n",
      "12800/12800 [==============================] - 2s 118us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 292/500\n",
      "12800/12800 [==============================] - 2s 136us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 293/500\n",
      "12800/12800 [==============================] - 1s 96us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 294/500\n",
      "12800/12800 [==============================] - 2s 158us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 295/500\n",
      "12800/12800 [==============================] - 2s 144us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 296/500\n",
      "12800/12800 [==============================] - 2s 137us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 297/500\n",
      "12800/12800 [==============================] - 2s 124us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 298/500\n",
      "12800/12800 [==============================] - 1s 104us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 299/500\n",
      "12800/12800 [==============================] - 1s 112us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 300/500\n",
      "12800/12800 [==============================] - 1s 113us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 301/500\n",
      "12800/12800 [==============================] - 1s 101us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 302/500\n",
      "12800/12800 [==============================] - 1s 99us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 303/500\n",
      "12800/12800 [==============================] - 1s 93us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 304/500\n",
      "12800/12800 [==============================] - 1s 104us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 305/500\n",
      "12800/12800 [==============================] - 1s 108us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 306/500\n",
      "12800/12800 [==============================] - 1s 98us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 307/500\n",
      "12800/12800 [==============================] - 1s 101us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 308/500\n",
      "12800/12800 [==============================] - 1s 109us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 309/500\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 310/500\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 311/500\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 312/500\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 313/500\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 314/500\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 315/500\n",
      "12800/12800 [==============================] - 1s 98us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 316/500\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 317/500\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 318/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 319/500\n",
      "12800/12800 [==============================] - 1s 88us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 320/500\n",
      "12800/12800 [==============================] - 1s 95us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 321/500\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 322/500\n",
      "12800/12800 [==============================] - 1s 95us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 323/500\n",
      "12800/12800 [==============================] - 2s 126us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 324/500\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 325/500\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 326/500\n",
      "12800/12800 [==============================] - 1s 97us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 327/500\n",
      "12800/12800 [==============================] - 1s 98us/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 328/500\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 329/500\n",
      "12800/12800 [==============================] - 1s 95us/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 330/500\n",
      "12800/12800 [==============================] - 1s 88us/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 331/500\n",
      "12800/12800 [==============================] - 1s 88us/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 332/500\n",
      "12800/12800 [==============================] - 2s 155us/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 333/500\n",
      "12800/12800 [==============================] - 1s 97us/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 334/500\n",
      "12800/12800 [==============================] - 1s 93us/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 335/500\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 336/500\n",
      "12800/12800 [==============================] - 1s 101us/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 337/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 338/500\n",
      "12800/12800 [==============================] - 1s 88us/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 339/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 1s 82us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 340/500\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 341/500\n",
      "12800/12800 [==============================] - 1s 99us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 342/500\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 343/500\n",
      "12800/12800 [==============================] - 2s 117us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 344/500\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 345/500\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 346/500\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 347/500\n",
      "12800/12800 [==============================] - 1s 110us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 348/500\n",
      "12800/12800 [==============================] - 1s 98us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 349/500\n",
      "12800/12800 [==============================] - 1s 109us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 350/500\n",
      "12800/12800 [==============================] - 1s 99us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 351/500\n",
      "12800/12800 [==============================] - 1s 116us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 352/500\n",
      "12800/12800 [==============================] - 1s 100us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 353/500\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 354/500\n",
      "12800/12800 [==============================] - 1s 108us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 355/500\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 356/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 357/500\n",
      "12800/12800 [==============================] - 1s 99us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 358/500\n",
      "12800/12800 [==============================] - 1s 92us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 359/500\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 360/500\n",
      "12800/12800 [==============================] - 1s 98us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 361/500\n",
      "12800/12800 [==============================] - 1s 106us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 362/500\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 363/500\n",
      "12800/12800 [==============================] - 2s 130us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 364/500\n",
      "12800/12800 [==============================] - 2s 161us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 365/500\n",
      "12800/12800 [==============================] - 2s 191us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 366/500\n",
      "12800/12800 [==============================] - 1s 112us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 367/500\n",
      "12800/12800 [==============================] - 2s 143us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 368/500\n",
      "12800/12800 [==============================] - 2s 117us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 369/500\n",
      "12800/12800 [==============================] - 1s 101us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 370/500\n",
      "12800/12800 [==============================] - 1s 105us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 371/500\n",
      "12800/12800 [==============================] - 1s 99us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 372/500\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 373/500\n",
      "12800/12800 [==============================] - 2s 131us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 374/500\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 375/500\n",
      "12800/12800 [==============================] - 1s 101us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 376/500\n",
      "12800/12800 [==============================] - 1s 113us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 377/500\n",
      "12800/12800 [==============================] - 1s 112us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 378/500\n",
      "12800/12800 [==============================] - 2s 118us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 379/500\n",
      "12800/12800 [==============================] - 2s 123us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 380/500\n",
      "12800/12800 [==============================] - 2s 156us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 381/500\n",
      "12800/12800 [==============================] - 1s 111us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 382/500\n",
      "12800/12800 [==============================] - 1s 104us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 383/500\n",
      "12800/12800 [==============================] - 1s 93us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 384/500\n",
      "12800/12800 [==============================] - 2s 125us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 385/500\n",
      "12800/12800 [==============================] - 1s 111us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 386/500\n",
      "12800/12800 [==============================] - 2s 119us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 387/500\n",
      "12800/12800 [==============================] - 2s 122us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 388/500\n",
      "12800/12800 [==============================] - 1s 96us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 389/500\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 390/500\n",
      "12800/12800 [==============================] - 1s 102us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 391/500\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 392/500\n",
      "12800/12800 [==============================] - 1s 109us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 393/500\n",
      "12800/12800 [==============================] - 1s 112us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 394/500\n",
      "12800/12800 [==============================] - 2s 140us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 395/500\n",
      "12800/12800 [==============================] - 2s 163us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 396/500\n",
      "12800/12800 [==============================] - 2s 159us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 397/500\n",
      "12800/12800 [==============================] - 2s 119us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 398/500\n",
      "12800/12800 [==============================] - 2s 155us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 399/500\n",
      "12800/12800 [==============================] - 2s 138us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 400/500\n",
      "12800/12800 [==============================] - 2s 166us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 401/500\n",
      "12800/12800 [==============================] - 2s 122us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 402/500\n",
      "12800/12800 [==============================] - 2s 117us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 403/500\n",
      "12800/12800 [==============================] - 2s 162us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 404/500\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 405/500\n",
      "12800/12800 [==============================] - 3s 216us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 406/500\n",
      "12800/12800 [==============================] - 2s 142us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 407/500\n",
      "12800/12800 [==============================] - 2s 175us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 408/500\n",
      "12800/12800 [==============================] - 3s 198us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 409/500\n",
      "12800/12800 [==============================] - 3s 215us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 410/500\n",
      "12800/12800 [==============================] - 2s 150us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 411/500\n",
      "12800/12800 [==============================] - 2s 132us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 412/500\n",
      "12800/12800 [==============================] - 2s 136us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 413/500\n",
      "12800/12800 [==============================] - 1s 115us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 414/500\n",
      "12800/12800 [==============================] - 1s 99us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 415/500\n",
      "12800/12800 [==============================] - 1s 103us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 416/500\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 417/500\n",
      "12800/12800 [==============================] - 2s 140us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 418/500\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 419/500\n",
      "12800/12800 [==============================] - 1s 88us/step - loss: 0.0017 - val_loss: 0.0016\n",
      "Epoch 420/500\n",
      "12800/12800 [==============================] - 1s 99us/step - loss: 0.0017 - val_loss: 0.0016\n",
      "Epoch 421/500\n",
      "12800/12800 [==============================] - 2s 119us/step - loss: 0.0017 - val_loss: 0.0016\n",
      "Epoch 422/500\n",
      "12800/12800 [==============================] - 2s 150us/step - loss: 0.0017 - val_loss: 0.0016\n",
      "Epoch 423/500\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.0017 - val_loss: 0.0016\n",
      "Epoch 424/500\n",
      "12800/12800 [==============================] - 2s 141us/step - loss: 0.0017 - val_loss: 0.0016\n",
      "Epoch 425/500\n",
      "12800/12800 [==============================] - 2s 121us/step - loss: 0.0017 - val_loss: 0.0016\n",
      "Epoch 426/500\n",
      "12800/12800 [==============================] - 1s 92us/step - loss: 0.0017 - val_loss: 0.0016\n",
      "Epoch 427/500\n",
      "12800/12800 [==============================] - 2s 124us/step - loss: 0.0017 - val_loss: 0.0016\n",
      "Epoch 428/500\n",
      "12800/12800 [==============================] - 2s 122us/step - loss: 0.0017 - val_loss: 0.0016\n",
      "Epoch 429/500\n",
      "12800/12800 [==============================] - 2s 152us/step - loss: 0.0017 - val_loss: 0.0016\n",
      "Epoch 430/500\n",
      "12800/12800 [==============================] - 2s 139us/step - loss: 0.0017 - val_loss: 0.0016\n",
      "Epoch 431/500\n",
      "12800/12800 [==============================] - 1s 97us/step - loss: 0.0017 - val_loss: 0.0016\n",
      "Epoch 432/500\n",
      "12800/12800 [==============================] - 1s 111us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 433/500\n",
      "12800/12800 [==============================] - 2s 118us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 434/500\n",
      "12800/12800 [==============================] - 1s 96us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 435/500\n",
      "12800/12800 [==============================] - 1s 92us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 436/500\n",
      "12800/12800 [==============================] - 1s 103us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 437/500\n",
      "12800/12800 [==============================] - 1s 104us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 438/500\n",
      "12800/12800 [==============================] - 1s 109us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 439/500\n",
      "12800/12800 [==============================] - 2s 128us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 440/500\n",
      "12800/12800 [==============================] - 1s 95us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 441/500\n",
      "12800/12800 [==============================] - 1s 97us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 442/500\n",
      "12800/12800 [==============================] - 2s 122us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 443/500\n",
      "12800/12800 [==============================] - 2s 131us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 444/500\n",
      "12800/12800 [==============================] - 1s 109us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 445/500\n",
      "12800/12800 [==============================] - 1s 112us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 446/500\n",
      "12800/12800 [==============================] - 1s 103us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 447/500\n",
      "12800/12800 [==============================] - 1s 97us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 448/500\n",
      "12800/12800 [==============================] - 2s 130us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 449/500\n",
      "12800/12800 [==============================] - 1s 113us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 450/500\n",
      "12800/12800 [==============================] - 1s 104us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 451/500\n",
      "12800/12800 [==============================] - 1s 115us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 452/500\n",
      "12800/12800 [==============================] - 1s 106us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 453/500\n",
      "12800/12800 [==============================] - 1s 95us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 454/500\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 455/500\n",
      "12800/12800 [==============================] - 1s 92us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 456/500\n",
      "12800/12800 [==============================] - 1s 97us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 457/500\n",
      "12800/12800 [==============================] - 2s 117us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 458/500\n",
      "12800/12800 [==============================] - 1s 97us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 459/500\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 460/500\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 461/500\n",
      "12800/12800 [==============================] - 2s 123us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 462/500\n",
      "12800/12800 [==============================] - 1s 105us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 463/500\n",
      "12800/12800 [==============================] - 1s 107us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 464/500\n",
      "12800/12800 [==============================] - 1s 92us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 465/500\n",
      "12800/12800 [==============================] - 1s 97us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 466/500\n",
      "12800/12800 [==============================] - 1s 93us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 467/500\n",
      "12800/12800 [==============================] - 2s 131us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 468/500\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 469/500\n",
      "12800/12800 [==============================] - 2s 142us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 470/500\n",
      "12800/12800 [==============================] - 2s 133us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 471/500\n",
      "12800/12800 [==============================] - 1s 108us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 472/500\n",
      "12800/12800 [==============================] - 1s 112us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 473/500\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 474/500\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 475/500\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 476/500\n",
      "12800/12800 [==============================] - 1s 100us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 477/500\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 478/500\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 479/500\n",
      "12800/12800 [==============================] - 2s 119us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 480/500\n",
      "12800/12800 [==============================] - 1s 106us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 481/500\n",
      "12800/12800 [==============================] - 1s 102us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 482/500\n",
      "12800/12800 [==============================] - 2s 132us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 483/500\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 484/500\n",
      "12800/12800 [==============================] - 1s 116us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 485/500\n",
      "12800/12800 [==============================] - 2s 119us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 486/500\n",
      "12800/12800 [==============================] - 1s 100us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 487/500\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 488/500\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 489/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 1s 80us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 490/500\n",
      "12800/12800 [==============================] - 1s 92us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 491/500\n",
      "12800/12800 [==============================] - 1s 95us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 492/500\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 493/500\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 494/500\n",
      "12800/12800 [==============================] - 1s 112us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 495/500\n",
      "12800/12800 [==============================] - 1s 102us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 496/500\n",
      "12800/12800 [==============================] - 1s 102us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 497/500\n",
      "12800/12800 [==============================] - 1s 104us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 498/500\n",
      "12800/12800 [==============================] - 2s 122us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 499/500\n",
      "12800/12800 [==============================] - 1s 102us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 500/500\n",
      "12800/12800 [==============================] - 1s 100us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Test MSE: 0.0016156173\n",
      "Total MSE: 0.0015927902\n",
      "1e-05\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input-layer (InputLayer)     (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "hidden-layer (Dense)         (None, 75)                22575     \n",
      "_________________________________________________________________\n",
      "output-layer (Dense)         (None, 300)               22800     \n",
      "=================================================================\n",
      "Total params: 45,375\n",
      "Trainable params: 45,375\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/500\n",
      "12800/12800 [==============================] - 3s 203us/step - loss: 0.0162 - val_loss: 0.0155\n",
      "Epoch 2/500\n",
      "12800/12800 [==============================] - 1s 99us/step - loss: 0.0157 - val_loss: 0.0151\n",
      "Epoch 3/500\n",
      "12800/12800 [==============================] - 2s 181us/step - loss: 0.0153 - val_loss: 0.0149\n",
      "Epoch 4/500\n",
      "12800/12800 [==============================] - 2s 125us/step - loss: 0.0151 - val_loss: 0.0146\n",
      "Epoch 5/500\n",
      "12800/12800 [==============================] - 2s 144us/step - loss: 0.0149 - val_loss: 0.0145\n",
      "Epoch 6/500\n",
      "12800/12800 [==============================] - 1s 102us/step - loss: 0.0147 - val_loss: 0.0143\n",
      "Epoch 7/500\n",
      "12800/12800 [==============================] - 1s 95us/step - loss: 0.0145 - val_loss: 0.0141\n",
      "Epoch 8/500\n",
      "12800/12800 [==============================] - 2s 132us/step - loss: 0.0144 - val_loss: 0.0140\n",
      "Epoch 9/500\n",
      "12800/12800 [==============================] - 2s 154us/step - loss: 0.0142 - val_loss: 0.0139\n",
      "Epoch 10/500\n",
      "12800/12800 [==============================] - 2s 135us/step - loss: 0.0141 - val_loss: 0.0138\n",
      "Epoch 11/500\n",
      "12800/12800 [==============================] - 2s 126us/step - loss: 0.0140 - val_loss: 0.0137\n",
      "Epoch 12/500\n",
      "12800/12800 [==============================] - 2s 122us/step - loss: 0.0139 - val_loss: 0.0136\n",
      "Epoch 13/500\n",
      "12800/12800 [==============================] - 1s 95us/step - loss: 0.0138 - val_loss: 0.0135\n",
      "Epoch 14/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 0.0137 - val_loss: 0.0134\n",
      "Epoch 15/500\n",
      "12800/12800 [==============================] - 1s 100us/step - loss: 0.0136 - val_loss: 0.0133\n",
      "Epoch 16/500\n",
      "12800/12800 [==============================] - 1s 94us/step - loss: 0.0135 - val_loss: 0.0132\n",
      "Epoch 17/500\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 0.0134 - val_loss: 0.0131\n",
      "Epoch 18/500\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 0.0134 - val_loss: 0.0131\n",
      "Epoch 19/500\n",
      "12800/12800 [==============================] - 1s 88us/step - loss: 0.0133 - val_loss: 0.0130\n",
      "Epoch 20/500\n",
      "12800/12800 [==============================] - 1s 101us/step - loss: 0.0132 - val_loss: 0.0129\n",
      "Epoch 21/500\n",
      "12800/12800 [==============================] - 1s 113us/step - loss: 0.0131 - val_loss: 0.0128\n",
      "Epoch 22/500\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 0.0131 - val_loss: 0.0128\n",
      "Epoch 23/500\n",
      "12800/12800 [==============================] - 2s 122us/step - loss: 0.0130 - val_loss: 0.0127\n",
      "Epoch 24/500\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 0.0130 - val_loss: 0.0127\n",
      "Epoch 25/500\n",
      "12800/12800 [==============================] - 2s 125us/step - loss: 0.0129 - val_loss: 0.0126\n",
      "Epoch 26/500\n",
      "12800/12800 [==============================] - 2s 124us/step - loss: 0.0128 - val_loss: 0.0125\n",
      "Epoch 27/500\n",
      "12800/12800 [==============================] - 2s 121us/step - loss: 0.0128 - val_loss: 0.0125\n",
      "Epoch 28/500\n",
      "12800/12800 [==============================] - 1s 110us/step - loss: 0.0127 - val_loss: 0.0124\n",
      "Epoch 29/500\n",
      "12800/12800 [==============================] - 1s 99us/step - loss: 0.0127 - val_loss: 0.0124\n",
      "Epoch 30/500\n",
      "12800/12800 [==============================] - 1s 93us/step - loss: 0.0126 - val_loss: 0.0123\n",
      "Epoch 31/500\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 0.0126 - val_loss: 0.0123\n",
      "Epoch 32/500\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 0.0125 - val_loss: 0.0122\n",
      "Epoch 33/500\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 0.0125 - val_loss: 0.0122\n",
      "Epoch 34/500\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 0.0124 - val_loss: 0.0121\n",
      "Epoch 35/500\n",
      "12800/12800 [==============================] - 1s 97us/step - loss: 0.0124 - val_loss: 0.0121\n",
      "Epoch 36/500\n",
      "12800/12800 [==============================] - 1s 111us/step - loss: 0.0123 - val_loss: 0.0121\n",
      "Epoch 37/500\n",
      "12800/12800 [==============================] - 2s 128us/step - loss: 0.0123 - val_loss: 0.0120\n",
      "Epoch 38/500\n",
      "12800/12800 [==============================] - 2s 119us/step - loss: 0.0122 - val_loss: 0.0120\n",
      "Epoch 39/500\n",
      "12800/12800 [==============================] - 1s 101us/step - loss: 0.0122 - val_loss: 0.0119\n",
      "Epoch 40/500\n",
      "12800/12800 [==============================] - 1s 94us/step - loss: 0.0122 - val_loss: 0.0119\n",
      "Epoch 41/500\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 0.0121 - val_loss: 0.0118\n",
      "Epoch 42/500\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 0.0121 - val_loss: 0.0118\n",
      "Epoch 43/500\n",
      "12800/12800 [==============================] - 2s 120us/step - loss: 0.0120 - val_loss: 0.0118\n",
      "Epoch 44/500\n",
      "12800/12800 [==============================] - 1s 93us/step - loss: 0.0120 - val_loss: 0.0117\n",
      "Epoch 45/500\n",
      "12800/12800 [==============================] - 2s 127us/step - loss: 0.0120 - val_loss: 0.0117\n",
      "Epoch 46/500\n",
      "12800/12800 [==============================] - 1s 94us/step - loss: 0.0119 - val_loss: 0.0117\n",
      "Epoch 47/500\n",
      "12800/12800 [==============================] - 1s 104us/step - loss: 0.0119 - val_loss: 0.0116\n",
      "Epoch 48/500\n",
      "12800/12800 [==============================] - 1s 103us/step - loss: 0.0118 - val_loss: 0.0116\n",
      "Epoch 49/500\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 0.0118 - val_loss: 0.0116\n",
      "Epoch 50/500\n",
      "12800/12800 [==============================] - 1s 88us/step - loss: 0.0118 - val_loss: 0.0115\n",
      "Epoch 51/500\n",
      "12800/12800 [==============================] - 2s 118us/step - loss: 0.0117 - val_loss: 0.0115\n",
      "Epoch 52/500\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 0.0117 - val_loss: 0.0114\n",
      "Epoch 53/500\n",
      "12800/12800 [==============================] - 1s 112us/step - loss: 0.0117 - val_loss: 0.0114\n",
      "Epoch 54/500\n",
      "12800/12800 [==============================] - 1s 104us/step - loss: 0.0116 - val_loss: 0.0114\n",
      "Epoch 55/500\n",
      "12800/12800 [==============================] - 1s 100us/step - loss: 0.0116 - val_loss: 0.0114\n",
      "Epoch 56/500\n",
      "12800/12800 [==============================] - 2s 131us/step - loss: 0.0116 - val_loss: 0.0113\n",
      "Epoch 57/500\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.0115 - val_loss: 0.0113\n",
      "Epoch 58/500\n",
      "12800/12800 [==============================] - 1s 98us/step - loss: 0.0115 - val_loss: 0.0113\n",
      "Epoch 59/500\n",
      "12800/12800 [==============================] - 2s 132us/step - loss: 0.0115 - val_loss: 0.0112\n",
      "Epoch 60/500\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.0114 - val_loss: 0.0112\n",
      "Epoch 61/500\n",
      "12800/12800 [==============================] - 2s 171us/step - loss: 0.0114 - val_loss: 0.0112\n",
      "Epoch 62/500\n",
      "12800/12800 [==============================] - 2s 125us/step - loss: 0.0114 - val_loss: 0.0111\n",
      "Epoch 63/500\n",
      "12800/12800 [==============================] - 2s 119us/step - loss: 0.0114 - val_loss: 0.0111\n",
      "Epoch 64/500\n",
      "12800/12800 [==============================] - 1s 88us/step - loss: 0.0113 - val_loss: 0.0111\n",
      "Epoch 65/500\n",
      "12800/12800 [==============================] - 1s 95us/step - loss: 0.0113 - val_loss: 0.0111\n",
      "Epoch 66/500\n",
      "12800/12800 [==============================] - 1s 88us/step - loss: 0.0113 - val_loss: 0.0110\n",
      "Epoch 67/500\n",
      "12800/12800 [==============================] - 1s 101us/step - loss: 0.0112 - val_loss: 0.0110\n",
      "Epoch 68/500\n",
      "12800/12800 [==============================] - 1s 98us/step - loss: 0.0112 - val_loss: 0.0110\n",
      "Epoch 69/500\n",
      "12800/12800 [==============================] - 1s 100us/step - loss: 0.0112 - val_loss: 0.0109\n",
      "Epoch 70/500\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 0.0112 - val_loss: 0.0109\n",
      "Epoch 71/500\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 0.0111 - val_loss: 0.0109\n",
      "Epoch 72/500\n",
      "12800/12800 [==============================] - 1s 111us/step - loss: 0.0111 - val_loss: 0.0109\n",
      "Epoch 73/500\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 0.0111 - val_loss: 0.0108\n",
      "Epoch 74/500\n",
      "12800/12800 [==============================] - 1s 98us/step - loss: 0.0111 - val_loss: 0.0108\n",
      "Epoch 75/500\n",
      "12800/12800 [==============================] - 2s 119us/step - loss: 0.0110 - val_loss: 0.0108\n",
      "Epoch 76/500\n",
      "12800/12800 [==============================] - 2s 118us/step - loss: 0.0110 - val_loss: 0.0108\n",
      "Epoch 77/500\n",
      "12800/12800 [==============================] - 2s 139us/step - loss: 0.0110 - val_loss: 0.0107\n",
      "Epoch 78/500\n",
      "12800/12800 [==============================] - 1s 101us/step - loss: 0.0110 - val_loss: 0.0107\n",
      "Epoch 79/500\n",
      "12800/12800 [==============================] - 1s 96us/step - loss: 0.0109 - val_loss: 0.0107\n",
      "Epoch 80/500\n",
      "12800/12800 [==============================] - 1s 97us/step - loss: 0.0109 - val_loss: 0.0107\n",
      "Epoch 81/500\n",
      "12800/12800 [==============================] - 1s 105us/step - loss: 0.0109 - val_loss: 0.0107\n",
      "Epoch 82/500\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 0.0109 - val_loss: 0.0106\n",
      "Epoch 83/500\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 0.0108 - val_loss: 0.0106\n",
      "Epoch 84/500\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 0.0108 - val_loss: 0.0106\n",
      "Epoch 85/500\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 0.0108 - val_loss: 0.0106\n",
      "Epoch 86/500\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 0.0108 - val_loss: 0.0105\n",
      "Epoch 87/500\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 0.0107 - val_loss: 0.0105\n",
      "Epoch 88/500\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 0.0107 - val_loss: 0.0105\n",
      "Epoch 89/500\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 0.0107 - val_loss: 0.0105\n",
      "Epoch 90/500\n",
      "12800/12800 [==============================] - 2s 128us/step - loss: 0.0107 - val_loss: 0.0104\n",
      "Epoch 91/500\n",
      "12800/12800 [==============================] - 1s 114us/step - loss: 0.0106 - val_loss: 0.0104\n",
      "Epoch 92/500\n",
      "12800/12800 [==============================] - 1s 112us/step - loss: 0.0106 - val_loss: 0.0104\n",
      "Epoch 93/500\n",
      "12800/12800 [==============================] - 1s 96us/step - loss: 0.0106 - val_loss: 0.0104\n",
      "Epoch 94/500\n",
      "12800/12800 [==============================] - 2s 124us/step - loss: 0.0106 - val_loss: 0.0104\n",
      "Epoch 95/500\n",
      "12800/12800 [==============================] - 1s 116us/step - loss: 0.0106 - val_loss: 0.0103\n",
      "Epoch 96/500\n",
      "12800/12800 [==============================] - 1s 112us/step - loss: 0.0105 - val_loss: 0.0103\n",
      "Epoch 97/500\n",
      "12800/12800 [==============================] - 1s 105us/step - loss: 0.0105 - val_loss: 0.0103\n",
      "Epoch 98/500\n",
      "12800/12800 [==============================] - 1s 101us/step - loss: 0.0105 - val_loss: 0.0103\n",
      "Epoch 99/500\n",
      "12800/12800 [==============================] - 1s 114us/step - loss: 0.0105 - val_loss: 0.0103\n",
      "Epoch 100/500\n",
      "12800/12800 [==============================] - 1s 100us/step - loss: 0.0105 - val_loss: 0.0102\n",
      "Epoch 101/500\n",
      "12800/12800 [==============================] - 1s 94us/step - loss: 0.0104 - val_loss: 0.0102\n",
      "Epoch 102/500\n",
      "12800/12800 [==============================] - 1s 96us/step - loss: 0.0104 - val_loss: 0.0102\n",
      "Epoch 103/500\n",
      "12800/12800 [==============================] - 1s 115us/step - loss: 0.0104 - val_loss: 0.0102\n",
      "Epoch 104/500\n",
      "12800/12800 [==============================] - 1s 92us/step - loss: 0.0104 - val_loss: 0.0102\n",
      "Epoch 105/500\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 0.0104 - val_loss: 0.0101\n",
      "Epoch 106/500\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 0.0103 - val_loss: 0.0101\n",
      "Epoch 107/500\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 0.0103 - val_loss: 0.0101\n",
      "Epoch 108/500\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 0.0103 - val_loss: 0.0101\n",
      "Epoch 109/500\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 0.0103 - val_loss: 0.0101\n",
      "Epoch 110/500\n",
      "12800/12800 [==============================] - 1s 100us/step - loss: 0.0103 - val_loss: 0.0101\n",
      "Epoch 111/500\n",
      "12800/12800 [==============================] - 2s 158us/step - loss: 0.0102 - val_loss: 0.0100\n",
      "Epoch 112/500\n",
      "12800/12800 [==============================] - 1s 109us/step - loss: 0.0102 - val_loss: 0.0100\n",
      "Epoch 113/500\n",
      "12800/12800 [==============================] - 1s 92us/step - loss: 0.0102 - val_loss: 0.0100\n",
      "Epoch 114/500\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 0.0102 - val_loss: 0.0100\n",
      "Epoch 115/500\n",
      "12800/12800 [==============================] - 1s 94us/step - loss: 0.0102 - val_loss: 0.0100\n",
      "Epoch 116/500\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 0.0102 - val_loss: 0.0099\n",
      "Epoch 117/500\n",
      "12800/12800 [==============================] - 2s 135us/step - loss: 0.0101 - val_loss: 0.0099\n",
      "Epoch 118/500\n",
      "12800/12800 [==============================] - 2s 125us/step - loss: 0.0101 - val_loss: 0.0099\n",
      "Epoch 119/500\n",
      "12800/12800 [==============================] - 1s 106us/step - loss: 0.0101 - val_loss: 0.0099\n",
      "Epoch 120/500\n",
      "12800/12800 [==============================] - 1s 98us/step - loss: 0.0101 - val_loss: 0.0099\n",
      "Epoch 121/500\n",
      "12800/12800 [==============================] - 2s 130us/step - loss: 0.0101 - val_loss: 0.0099\n",
      "Epoch 122/500\n",
      "12800/12800 [==============================] - 2s 126us/step - loss: 0.0100 - val_loss: 0.0098\n",
      "Epoch 123/500\n",
      "12800/12800 [==============================] - 2s 122us/step - loss: 0.0100 - val_loss: 0.0098\n",
      "Epoch 124/500\n",
      "12800/12800 [==============================] - 1s 114us/step - loss: 0.0100 - val_loss: 0.0098\n",
      "Epoch 125/500\n",
      "12800/12800 [==============================] - 1s 112us/step - loss: 0.0100 - val_loss: 0.0098\n",
      "Epoch 126/500\n",
      "12800/12800 [==============================] - 2s 187us/step - loss: 0.0100 - val_loss: 0.0098\n",
      "Epoch 127/500\n",
      "12800/12800 [==============================] - 1s 102us/step - loss: 0.0100 - val_loss: 0.0098\n",
      "Epoch 128/500\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 0.0099 - val_loss: 0.0097\n",
      "Epoch 129/500\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 0.0099 - val_loss: 0.0097\n",
      "Epoch 130/500\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 0.0099 - val_loss: 0.0097\n",
      "Epoch 131/500\n",
      "12800/12800 [==============================] - 2s 154us/step - loss: 0.0099 - val_loss: 0.0097\n",
      "Epoch 132/500\n",
      "12800/12800 [==============================] - 1s 104us/step - loss: 0.0099 - val_loss: 0.0097\n",
      "Epoch 133/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 2s 132us/step - loss: 0.0099 - val_loss: 0.0097\n",
      "Epoch 134/500\n",
      "12800/12800 [==============================] - 1s 102us/step - loss: 0.0098 - val_loss: 0.0096\n",
      "Epoch 135/500\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 0.0098 - val_loss: 0.0096\n",
      "Epoch 136/500\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 0.0098 - val_loss: 0.0096\n",
      "Epoch 137/500\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 0.0098 - val_loss: 0.0096\n",
      "Epoch 138/500\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 0.0098 - val_loss: 0.0096\n",
      "Epoch 139/500\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 0.0098 - val_loss: 0.0096\n",
      "Epoch 140/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 0.0097 - val_loss: 0.0095\n",
      "Epoch 141/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 0.0097 - val_loss: 0.0095\n",
      "Epoch 142/500\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 0.0097 - val_loss: 0.0095\n",
      "Epoch 143/500\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 0.0097 - val_loss: 0.0095\n",
      "Epoch 144/500\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 0.0097 - val_loss: 0.0095\n",
      "Epoch 145/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 0.0097 - val_loss: 0.0095\n",
      "Epoch 146/500\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 0.0097 - val_loss: 0.0095\n",
      "Epoch 147/500\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 0.0096 - val_loss: 0.0094\n",
      "Epoch 148/500\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 0.0096 - val_loss: 0.0094\n",
      "Epoch 149/500\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 0.0096 - val_loss: 0.0094\n",
      "Epoch 150/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 0.0096 - val_loss: 0.0094\n",
      "Epoch 151/500\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 0.0096 - val_loss: 0.0094\n",
      "Epoch 152/500\n",
      "12800/12800 [==============================] - 1s 95us/step - loss: 0.0096 - val_loss: 0.0094\n",
      "Epoch 153/500\n",
      "12800/12800 [==============================] - 1s 105us/step - loss: 0.0095 - val_loss: 0.0094\n",
      "Epoch 154/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 0.0095 - val_loss: 0.0093\n",
      "Epoch 155/500\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 0.0095 - val_loss: 0.0093\n",
      "Epoch 156/500\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 0.0095 - val_loss: 0.0093\n",
      "Epoch 157/500\n",
      "12800/12800 [==============================] - 1s 88us/step - loss: 0.0095 - val_loss: 0.0093\n",
      "Epoch 158/500\n",
      "12800/12800 [==============================] - 1s 95us/step - loss: 0.0095 - val_loss: 0.0093\n",
      "Epoch 159/500\n",
      "12800/12800 [==============================] - 1s 98us/step - loss: 0.0095 - val_loss: 0.0093\n",
      "Epoch 160/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 0.0094 - val_loss: 0.0093\n",
      "Epoch 161/500\n",
      "12800/12800 [==============================] - 1s 93us/step - loss: 0.0094 - val_loss: 0.0092\n",
      "Epoch 162/500\n",
      "12800/12800 [==============================] - 1s 102us/step - loss: 0.0094 - val_loss: 0.0092\n",
      "Epoch 163/500\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 0.0094 - val_loss: 0.0092\n",
      "Epoch 164/500\n",
      "12800/12800 [==============================] - 1s 88us/step - loss: 0.0094 - val_loss: 0.0092\n",
      "Epoch 165/500\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 0.0094 - val_loss: 0.0092\n",
      "Epoch 166/500\n",
      "12800/12800 [==============================] - 1s 88us/step - loss: 0.0094 - val_loss: 0.0092\n",
      "Epoch 167/500\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 0.0094 - val_loss: 0.0092\n",
      "Epoch 168/500\n",
      "12800/12800 [==============================] - 1s 88us/step - loss: 0.0093 - val_loss: 0.0092\n",
      "Epoch 169/500\n",
      "12800/12800 [==============================] - 1s 98us/step - loss: 0.0093 - val_loss: 0.0091\n",
      "Epoch 170/500\n",
      "12800/12800 [==============================] - 1s 93us/step - loss: 0.0093 - val_loss: 0.0091\n",
      "Epoch 171/500\n",
      "12800/12800 [==============================] - 1s 95us/step - loss: 0.0093 - val_loss: 0.0091\n",
      "Epoch 172/500\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 0.0093 - val_loss: 0.0091\n",
      "Epoch 173/500\n",
      "12800/12800 [==============================] - 1s 93us/step - loss: 0.0093 - val_loss: 0.0091\n",
      "Epoch 174/500\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 0.0093 - val_loss: 0.0091\n",
      "Epoch 175/500\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 0.0092 - val_loss: 0.0091\n",
      "Epoch 176/500\n",
      "12800/12800 [==============================] - 2s 124us/step - loss: 0.0092 - val_loss: 0.0090\n",
      "Epoch 177/500\n",
      "12800/12800 [==============================] - 1s 95us/step - loss: 0.0092 - val_loss: 0.0090\n",
      "Epoch 178/500\n",
      "12800/12800 [==============================] - 1s 100us/step - loss: 0.0092 - val_loss: 0.0090\n",
      "Epoch 179/500\n",
      "12800/12800 [==============================] - 1s 92us/step - loss: 0.0092 - val_loss: 0.0090\n",
      "Epoch 180/500\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 0.0092 - val_loss: 0.0090\n",
      "Epoch 181/500\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 0.0092 - val_loss: 0.0090\n",
      "Epoch 182/500\n",
      "12800/12800 [==============================] - 1s 97us/step - loss: 0.0092 - val_loss: 0.0090\n",
      "Epoch 183/500\n",
      "12800/12800 [==============================] - 1s 96us/step - loss: 0.0091 - val_loss: 0.0090\n",
      "Epoch 184/500\n",
      "12800/12800 [==============================] - 2s 125us/step - loss: 0.0091 - val_loss: 0.0089\n",
      "Epoch 185/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 0.0091 - val_loss: 0.0089\n",
      "Epoch 186/500\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 0.0091 - val_loss: 0.0089\n",
      "Epoch 187/500\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 0.0091 - val_loss: 0.0089\n",
      "Epoch 188/500\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 0.0091 - val_loss: 0.0089\n",
      "Epoch 189/500\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 0.0091 - val_loss: 0.0089\n",
      "Epoch 190/500\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 0.0091 - val_loss: 0.0089\n",
      "Epoch 191/500\n",
      "12800/12800 [==============================] - 1s 100us/step - loss: 0.0090 - val_loss: 0.0089\n",
      "Epoch 192/500\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 0.0090 - val_loss: 0.0089\n",
      "Epoch 193/500\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 0.0090 - val_loss: 0.0088\n",
      "Epoch 194/500\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 0.0090 - val_loss: 0.0088\n",
      "Epoch 195/500\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 0.0090 - val_loss: 0.0088\n",
      "Epoch 196/500\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 0.0090 - val_loss: 0.0088\n",
      "Epoch 197/500\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 0.0090 - val_loss: 0.0088\n",
      "Epoch 198/500\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 0.0090 - val_loss: 0.0088\n",
      "Epoch 199/500\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 0.0089 - val_loss: 0.0088\n",
      "Epoch 200/500\n",
      "12800/12800 [==============================] - 1s 110us/step - loss: 0.0089 - val_loss: 0.0088\n",
      "Epoch 201/500\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 0.0089 - val_loss: 0.0087\n",
      "Epoch 202/500\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 0.0089 - val_loss: 0.0087\n",
      "Epoch 203/500\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 0.0089 - val_loss: 0.0087\n",
      "Epoch 204/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 0.0089 - val_loss: 0.0087\n",
      "Epoch 205/500\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 0.0089 - val_loss: 0.0087\n",
      "Epoch 206/500\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 0.0089 - val_loss: 0.0087\n",
      "Epoch 207/500\n",
      "12800/12800 [==============================] - 1s 92us/step - loss: 0.0089 - val_loss: 0.0087\n",
      "Epoch 208/500\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 0.0088 - val_loss: 0.0087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 209/500\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 0.0088 - val_loss: 0.0087\n",
      "Epoch 210/500\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 0.0088 - val_loss: 0.0086\n",
      "Epoch 211/500\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 0.0088 - val_loss: 0.0086\n",
      "Epoch 212/500\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 0.0088 - val_loss: 0.0086\n",
      "Epoch 213/500\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 0.0088 - val_loss: 0.0086\n",
      "Epoch 214/500\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 0.0088 - val_loss: 0.0086\n",
      "Epoch 215/500\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 0.0088 - val_loss: 0.0086\n",
      "Epoch 216/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 0.0087 - val_loss: 0.0086\n",
      "Epoch 217/500\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 0.0087 - val_loss: 0.0086\n",
      "Epoch 218/500\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 0.0087 - val_loss: 0.0086\n",
      "Epoch 219/500\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 0.0087 - val_loss: 0.0085\n",
      "Epoch 220/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 0.0087 - val_loss: 0.0085\n",
      "Epoch 221/500\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 0.0087 - val_loss: 0.0085\n",
      "Epoch 222/500\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 0.0087 - val_loss: 0.0085\n",
      "Epoch 223/500\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 0.0087 - val_loss: 0.0085\n",
      "Epoch 224/500\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 0.0087 - val_loss: 0.0085\n",
      "Epoch 225/500\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 0.0087 - val_loss: 0.0085\n",
      "Epoch 226/500\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 0.0086 - val_loss: 0.0085\n",
      "Epoch 227/500\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 0.0086 - val_loss: 0.0085\n",
      "Epoch 228/500\n",
      "12800/12800 [==============================] - 1s 96us/step - loss: 0.0086 - val_loss: 0.0085\n",
      "Epoch 229/500\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 0.0086 - val_loss: 0.0084\n",
      "Epoch 230/500\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 0.0086 - val_loss: 0.0084\n",
      "Epoch 231/500\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 0.0086 - val_loss: 0.0084\n",
      "Epoch 232/500\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 0.0086 - val_loss: 0.0084\n",
      "Epoch 233/500\n",
      "12800/12800 [==============================] - 1s 88us/step - loss: 0.0086 - val_loss: 0.0084\n",
      "Epoch 234/500\n",
      "12800/12800 [==============================] - 1s 88us/step - loss: 0.0086 - val_loss: 0.0084\n",
      "Epoch 235/500\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 0.0085 - val_loss: 0.0084\n",
      "Epoch 236/500\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 0.0085 - val_loss: 0.0084\n",
      "Epoch 237/500\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 0.0085 - val_loss: 0.0084\n",
      "Epoch 238/500\n",
      "12800/12800 [==============================] - 1s 106us/step - loss: 0.0085 - val_loss: 0.0084\n",
      "Epoch 239/500\n",
      "12800/12800 [==============================] - 2s 132us/step - loss: 0.0085 - val_loss: 0.0083\n",
      "Epoch 240/500\n",
      "12800/12800 [==============================] - 1s 94us/step - loss: 0.0085 - val_loss: 0.0083\n",
      "Epoch 241/500\n",
      "12800/12800 [==============================] - 2s 121us/step - loss: 0.0085 - val_loss: 0.0083\n",
      "Epoch 242/500\n",
      "12800/12800 [==============================] - 2s 181us/step - loss: 0.0085 - val_loss: 0.0083\n",
      "Epoch 243/500\n",
      "12800/12800 [==============================] - 2s 144us/step - loss: 0.0085 - val_loss: 0.0083\n",
      "Epoch 244/500\n",
      "12800/12800 [==============================] - 2s 127us/step - loss: 0.0085 - val_loss: 0.0083\n",
      "Epoch 245/500\n",
      "12800/12800 [==============================] - 2s 135us/step - loss: 0.0084 - val_loss: 0.0083\n",
      "Epoch 246/500\n",
      "12800/12800 [==============================] - 1s 114us/step - loss: 0.0084 - val_loss: 0.0083\n",
      "Epoch 247/500\n",
      "12800/12800 [==============================] - 1s 112us/step - loss: 0.0084 - val_loss: 0.0083\n",
      "Epoch 248/500\n",
      "12800/12800 [==============================] - 1s 105us/step - loss: 0.0084 - val_loss: 0.0083\n",
      "Epoch 249/500\n",
      "12800/12800 [==============================] - 1s 106us/step - loss: 0.0084 - val_loss: 0.0082\n",
      "Epoch 250/500\n",
      "12800/12800 [==============================] - 1s 108us/step - loss: 0.0084 - val_loss: 0.0082\n",
      "Epoch 251/500\n",
      "12800/12800 [==============================] - 2s 125us/step - loss: 0.0084 - val_loss: 0.0082\n",
      "Epoch 252/500\n",
      "12800/12800 [==============================] - 1s 103us/step - loss: 0.0084 - val_loss: 0.0082\n",
      "Epoch 253/500\n",
      "12800/12800 [==============================] - 1s 106us/step - loss: 0.0084 - val_loss: 0.0082\n",
      "Epoch 254/500\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 0.0084 - val_loss: 0.0082\n",
      "Epoch 255/500\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 0.0083 - val_loss: 0.0082\n",
      "Epoch 256/500\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 0.0083 - val_loss: 0.0082\n",
      "Epoch 257/500\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 0.0083 - val_loss: 0.0082\n",
      "Epoch 258/500\n",
      "12800/12800 [==============================] - 1s 116us/step - loss: 0.0083 - val_loss: 0.0082\n",
      "Epoch 259/500\n",
      "12800/12800 [==============================] - 3s 204us/step - loss: 0.0083 - val_loss: 0.0081\n",
      "Epoch 260/500\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 0.0083 - val_loss: 0.0081\n",
      "Epoch 261/500\n",
      "12800/12800 [==============================] - 2s 144us/step - loss: 0.0083 - val_loss: 0.0081\n",
      "Epoch 262/500\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.0083 - val_loss: 0.0081\n",
      "Epoch 263/500\n",
      "12800/12800 [==============================] - 1s 111us/step - loss: 0.0083 - val_loss: 0.0081\n",
      "Epoch 264/500\n",
      "12800/12800 [==============================] - 1s 97us/step - loss: 0.0083 - val_loss: 0.0081\n",
      "Epoch 265/500\n",
      "12800/12800 [==============================] - 1s 95us/step - loss: 0.0083 - val_loss: 0.0081\n",
      "Epoch 266/500\n",
      "12800/12800 [==============================] - 1s 100us/step - loss: 0.0082 - val_loss: 0.0081\n",
      "Epoch 267/500\n",
      "12800/12800 [==============================] - 1s 103us/step - loss: 0.0082 - val_loss: 0.0081\n",
      "Epoch 268/500\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 0.0082 - val_loss: 0.0081\n",
      "Epoch 269/500\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 0.0082 - val_loss: 0.0081\n",
      "Epoch 270/500\n",
      "12800/12800 [==============================] - 1s 97us/step - loss: 0.0082 - val_loss: 0.0080\n",
      "Epoch 271/500\n",
      "12800/12800 [==============================] - 1s 88us/step - loss: 0.0082 - val_loss: 0.0080\n",
      "Epoch 272/500\n",
      "12800/12800 [==============================] - 1s 93us/step - loss: 0.0082 - val_loss: 0.0080\n",
      "Epoch 273/500\n",
      "12800/12800 [==============================] - 1s 112us/step - loss: 0.0082 - val_loss: 0.0080\n",
      "Epoch 274/500\n",
      "12800/12800 [==============================] - 1s 100us/step - loss: 0.0082 - val_loss: 0.0080\n",
      "Epoch 275/500\n",
      "12800/12800 [==============================] - 2s 122us/step - loss: 0.0082 - val_loss: 0.0080\n",
      "Epoch 276/500\n",
      "12800/12800 [==============================] - 1s 100us/step - loss: 0.0082 - val_loss: 0.0080\n",
      "Epoch 277/500\n",
      "12800/12800 [==============================] - 1s 99us/step - loss: 0.0081 - val_loss: 0.0080\n",
      "Epoch 278/500\n",
      "12800/12800 [==============================] - 1s 104us/step - loss: 0.0081 - val_loss: 0.0080\n",
      "Epoch 279/500\n",
      "12800/12800 [==============================] - 1s 111us/step - loss: 0.0081 - val_loss: 0.0080\n",
      "Epoch 280/500\n",
      "12800/12800 [==============================] - 2s 156us/step - loss: 0.0081 - val_loss: 0.0080\n",
      "Epoch 281/500\n",
      "12800/12800 [==============================] - 1s 101us/step - loss: 0.0081 - val_loss: 0.0080\n",
      "Epoch 282/500\n",
      "12800/12800 [==============================] - 2s 139us/step - loss: 0.0081 - val_loss: 0.0079\n",
      "Epoch 283/500\n",
      "12800/12800 [==============================] - 3s 203us/step - loss: 0.0081 - val_loss: 0.0079\n",
      "Epoch 284/500\n",
      "12800/12800 [==============================] - 2s 178us/step - loss: 0.0081 - val_loss: 0.0079\n",
      "Epoch 285/500\n",
      "12800/12800 [==============================] - 2s 158us/step - loss: 0.0081 - val_loss: 0.0079\n",
      "Epoch 286/500\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 0.0081 - val_loss: 0.0079\n",
      "Epoch 287/500\n",
      "12800/12800 [==============================] - 1s 102us/step - loss: 0.0081 - val_loss: 0.0079\n",
      "Epoch 288/500\n",
      "12800/12800 [==============================] - 2s 141us/step - loss: 0.0080 - val_loss: 0.0079\n",
      "Epoch 289/500\n",
      "12800/12800 [==============================] - 1s 103us/step - loss: 0.0080 - val_loss: 0.0079\n",
      "Epoch 290/500\n",
      "12800/12800 [==============================] - 1s 96us/step - loss: 0.0080 - val_loss: 0.0079\n",
      "Epoch 291/500\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 0.0080 - val_loss: 0.0079\n",
      "Epoch 292/500\n",
      "12800/12800 [==============================] - 1s 108us/step - loss: 0.0080 - val_loss: 0.0079\n",
      "Epoch 293/500\n",
      "12800/12800 [==============================] - 1s 94us/step - loss: 0.0080 - val_loss: 0.0079\n",
      "Epoch 294/500\n",
      "12800/12800 [==============================] - 1s 106us/step - loss: 0.0080 - val_loss: 0.0078\n",
      "Epoch 295/500\n",
      "12800/12800 [==============================] - 1s 112us/step - loss: 0.0080 - val_loss: 0.0078\n",
      "Epoch 296/500\n",
      "12800/12800 [==============================] - 1s 106us/step - loss: 0.0080 - val_loss: 0.0078\n",
      "Epoch 297/500\n",
      "12800/12800 [==============================] - 1s 114us/step - loss: 0.0080 - val_loss: 0.0078\n",
      "Epoch 298/500\n",
      "12800/12800 [==============================] - 2s 192us/step - loss: 0.0080 - val_loss: 0.0078\n",
      "Epoch 299/500\n",
      "12800/12800 [==============================] - 1s 96us/step - loss: 0.0080 - val_loss: 0.0078\n",
      "Epoch 300/500\n",
      "12800/12800 [==============================] - 1s 93us/step - loss: 0.0079 - val_loss: 0.0078\n",
      "Epoch 301/500\n",
      "12800/12800 [==============================] - 1s 109us/step - loss: 0.0079 - val_loss: 0.0078\n",
      "Epoch 302/500\n",
      "12800/12800 [==============================] - 2s 165us/step - loss: 0.0079 - val_loss: 0.0078\n",
      "Epoch 303/500\n",
      "12800/12800 [==============================] - 1s 104us/step - loss: 0.0079 - val_loss: 0.0078\n",
      "Epoch 304/500\n",
      "12800/12800 [==============================] - 2s 122us/step - loss: 0.0079 - val_loss: 0.0078\n",
      "Epoch 305/500\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 0.0079 - val_loss: 0.0078\n",
      "Epoch 306/500\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 0.0079 - val_loss: 0.0077\n",
      "Epoch 307/500\n",
      "12800/12800 [==============================] - 2s 130us/step - loss: 0.0079 - val_loss: 0.0077\n",
      "Epoch 308/500\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 0.0079 - val_loss: 0.0077\n",
      "Epoch 309/500\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 0.0079 - val_loss: 0.0077\n",
      "Epoch 310/500\n",
      "12800/12800 [==============================] - 1s 112us/step - loss: 0.0079 - val_loss: 0.0077\n",
      "Epoch 311/500\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 0.0079 - val_loss: 0.0077\n",
      "Epoch 312/500\n",
      "12800/12800 [==============================] - 2s 125us/step - loss: 0.0078 - val_loss: 0.0077\n",
      "Epoch 313/500\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 0.0078 - val_loss: 0.0077\n",
      "Epoch 314/500\n",
      "12800/12800 [==============================] - 1s 72us/step - loss: 0.0078 - val_loss: 0.0077\n",
      "Epoch 315/500\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 0.0078 - val_loss: 0.0077\n",
      "Epoch 316/500\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 0.0078 - val_loss: 0.0077\n",
      "Epoch 317/500\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 0.0078 - val_loss: 0.0077\n",
      "Epoch 318/500\n",
      "12800/12800 [==============================] - 1s 99us/step - loss: 0.0078 - val_loss: 0.0077\n",
      "Epoch 319/500\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 0.0078 - val_loss: 0.0076\n",
      "Epoch 320/500\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 0.0078 - val_loss: 0.0076\n",
      "Epoch 321/500\n",
      "12800/12800 [==============================] - 1s 92us/step - loss: 0.0078 - val_loss: 0.0076\n",
      "Epoch 322/500\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 0.0078 - val_loss: 0.0076\n",
      "Epoch 323/500\n",
      "12800/12800 [==============================] - 2s 140us/step - loss: 0.0078 - val_loss: 0.0076\n",
      "Epoch 324/500\n",
      "12800/12800 [==============================] - 1s 102us/step - loss: 0.0077 - val_loss: 0.0076\n",
      "Epoch 325/500\n",
      "12800/12800 [==============================] - 2s 130us/step - loss: 0.0077 - val_loss: 0.0076\n",
      "Epoch 326/500\n",
      "12800/12800 [==============================] - 2s 131us/step - loss: 0.0077 - val_loss: 0.0076\n",
      "Epoch 327/500\n",
      "12800/12800 [==============================] - 1s 99us/step - loss: 0.0077 - val_loss: 0.0076\n",
      "Epoch 328/500\n",
      "12800/12800 [==============================] - 2s 118us/step - loss: 0.0077 - val_loss: 0.0076\n",
      "Epoch 329/500\n",
      "12800/12800 [==============================] - 1s 99us/step - loss: 0.0077 - val_loss: 0.0076\n",
      "Epoch 330/500\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 0.0077 - val_loss: 0.0076\n",
      "Epoch 331/500\n",
      "12800/12800 [==============================] - 2s 122us/step - loss: 0.0077 - val_loss: 0.0076\n",
      "Epoch 332/500\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 0.0077 - val_loss: 0.0075\n",
      "Epoch 333/500\n",
      "12800/12800 [==============================] - 1s 102us/step - loss: 0.0077 - val_loss: 0.0075\n",
      "Epoch 334/500\n",
      "12800/12800 [==============================] - 1s 93us/step - loss: 0.0077 - val_loss: 0.0075\n",
      "Epoch 335/500\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 0.0077 - val_loss: 0.0075\n",
      "Epoch 336/500\n",
      "12800/12800 [==============================] - 1s 93us/step - loss: 0.0077 - val_loss: 0.0075\n",
      "Epoch 337/500\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 0.0076 - val_loss: 0.0075\n",
      "Epoch 338/500\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 0.0076 - val_loss: 0.0075\n",
      "Epoch 339/500\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 0.0076 - val_loss: 0.0075\n",
      "Epoch 340/500\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 0.0076 - val_loss: 0.0075\n",
      "Epoch 341/500\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 0.0076 - val_loss: 0.0075\n",
      "Epoch 342/500\n",
      "12800/12800 [==============================] - 1s 92us/step - loss: 0.0076 - val_loss: 0.0075\n",
      "Epoch 343/500\n",
      "12800/12800 [==============================] - 1s 100us/step - loss: 0.0076 - val_loss: 0.0075\n",
      "Epoch 344/500\n",
      "12800/12800 [==============================] - 1s 105us/step - loss: 0.0076 - val_loss: 0.0075\n",
      "Epoch 345/500\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 0.0076 - val_loss: 0.0074\n",
      "Epoch 346/500\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 0.0076 - val_loss: 0.0074\n",
      "Epoch 347/500\n",
      "12800/12800 [==============================] - 1s 64us/step - loss: 0.0076 - val_loss: 0.0074\n",
      "Epoch 348/500\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 0.0076 - val_loss: 0.0074\n",
      "Epoch 349/500\n",
      "12800/12800 [==============================] - 1s 68us/step - loss: 0.0076 - val_loss: 0.0074\n",
      "Epoch 350/500\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 0.0076 - val_loss: 0.0074\n",
      "Epoch 351/500\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 0.0075 - val_loss: 0.0074\n",
      "Epoch 352/500\n",
      "12800/12800 [==============================] - 1s 68us/step - loss: 0.0075 - val_loss: 0.0074\n",
      "Epoch 353/500\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 0.0075 - val_loss: 0.0074\n",
      "Epoch 354/500\n",
      "12800/12800 [==============================] - 2s 164us/step - loss: 0.0075 - val_loss: 0.0074\n",
      "Epoch 355/500\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 0.0075 - val_loss: 0.0074\n",
      "Epoch 356/500\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 0.0075 - val_loss: 0.0074\n",
      "Epoch 357/500\n",
      "12800/12800 [==============================] - 1s 72us/step - loss: 0.0075 - val_loss: 0.0074\n",
      "Epoch 358/500\n",
      "12800/12800 [==============================] - 1s 116us/step - loss: 0.0075 - val_loss: 0.0074\n",
      "Epoch 359/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 1s 82us/step - loss: 0.0075 - val_loss: 0.0074\n",
      "Epoch 360/500\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 0.0075 - val_loss: 0.0073\n",
      "Epoch 361/500\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 0.0075 - val_loss: 0.0073\n",
      "Epoch 362/500\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 0.0075 - val_loss: 0.0073\n",
      "Epoch 363/500\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 0.0075 - val_loss: 0.0073\n",
      "Epoch 364/500\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 0.0075 - val_loss: 0.0073\n",
      "Epoch 365/500\n",
      "12800/12800 [==============================] - 1s 69us/step - loss: 0.0074 - val_loss: 0.0073\n",
      "Epoch 366/500\n",
      "12800/12800 [==============================] - 1s 68us/step - loss: 0.0074 - val_loss: 0.0073\n",
      "Epoch 367/500\n",
      "12800/12800 [==============================] - 1s 69us/step - loss: 0.0074 - val_loss: 0.0073\n",
      "Epoch 368/500\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 0.0074 - val_loss: 0.0073\n",
      "Epoch 369/500\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 0.0074 - val_loss: 0.0073\n",
      "Epoch 370/500\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 0.0074 - val_loss: 0.0073\n",
      "Epoch 371/500\n",
      "12800/12800 [==============================] - 1s 96us/step - loss: 0.0074 - val_loss: 0.0073\n",
      "Epoch 372/500\n",
      "12800/12800 [==============================] - 1s 95us/step - loss: 0.0074 - val_loss: 0.0073\n",
      "Epoch 373/500\n",
      "12800/12800 [==============================] - 1s 92us/step - loss: 0.0074 - val_loss: 0.0073\n",
      "Epoch 374/500\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 0.0074 - val_loss: 0.0073\n",
      "Epoch 375/500\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 0.0074 - val_loss: 0.0072\n",
      "Epoch 376/500\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 0.0074 - val_loss: 0.0072\n",
      "Epoch 377/500\n",
      "12800/12800 [==============================] - 1s 112us/step - loss: 0.0074 - val_loss: 0.0072\n",
      "Epoch 378/500\n",
      "12800/12800 [==============================] - 2s 143us/step - loss: 0.0074 - val_loss: 0.0072\n",
      "Epoch 379/500\n",
      "12800/12800 [==============================] - 2s 146us/step - loss: 0.0074 - val_loss: 0.0072\n",
      "Epoch 380/500\n",
      "12800/12800 [==============================] - 1s 108us/step - loss: 0.0073 - val_loss: 0.0072\n",
      "Epoch 381/500\n",
      "12800/12800 [==============================] - 1s 105us/step - loss: 0.0073 - val_loss: 0.0072\n",
      "Epoch 382/500\n",
      "12800/12800 [==============================] - 1s 109us/step - loss: 0.0073 - val_loss: 0.0072\n",
      "Epoch 383/500\n",
      "12800/12800 [==============================] - 1s 97us/step - loss: 0.0073 - val_loss: 0.0072\n",
      "Epoch 384/500\n",
      "12800/12800 [==============================] - 1s 100us/step - loss: 0.0073 - val_loss: 0.0072\n",
      "Epoch 385/500\n",
      "12800/12800 [==============================] - 1s 109us/step - loss: 0.0073 - val_loss: 0.0072\n",
      "Epoch 386/500\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 0.0073 - val_loss: 0.0072\n",
      "Epoch 387/500\n",
      "12800/12800 [==============================] - 2s 127us/step - loss: 0.0073 - val_loss: 0.0072\n",
      "Epoch 388/500\n",
      "12800/12800 [==============================] - 1s 102us/step - loss: 0.0073 - val_loss: 0.0072\n",
      "Epoch 389/500\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 0.0073 - val_loss: 0.0072\n",
      "Epoch 390/500\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 0.0073 - val_loss: 0.0071\n",
      "Epoch 391/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 0.0073 - val_loss: 0.0071\n",
      "Epoch 392/500\n",
      "12800/12800 [==============================] - 1s 110us/step - loss: 0.0073 - val_loss: 0.0071\n",
      "Epoch 393/500\n",
      "12800/12800 [==============================] - 2s 143us/step - loss: 0.0073 - val_loss: 0.0071\n",
      "Epoch 394/500\n",
      "12800/12800 [==============================] - 1s 92us/step - loss: 0.0073 - val_loss: 0.0071\n",
      "Epoch 395/500\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 0.0072 - val_loss: 0.0071\n",
      "Epoch 396/500\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 0.0072 - val_loss: 0.0071\n",
      "Epoch 397/500\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 0.0072 - val_loss: 0.0071\n",
      "Epoch 398/500\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 0.0072 - val_loss: 0.0071\n",
      "Epoch 399/500\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 0.0072 - val_loss: 0.0071\n",
      "Epoch 400/500\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 0.0072 - val_loss: 0.0071\n",
      "Epoch 401/500\n",
      "12800/12800 [==============================] - 2s 118us/step - loss: 0.0072 - val_loss: 0.0071\n",
      "Epoch 402/500\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 0.0072 - val_loss: 0.0071\n",
      "Epoch 403/500\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 0.0072 - val_loss: 0.0071\n",
      "Epoch 404/500\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 0.0072 - val_loss: 0.0071\n",
      "Epoch 405/500\n",
      "12800/12800 [==============================] - 1s 103us/step - loss: 0.0072 - val_loss: 0.0071\n",
      "Epoch 406/500\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 0.0072 - val_loss: 0.0070\n",
      "Epoch 407/500\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 0.0072 - val_loss: 0.0070\n",
      "Epoch 408/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 0.0072 - val_loss: 0.0070\n",
      "Epoch 409/500\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 0.0072 - val_loss: 0.0070\n",
      "Epoch 410/500\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 0.0072 - val_loss: 0.0070\n",
      "Epoch 411/500\n",
      "12800/12800 [==============================] - 1s 69us/step - loss: 0.0071 - val_loss: 0.0070\n",
      "Epoch 412/500\n",
      "12800/12800 [==============================] - 1s 68us/step - loss: 0.0071 - val_loss: 0.0070\n",
      "Epoch 413/500\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 0.0071 - val_loss: 0.0070\n",
      "Epoch 414/500\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 0.0071 - val_loss: 0.0070\n",
      "Epoch 415/500\n",
      "12800/12800 [==============================] - 1s 72us/step - loss: 0.0071 - val_loss: 0.0070\n",
      "Epoch 416/500\n",
      "12800/12800 [==============================] - 1s 69us/step - loss: 0.0071 - val_loss: 0.0070\n",
      "Epoch 417/500\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 0.0071 - val_loss: 0.0070\n",
      "Epoch 418/500\n",
      "12800/12800 [==============================] - 1s 98us/step - loss: 0.0071 - val_loss: 0.0070\n",
      "Epoch 419/500\n",
      "12800/12800 [==============================] - 1s 72us/step - loss: 0.0071 - val_loss: 0.0070\n",
      "Epoch 420/500\n",
      "12800/12800 [==============================] - 1s 69us/step - loss: 0.0071 - val_loss: 0.0070\n",
      "Epoch 421/500\n",
      "12800/12800 [==============================] - 1s 100us/step - loss: 0.0071 - val_loss: 0.0070\n",
      "Epoch 422/500\n",
      "12800/12800 [==============================] - 1s 111us/step - loss: 0.0071 - val_loss: 0.0070\n",
      "Epoch 423/500\n",
      "12800/12800 [==============================] - 1s 72us/step - loss: 0.0071 - val_loss: 0.0069\n",
      "Epoch 424/500\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 0.0071 - val_loss: 0.0069\n",
      "Epoch 425/500\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 0.0071 - val_loss: 0.0069\n",
      "Epoch 426/500\n",
      "12800/12800 [==============================] - 1s 69us/step - loss: 0.0071 - val_loss: 0.0069\n",
      "Epoch 427/500\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 428/500\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 429/500\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 430/500\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 431/500\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 432/500\n",
      "12800/12800 [==============================] - 1s 92us/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 433/500\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 434/500\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 0.0070 - val_loss: 0.0069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 435/500\n",
      "12800/12800 [==============================] - ETA: 0s - loss: 0.007 - 1s 92us/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 436/500\n",
      "12800/12800 [==============================] - 1s 99us/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 437/500\n",
      "12800/12800 [==============================] - 1s 96us/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 438/500\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 439/500\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 440/500\n",
      "12800/12800 [==============================] - 1s 99us/step - loss: 0.0070 - val_loss: 0.0068\n",
      "Epoch 441/500\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 0.0070 - val_loss: 0.0068\n",
      "Epoch 442/500\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 0.0070 - val_loss: 0.0068\n",
      "Epoch 443/500\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 0.0070 - val_loss: 0.0068\n",
      "Epoch 444/500\n",
      "12800/12800 [==============================] - 1s 116us/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 445/500\n",
      "12800/12800 [==============================] - 1s 102us/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 446/500\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 447/500\n",
      "12800/12800 [==============================] - 2s 122us/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 448/500\n",
      "12800/12800 [==============================] - 1s 109us/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 449/500\n",
      "12800/12800 [==============================] - 2s 126us/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 450/500\n",
      "12800/12800 [==============================] - 1s 103us/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 451/500\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 452/500\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 453/500\n",
      "12800/12800 [==============================] - 1s 95us/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 454/500\n",
      "12800/12800 [==============================] - 1s 105us/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 455/500\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 456/500\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 457/500\n",
      "12800/12800 [==============================] - 1s 96us/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 458/500\n",
      "12800/12800 [==============================] - 1s 95us/step - loss: 0.0069 - val_loss: 0.0067\n",
      "Epoch 459/500\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 0.0069 - val_loss: 0.0067\n",
      "Epoch 460/500\n",
      "12800/12800 [==============================] - 1s 112us/step - loss: 0.0069 - val_loss: 0.0067\n",
      "Epoch 461/500\n",
      "12800/12800 [==============================] - 2s 120us/step - loss: 0.0069 - val_loss: 0.0067\n",
      "Epoch 462/500\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 0.0068 - val_loss: 0.0067\n",
      "Epoch 463/500\n",
      "12800/12800 [==============================] - 1s 95us/step - loss: 0.0068 - val_loss: 0.0067\n",
      "Epoch 464/500\n",
      "12800/12800 [==============================] - 2s 118us/step - loss: 0.0068 - val_loss: 0.0067\n",
      "Epoch 465/500\n",
      "12800/12800 [==============================] - 1s 93us/step - loss: 0.0068 - val_loss: 0.0067\n",
      "Epoch 466/500\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 0.0068 - val_loss: 0.0067\n",
      "Epoch 467/500\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 0.0068 - val_loss: 0.0067\n",
      "Epoch 468/500\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 0.0068 - val_loss: 0.0067\n",
      "Epoch 469/500\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 0.0068 - val_loss: 0.0067\n",
      "Epoch 470/500\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 0.0068 - val_loss: 0.0067\n",
      "Epoch 471/500\n",
      "12800/12800 [==============================] - 1s 69us/step - loss: 0.0068 - val_loss: 0.0067\n",
      "Epoch 472/500\n",
      "12800/12800 [==============================] - 1s 68us/step - loss: 0.0068 - val_loss: 0.0067\n",
      "Epoch 473/500\n",
      "12800/12800 [==============================] - 1s 107us/step - loss: 0.0068 - val_loss: 0.0067\n",
      "Epoch 474/500\n",
      "12800/12800 [==============================] - 2s 150us/step - loss: 0.0068 - val_loss: 0.0067\n",
      "Epoch 475/500\n",
      "12800/12800 [==============================] - 2s 154us/step - loss: 0.0068 - val_loss: 0.0067\n",
      "Epoch 476/500\n",
      "12800/12800 [==============================] - 2s 192us/step - loss: 0.0068 - val_loss: 0.0067\n",
      "Epoch 477/500\n",
      "12800/12800 [==============================] - 2s 192us/step - loss: 0.0068 - val_loss: 0.0066\n",
      "Epoch 478/500\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.0068 - val_loss: 0.0066\n",
      "Epoch 479/500\n",
      "12800/12800 [==============================] - 2s 134us/step - loss: 0.0068 - val_loss: 0.0066\n",
      "Epoch 480/500\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 0.0068 - val_loss: 0.0066\n",
      "Epoch 481/500\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 0.0067 - val_loss: 0.0066\n",
      "Epoch 482/500\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 0.0067 - val_loss: 0.0066\n",
      "Epoch 483/500\n",
      "12800/12800 [==============================] - 1s 88us/step - loss: 0.0067 - val_loss: 0.0066\n",
      "Epoch 484/500\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 0.0067 - val_loss: 0.0066\n",
      "Epoch 485/500\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 0.0067 - val_loss: 0.0066\n",
      "Epoch 486/500\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 0.0067 - val_loss: 0.0066\n",
      "Epoch 487/500\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 0.0067 - val_loss: 0.0066\n",
      "Epoch 488/500\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 0.0067 - val_loss: 0.0066\n",
      "Epoch 489/500\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 0.0067 - val_loss: 0.0066\n",
      "Epoch 490/500\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 0.0067 - val_loss: 0.0066\n",
      "Epoch 491/500\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 0.0067 - val_loss: 0.0066\n",
      "Epoch 492/500\n",
      "12800/12800 [==============================] - 1s 94us/step - loss: 0.0067 - val_loss: 0.0066\n",
      "Epoch 493/500\n",
      "12800/12800 [==============================] - 1s 106us/step - loss: 0.0067 - val_loss: 0.0066\n",
      "Epoch 494/500\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 0.0067 - val_loss: 0.0066\n",
      "Epoch 495/500\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 0.0067 - val_loss: 0.0066\n",
      "Epoch 496/500\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 0.0067 - val_loss: 0.0066\n",
      "Epoch 497/500\n",
      "12800/12800 [==============================] - 1s 96us/step - loss: 0.0067 - val_loss: 0.0065\n",
      "Epoch 498/500\n",
      "12800/12800 [==============================] - 1s 97us/step - loss: 0.0067 - val_loss: 0.0065\n",
      "Epoch 499/500\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 0.0067 - val_loss: 0.0065\n",
      "Epoch 500/500\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 0.0066 - val_loss: 0.0065\n",
      "Test MSE: 0.0066175764\n",
      "Total MSE: 0.0066215868\n"
     ]
    }
   ],
   "source": [
    "lr_epochs = {}\n",
    "for lr in [0.1, 0.01, 0.001, 0.0001, 0.00001]:\n",
    "    print(lr)\n",
    "    lr_epochs[lr] = process_image('a.jpg', 4, 5e-4, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x468 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['font.size'] = 15\n",
    "plt.rcParams['font.weight'] = 100\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(15, 6.5)\n",
    "\n",
    "ax.set(xlabel='Learning rate lr', ylabel='Epochs count')\n",
    "ax.plot(lr_epochs.keys(), [len(e['loss']) for e in lr_epochs.values()], '-o', label='Epochs count', linewidth=2)\n",
    "ax.set_xticks(list(lr_epochs.keys()))\n",
    "ax.set_xscale('log')\n",
    "\n",
    "ax.grid()\n",
    "ax.legend(loc='upper center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[192, 3, 102, 500, 500]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(e['loss']) for e in lr_epochs.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.jpg\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 75)                22575     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 300)               22800     \n",
      "=================================================================\n",
      "Total params: 45,375\n",
      "Trainable params: 45,375\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/500\n",
      "12800/12800 [==============================] - 1s 92us/step - loss: 0.0015 - val_loss: 8.3754e-04\n",
      "Epoch 2/500\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 6.6282e-04 - val_loss: 5.5253e-04\n",
      "Epoch 3/500\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 4.7003e-04 - val_loss: 4.2036e-04\n",
      "Test MSE: 0.00041522783\n",
      "Total MSE: 0.00041219425\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input-layer (InputLayer)     (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "hidden-layer-1 (Dense)       (None, 150)               45150     \n",
      "_________________________________________________________________\n",
      "hidden-layer-2 (Dense)       (None, 75)                11325     \n",
      "_________________________________________________________________\n",
      "hidden-layer-3 (Dense)       (None, 150)               11400     \n",
      "_________________________________________________________________\n",
      "output-layer (Dense)         (None, 300)               45300     \n",
      "=================================================================\n",
      "Total params: 113,175\n",
      "Trainable params: 113,175\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/500\n",
      "12800/12800 [==============================] - 1s 105us/step - loss: 0.0038 - val_loss: 0.0017\n",
      "Epoch 2/500\n",
      "12800/12800 [==============================] - 1s 113us/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 3/500\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 4/500\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 5/500\n",
      "12800/12800 [==============================] - 1s 114us/step - loss: 9.2733e-04 - val_loss: 9.1919e-04\n",
      "Epoch 6/500\n",
      "12800/12800 [==============================] - 2s 120us/step - loss: 8.3975e-04 - val_loss: 8.3733e-04\n",
      "Epoch 7/500\n",
      "12800/12800 [==============================] - 2s 123us/step - loss: 7.6874e-04 - val_loss: 7.6949e-04\n",
      "Epoch 8/500\n",
      "12800/12800 [==============================] - 2s 152us/step - loss: 7.0943e-04 - val_loss: 7.1282e-04\n",
      "Epoch 9/500\n",
      "12800/12800 [==============================] - 2s 148us/step - loss: 6.5883e-04 - val_loss: 6.6481e-04\n",
      "Epoch 10/500\n",
      "12800/12800 [==============================] - 2s 183us/step - loss: 6.1528e-04 - val_loss: 6.2227e-04\n",
      "Epoch 11/500\n",
      "12800/12800 [==============================] - 1s 106us/step - loss: 5.7745e-04 - val_loss: 5.8593e-04\n",
      "Epoch 12/500\n",
      "12800/12800 [==============================] - 1s 105us/step - loss: 5.4421e-04 - val_loss: 5.5327e-04\n",
      "Epoch 13/500\n",
      "12800/12800 [==============================] - 1s 95us/step - loss: 5.1472e-04 - val_loss: 5.2475e-04\n",
      "Epoch 14/500\n",
      "12800/12800 [==============================] - 1s 108us/step - loss: 4.8859e-04 - val_loss: 4.9917e-04\n",
      "Test MSE: 0.00048317841\n",
      "Total MSE: 0.00048054007\n",
      "b.jpg\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 75)                22575     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 300)               22800     \n",
      "=================================================================\n",
      "Total params: 45,375\n",
      "Trainable params: 45,375\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 5056 samples, validate on 1264 samples\n",
      "Epoch 1/500\n",
      "5056/5056 [==============================] - 1s 106us/step - loss: 0.0114 - val_loss: 0.0068\n",
      "Epoch 2/500\n",
      "5056/5056 [==============================] - 0s 75us/step - loss: 0.0058 - val_loss: 0.0050\n",
      "Epoch 3/500\n",
      "5056/5056 [==============================] - 0s 54us/step - loss: 0.0045 - val_loss: 0.0041\n",
      "Epoch 4/500\n",
      "5056/5056 [==============================] - 0s 63us/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 5/500\n",
      "5056/5056 [==============================] - 0s 61us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 6/500\n",
      "5056/5056 [==============================] - 0s 59us/step - loss: 0.0029 - val_loss: 0.0028\n",
      "Epoch 7/500\n",
      "5056/5056 [==============================] - 0s 61us/step - loss: 0.0027 - val_loss: 0.0026\n",
      "Epoch 8/500\n",
      "5056/5056 [==============================] - 0s 68us/step - loss: 0.0025 - val_loss: 0.0024\n",
      "Epoch 9/500\n",
      "5056/5056 [==============================] - 0s 62us/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 10/500\n",
      "5056/5056 [==============================] - 0s 54us/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 11/500\n",
      "5056/5056 [==============================] - 0s 57us/step - loss: 0.0021 - val_loss: 0.0020\n",
      "Epoch 12/500\n",
      "5056/5056 [==============================] - 0s 53us/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 13/500\n",
      "5056/5056 [==============================] - 0s 50us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 14/500\n",
      "5056/5056 [==============================] - 0s 52us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 15/500\n",
      "5056/5056 [==============================] - 0s 59us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 16/500\n",
      "5056/5056 [==============================] - 0s 67us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 17/500\n",
      "5056/5056 [==============================] - 0s 75us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 18/500\n",
      "5056/5056 [==============================] - 0s 82us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 19/500\n",
      "5056/5056 [==============================] - 0s 60us/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 20/500\n",
      "5056/5056 [==============================] - 0s 60us/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 21/500\n",
      "5056/5056 [==============================] - 0s 59us/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 22/500\n",
      "5056/5056 [==============================] - 0s 54us/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 23/500\n",
      "5056/5056 [==============================] - 0s 58us/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 24/500\n",
      "5056/5056 [==============================] - 0s 67us/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 25/500\n",
      "5056/5056 [==============================] - 1s 114us/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 26/500\n",
      "5056/5056 [==============================] - 1s 101us/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 27/500\n",
      "5056/5056 [==============================] - 1s 109us/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 28/500\n",
      "5056/5056 [==============================] - 0s 73us/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 29/500\n",
      "5056/5056 [==============================] - 0s 76us/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 30/500\n",
      "5056/5056 [==============================] - 0s 63us/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 31/500\n",
      "5056/5056 [==============================] - 0s 68us/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 32/500\n",
      "5056/5056 [==============================] - 0s 82us/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 33/500\n",
      "5056/5056 [==============================] - 0s 62us/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 34/500\n",
      "5056/5056 [==============================] - 0s 77us/step - loss: 0.0011 - val_loss: 0.0011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/500\n",
      "5056/5056 [==============================] - 0s 75us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 36/500\n",
      "5056/5056 [==============================] - 0s 62us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 37/500\n",
      "5056/5056 [==============================] - 0s 52us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 38/500\n",
      "5056/5056 [==============================] - 0s 77us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 39/500\n",
      "5056/5056 [==============================] - 0s 67us/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 40/500\n",
      "5056/5056 [==============================] - 0s 59us/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 41/500\n",
      "5056/5056 [==============================] - 0s 64us/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 42/500\n",
      "5056/5056 [==============================] - 0s 64us/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 43/500\n",
      "5056/5056 [==============================] - 0s 60us/step - loss: 9.9074e-04 - val_loss: 0.0010\n",
      "Epoch 44/500\n",
      "5056/5056 [==============================] - 0s 53us/step - loss: 9.8002e-04 - val_loss: 0.0010\n",
      "Epoch 45/500\n",
      "5056/5056 [==============================] - 0s 57us/step - loss: 9.6990e-04 - val_loss: 0.0010\n",
      "Epoch 46/500\n",
      "5056/5056 [==============================] - 0s 57us/step - loss: 9.6008e-04 - val_loss: 9.9103e-04\n",
      "Epoch 47/500\n",
      "5056/5056 [==============================] - 0s 60us/step - loss: 9.5073e-04 - val_loss: 9.8165e-04\n",
      "Epoch 48/500\n",
      "5056/5056 [==============================] - 0s 54us/step - loss: 9.4166e-04 - val_loss: 9.7326e-04\n",
      "Epoch 49/500\n",
      "5056/5056 [==============================] - 0s 59us/step - loss: 9.3328e-04 - val_loss: 9.6396e-04\n",
      "Epoch 50/500\n",
      "5056/5056 [==============================] - 0s 62us/step - loss: 9.2481e-04 - val_loss: 9.5578e-04\n",
      "Epoch 51/500\n",
      "5056/5056 [==============================] - 0s 58us/step - loss: 9.1691e-04 - val_loss: 9.4746e-04\n",
      "Epoch 52/500\n",
      "5056/5056 [==============================] - 0s 61us/step - loss: 9.0931e-04 - val_loss: 9.3996e-04\n",
      "Epoch 53/500\n",
      "5056/5056 [==============================] - 0s 53us/step - loss: 9.0189e-04 - val_loss: 9.3266e-04\n",
      "Epoch 54/500\n",
      "5056/5056 [==============================] - 0s 57us/step - loss: 8.9481e-04 - val_loss: 9.2592e-04\n",
      "Epoch 55/500\n",
      "5056/5056 [==============================] - 0s 52us/step - loss: 8.8805e-04 - val_loss: 9.1914e-04\n",
      "Epoch 56/500\n",
      "5056/5056 [==============================] - 0s 61us/step - loss: 8.8145e-04 - val_loss: 9.1311e-04\n",
      "Epoch 57/500\n",
      "5056/5056 [==============================] - ETA: 0s - loss: 8.7766e-0 - 0s 60us/step - loss: 8.7526e-04 - val_loss: 9.0630e-04\n",
      "Epoch 58/500\n",
      "5056/5056 [==============================] - 0s 65us/step - loss: 8.6921e-04 - val_loss: 9.0022e-04\n",
      "Epoch 59/500\n",
      "5056/5056 [==============================] - 0s 63us/step - loss: 8.6324e-04 - val_loss: 8.9409e-04\n",
      "Epoch 60/500\n",
      "5056/5056 [==============================] - 0s 56us/step - loss: 8.5748e-04 - val_loss: 8.8963e-04\n",
      "Epoch 61/500\n",
      "5056/5056 [==============================] - 0s 66us/step - loss: 8.5211e-04 - val_loss: 8.8365e-04\n",
      "Epoch 62/500\n",
      "5056/5056 [==============================] - 0s 55us/step - loss: 8.4690e-04 - val_loss: 8.7774e-04\n",
      "Epoch 63/500\n",
      "5056/5056 [==============================] - 0s 53us/step - loss: 8.4168e-04 - val_loss: 8.7243e-04\n",
      "Epoch 64/500\n",
      "5056/5056 [==============================] - 0s 79us/step - loss: 8.3662e-04 - val_loss: 8.6787e-04\n",
      "Epoch 65/500\n",
      "5056/5056 [==============================] - 0s 81us/step - loss: 8.3205e-04 - val_loss: 8.6210e-04\n",
      "Epoch 66/500\n",
      "5056/5056 [==============================] - 0s 80us/step - loss: 8.2731e-04 - val_loss: 8.5866e-04\n",
      "Epoch 67/500\n",
      "5056/5056 [==============================] - 0s 63us/step - loss: 8.2267e-04 - val_loss: 8.5352e-04\n",
      "Epoch 68/500\n",
      "5056/5056 [==============================] - 0s 54us/step - loss: 8.1836e-04 - val_loss: 8.4947e-04\n",
      "Epoch 69/500\n",
      "5056/5056 [==============================] - 0s 59us/step - loss: 8.1424e-04 - val_loss: 8.4569e-04\n",
      "Epoch 70/500\n",
      "5056/5056 [==============================] - 0s 54us/step - loss: 8.1006e-04 - val_loss: 8.4069e-04\n",
      "Epoch 71/500\n",
      "5056/5056 [==============================] - 0s 55us/step - loss: 8.0619e-04 - val_loss: 8.3706e-04\n",
      "Epoch 72/500\n",
      "5056/5056 [==============================] - 0s 62us/step - loss: 8.0231e-04 - val_loss: 8.3255e-04\n",
      "Epoch 73/500\n",
      "5056/5056 [==============================] - 0s 66us/step - loss: 7.9856e-04 - val_loss: 8.2927e-04\n",
      "Epoch 74/500\n",
      "5056/5056 [==============================] - 0s 53us/step - loss: 7.9501e-04 - val_loss: 8.2464e-04\n",
      "Epoch 75/500\n",
      "5056/5056 [==============================] - 0s 52us/step - loss: 7.9133e-04 - val_loss: 8.2157e-04\n",
      "Epoch 76/500\n",
      "5056/5056 [==============================] - 0s 57us/step - loss: 7.8803e-04 - val_loss: 8.1831e-04\n",
      "Epoch 77/500\n",
      "5056/5056 [==============================] - 0s 52us/step - loss: 7.8463e-04 - val_loss: 8.1567e-04\n",
      "Epoch 78/500\n",
      "5056/5056 [==============================] - 0s 61us/step - loss: 7.8151e-04 - val_loss: 8.1144e-04\n",
      "Epoch 79/500\n",
      "5056/5056 [==============================] - 0s 58us/step - loss: 7.7818e-04 - val_loss: 8.0902e-04\n",
      "Epoch 80/500\n",
      "5056/5056 [==============================] - 0s 62us/step - loss: 7.7541e-04 - val_loss: 8.0522e-04\n",
      "Epoch 81/500\n",
      "5056/5056 [==============================] - 0s 63us/step - loss: 7.7231e-04 - val_loss: 8.0183e-04\n",
      "Epoch 82/500\n",
      "5056/5056 [==============================] - 0s 50us/step - loss: 7.6937e-04 - val_loss: 7.9914e-04\n",
      "Epoch 83/500\n",
      "5056/5056 [==============================] - 0s 56us/step - loss: 7.6658e-04 - val_loss: 7.9751e-04\n",
      "Epoch 84/500\n",
      "5056/5056 [==============================] - 0s 59us/step - loss: 7.6399e-04 - val_loss: 7.9353e-04\n",
      "Epoch 85/500\n",
      "5056/5056 [==============================] - 0s 53us/step - loss: 7.6115e-04 - val_loss: 7.9106e-04\n",
      "Epoch 86/500\n",
      "5056/5056 [==============================] - 0s 63us/step - loss: 7.5862e-04 - val_loss: 7.8904e-04\n",
      "Epoch 87/500\n",
      "5056/5056 [==============================] - 0s 59us/step - loss: 7.5608e-04 - val_loss: 7.8624e-04\n",
      "Epoch 88/500\n",
      "5056/5056 [==============================] - 0s 79us/step - loss: 7.5383e-04 - val_loss: 7.8352e-04\n",
      "Epoch 89/500\n",
      "5056/5056 [==============================] - 0s 75us/step - loss: 7.5120e-04 - val_loss: 7.8172e-04\n",
      "Epoch 90/500\n",
      "5056/5056 [==============================] - 0s 67us/step - loss: 7.4880e-04 - val_loss: 7.7886e-04\n",
      "Epoch 91/500\n",
      "5056/5056 [==============================] - 0s 89us/step - loss: 7.4663e-04 - val_loss: 7.7695e-04\n",
      "Epoch 92/500\n",
      "5056/5056 [==============================] - 0s 78us/step - loss: 7.4443e-04 - val_loss: 7.7481e-04\n",
      "Epoch 93/500\n",
      "5056/5056 [==============================] - 1s 106us/step - loss: 7.4234e-04 - val_loss: 7.7301e-04\n",
      "Epoch 94/500\n",
      "5056/5056 [==============================] - 0s 97us/step - loss: 7.4042e-04 - val_loss: 7.6968e-04\n",
      "Epoch 95/500\n",
      "5056/5056 [==============================] - 1s 102us/step - loss: 7.3812e-04 - val_loss: 7.6873e-04\n",
      "Epoch 96/500\n",
      "5056/5056 [==============================] - 0s 75us/step - loss: 7.3625e-04 - val_loss: 7.6598e-04\n",
      "Epoch 97/500\n",
      "5056/5056 [==============================] - 0s 56us/step - loss: 7.3418e-04 - val_loss: 7.6469e-04\n",
      "Epoch 98/500\n",
      "5056/5056 [==============================] - 0s 60us/step - loss: 7.3225e-04 - val_loss: 7.6246e-04\n",
      "Epoch 99/500\n",
      "5056/5056 [==============================] - 0s 74us/step - loss: 7.3037e-04 - val_loss: 7.6043e-04\n",
      "Epoch 100/500\n",
      "5056/5056 [==============================] - 0s 96us/step - loss: 7.2871e-04 - val_loss: 7.5867e-04\n",
      "Epoch 101/500\n",
      "5056/5056 [==============================] - 0s 67us/step - loss: 7.2688e-04 - val_loss: 7.5632e-04\n",
      "Epoch 102/500\n",
      "5056/5056 [==============================] - 0s 78us/step - loss: 7.2519e-04 - val_loss: 7.5533e-04\n",
      "Epoch 103/500\n",
      "5056/5056 [==============================] - 1s 103us/step - loss: 7.2354e-04 - val_loss: 7.5311e-04\n",
      "Epoch 104/500\n",
      "5056/5056 [==============================] - 0s 83us/step - loss: 7.2194e-04 - val_loss: 7.5185e-04\n",
      "Epoch 105/500\n",
      "5056/5056 [==============================] - 0s 74us/step - loss: 7.2021e-04 - val_loss: 7.5028e-04\n",
      "Epoch 106/500\n",
      "5056/5056 [==============================] - 0s 81us/step - loss: 7.1860e-04 - val_loss: 7.4851e-04\n",
      "Epoch 107/500\n",
      "5056/5056 [==============================] - 0s 79us/step - loss: 7.1712e-04 - val_loss: 7.4808e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/500\n",
      "5056/5056 [==============================] - 0s 82us/step - loss: 7.1565e-04 - val_loss: 7.4503e-04\n",
      "Epoch 109/500\n",
      "5056/5056 [==============================] - 0s 64us/step - loss: 7.1419e-04 - val_loss: 7.4400e-04\n",
      "Epoch 110/500\n",
      "5056/5056 [==============================] - 0s 64us/step - loss: 7.1283e-04 - val_loss: 7.4226e-04\n",
      "Epoch 111/500\n",
      "5056/5056 [==============================] - 1s 103us/step - loss: 7.1140e-04 - val_loss: 7.4138e-04\n",
      "Epoch 112/500\n",
      "5056/5056 [==============================] - 0s 59us/step - loss: 7.0997e-04 - val_loss: 7.3980e-04\n",
      "Epoch 113/500\n",
      "5056/5056 [==============================] - 0s 82us/step - loss: 7.0865e-04 - val_loss: 7.3824e-04\n",
      "Epoch 114/500\n",
      "5056/5056 [==============================] - 0s 77us/step - loss: 7.0738e-04 - val_loss: 7.3730e-04\n",
      "Epoch 115/500\n",
      "5056/5056 [==============================] - 0s 73us/step - loss: 7.0619e-04 - val_loss: 7.3554e-04\n",
      "Epoch 116/500\n",
      "5056/5056 [==============================] - 0s 79us/step - loss: 7.0487e-04 - val_loss: 7.3459e-04\n",
      "Epoch 117/500\n",
      "5056/5056 [==============================] - 0s 57us/step - loss: 7.0361e-04 - val_loss: 7.3270e-04\n",
      "Epoch 118/500\n",
      "5056/5056 [==============================] - 1s 124us/step - loss: 7.0251e-04 - val_loss: 7.3356e-04\n",
      "Epoch 119/500\n",
      "5056/5056 [==============================] - 0s 65us/step - loss: 7.0135e-04 - val_loss: 7.3109e-04\n",
      "Epoch 120/500\n",
      "5056/5056 [==============================] - 0s 70us/step - loss: 7.0019e-04 - val_loss: 7.2999e-04\n",
      "Epoch 121/500\n",
      "5056/5056 [==============================] - 0s 61us/step - loss: 6.9906e-04 - val_loss: 7.2905e-04\n",
      "Epoch 122/500\n",
      "5056/5056 [==============================] - 0s 67us/step - loss: 6.9804e-04 - val_loss: 7.2755e-04\n",
      "Epoch 123/500\n",
      "5056/5056 [==============================] - 0s 87us/step - loss: 6.9691e-04 - val_loss: 7.2694e-04\n",
      "Epoch 124/500\n",
      "5056/5056 [==============================] - 0s 79us/step - loss: 6.9582e-04 - val_loss: 7.2588e-04\n",
      "Epoch 125/500\n",
      "5056/5056 [==============================] - 0s 67us/step - loss: 6.9487e-04 - val_loss: 7.2446e-04\n",
      "Epoch 126/500\n",
      "5056/5056 [==============================] - 0s 54us/step - loss: 6.9371e-04 - val_loss: 7.2350e-04\n",
      "Epoch 127/500\n",
      "5056/5056 [==============================] - 0s 56us/step - loss: 6.9285e-04 - val_loss: 7.2256e-04\n",
      "Epoch 128/500\n",
      "5056/5056 [==============================] - 0s 58us/step - loss: 6.9196e-04 - val_loss: 7.2174e-04\n",
      "Epoch 129/500\n",
      "5056/5056 [==============================] - 0s 76us/step - loss: 6.9094e-04 - val_loss: 7.2060e-04\n",
      "Epoch 130/500\n",
      "5056/5056 [==============================] - 0s 78us/step - loss: 6.9000e-04 - val_loss: 7.1983e-04\n",
      "Epoch 131/500\n",
      "5056/5056 [==============================] - 0s 85us/step - loss: 6.8913e-04 - val_loss: 7.1861e-04\n",
      "Epoch 132/500\n",
      "5056/5056 [==============================] - 0s 67us/step - loss: 6.8821e-04 - val_loss: 7.1821e-04\n",
      "Epoch 133/500\n",
      "5056/5056 [==============================] - 0s 97us/step - loss: 6.8752e-04 - val_loss: 7.1693e-04\n",
      "Epoch 134/500\n",
      "5056/5056 [==============================] - 0s 76us/step - loss: 6.8648e-04 - val_loss: 7.1669e-04\n",
      "Epoch 135/500\n",
      "5056/5056 [==============================] - 0s 85us/step - loss: 6.8567e-04 - val_loss: 7.1593e-04\n",
      "Epoch 136/500\n",
      "5056/5056 [==============================] - 0s 82us/step - loss: 6.8496e-04 - val_loss: 7.1472e-04\n",
      "Epoch 137/500\n",
      "5056/5056 [==============================] - 0s 94us/step - loss: 6.8417e-04 - val_loss: 7.1427e-04\n",
      "Epoch 138/500\n",
      "5056/5056 [==============================] - 0s 63us/step - loss: 6.8344e-04 - val_loss: 7.1301e-04\n",
      "Epoch 139/500\n",
      "5056/5056 [==============================] - 0s 63us/step - loss: 6.8263e-04 - val_loss: 7.1262e-04\n",
      "Epoch 140/500\n",
      "5056/5056 [==============================] - 0s 66us/step - loss: 6.8176e-04 - val_loss: 7.1191e-04\n",
      "Epoch 141/500\n",
      "5056/5056 [==============================] - 0s 81us/step - loss: 6.8117e-04 - val_loss: 7.1091e-04\n",
      "Epoch 142/500\n",
      "5056/5056 [==============================] - 0s 62us/step - loss: 6.8038e-04 - val_loss: 7.0959e-04\n",
      "Epoch 143/500\n",
      "5056/5056 [==============================] - 0s 64us/step - loss: 6.7962e-04 - val_loss: 7.0976e-04\n",
      "Epoch 144/500\n",
      "5056/5056 [==============================] - 0s 83us/step - loss: 6.7891e-04 - val_loss: 7.0895e-04\n",
      "Epoch 145/500\n",
      "5056/5056 [==============================] - 0s 87us/step - loss: 6.7836e-04 - val_loss: 7.0779e-04\n",
      "Epoch 146/500\n",
      "5056/5056 [==============================] - 0s 84us/step - loss: 6.7767e-04 - val_loss: 7.0751e-04\n",
      "Epoch 147/500\n",
      "5056/5056 [==============================] - 0s 61us/step - loss: 6.7695e-04 - val_loss: 7.0749e-04\n",
      "Epoch 148/500\n",
      "5056/5056 [==============================] - 0s 60us/step - loss: 6.7639e-04 - val_loss: 7.0648e-04\n",
      "Epoch 149/500\n",
      "5056/5056 [==============================] - 0s 70us/step - loss: 6.7573e-04 - val_loss: 7.0524e-04\n",
      "Epoch 150/500\n",
      "5056/5056 [==============================] - 0s 62us/step - loss: 6.7506e-04 - val_loss: 7.0494e-04\n",
      "Epoch 151/500\n",
      "5056/5056 [==============================] - 0s 84us/step - loss: 6.7451e-04 - val_loss: 7.0445e-04\n",
      "Epoch 152/500\n",
      "5056/5056 [==============================] - 0s 69us/step - loss: 6.7386e-04 - val_loss: 7.0348e-04\n",
      "Epoch 153/500\n",
      "5056/5056 [==============================] - 0s 66us/step - loss: 6.7335e-04 - val_loss: 7.0293e-04\n",
      "Epoch 154/500\n",
      "5056/5056 [==============================] - 0s 60us/step - loss: 6.7282e-04 - val_loss: 7.0269e-04\n",
      "Epoch 155/500\n",
      "5056/5056 [==============================] - 0s 63us/step - loss: 6.7213e-04 - val_loss: 7.0215e-04\n",
      "Epoch 156/500\n",
      "5056/5056 [==============================] - 0s 61us/step - loss: 6.7166e-04 - val_loss: 7.0145e-04\n",
      "Epoch 157/500\n",
      "5056/5056 [==============================] - 0s 60us/step - loss: 6.7115e-04 - val_loss: 7.0087e-04\n",
      "Epoch 158/500\n",
      "5056/5056 [==============================] - 0s 63us/step - loss: 6.7059e-04 - val_loss: 7.0043e-04\n",
      "Epoch 159/500\n",
      "5056/5056 [==============================] - 0s 59us/step - loss: 6.7005e-04 - val_loss: 7.0031e-04\n",
      "Epoch 160/500\n",
      "5056/5056 [==============================] - 0s 69us/step - loss: 6.6964e-04 - val_loss: 6.9911e-04\n",
      "Epoch 161/500\n",
      "5056/5056 [==============================] - 0s 63us/step - loss: 6.6905e-04 - val_loss: 6.9891e-04\n",
      "Epoch 162/500\n",
      "5056/5056 [==============================] - 0s 59us/step - loss: 6.6856e-04 - val_loss: 6.9832e-04\n",
      "Epoch 163/500\n",
      "5056/5056 [==============================] - 0s 68us/step - loss: 6.6809e-04 - val_loss: 6.9808e-04\n",
      "Epoch 164/500\n",
      "5056/5056 [==============================] - 0s 59us/step - loss: 6.6752e-04 - val_loss: 6.9823e-04\n",
      "Epoch 165/500\n",
      "5056/5056 [==============================] - 0s 59us/step - loss: 6.6710e-04 - val_loss: 6.9702e-04\n",
      "Epoch 166/500\n",
      "5056/5056 [==============================] - 0s 61us/step - loss: 6.6666e-04 - val_loss: 6.9659e-04\n",
      "Epoch 167/500\n",
      "5056/5056 [==============================] - 0s 63us/step - loss: 6.6623e-04 - val_loss: 6.9625e-04\n",
      "Epoch 168/500\n",
      "5056/5056 [==============================] - 0s 60us/step - loss: 6.6575e-04 - val_loss: 6.9608e-04\n",
      "Epoch 169/500\n",
      "5056/5056 [==============================] - 0s 71us/step - loss: 6.6533e-04 - val_loss: 6.9547e-04\n",
      "Epoch 170/500\n",
      "5056/5056 [==============================] - 0s 65us/step - loss: 6.6490e-04 - val_loss: 6.9496e-04\n",
      "Epoch 171/500\n",
      "5056/5056 [==============================] - 0s 57us/step - loss: 6.6446e-04 - val_loss: 6.9490e-04\n",
      "Epoch 172/500\n",
      "5056/5056 [==============================] - 0s 62us/step - loss: 6.6404e-04 - val_loss: 6.9420e-04\n",
      "Epoch 173/500\n",
      "5056/5056 [==============================] - 0s 60us/step - loss: 6.6367e-04 - val_loss: 6.9379e-04\n",
      "Epoch 174/500\n",
      "5056/5056 [==============================] - 0s 73us/step - loss: 6.6326e-04 - val_loss: 6.9285e-04\n",
      "Epoch 175/500\n",
      "5056/5056 [==============================] - 1s 101us/step - loss: 6.6292e-04 - val_loss: 6.9292e-04\n",
      "Epoch 176/500\n",
      "5056/5056 [==============================] - 0s 98us/step - loss: 6.6253e-04 - val_loss: 6.9227e-04\n",
      "Epoch 177/500\n",
      "5056/5056 [==============================] - 0s 87us/step - loss: 6.6208e-04 - val_loss: 6.9167e-04\n",
      "Epoch 178/500\n",
      "5056/5056 [==============================] - 0s 77us/step - loss: 6.6165e-04 - val_loss: 6.9159e-04\n",
      "Epoch 179/500\n",
      "5056/5056 [==============================] - 0s 60us/step - loss: 6.6128e-04 - val_loss: 6.9109e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 180/500\n",
      "5056/5056 [==============================] - 0s 59us/step - loss: 6.6102e-04 - val_loss: 6.9128e-04\n",
      "Epoch 181/500\n",
      "5056/5056 [==============================] - 0s 53us/step - loss: 6.6070e-04 - val_loss: 6.9053e-04\n",
      "Epoch 182/500\n",
      "5056/5056 [==============================] - 0s 57us/step - loss: 6.6020e-04 - val_loss: 6.9033e-04\n",
      "Epoch 183/500\n",
      "5056/5056 [==============================] - 0s 56us/step - loss: 6.5996e-04 - val_loss: 6.8958e-04\n",
      "Epoch 184/500\n",
      "5056/5056 [==============================] - 0s 59us/step - loss: 6.5954e-04 - val_loss: 6.8932e-04\n",
      "Epoch 185/500\n",
      "5056/5056 [==============================] - 0s 60us/step - loss: 6.5912e-04 - val_loss: 6.8896e-04\n",
      "Epoch 186/500\n",
      "5056/5056 [==============================] - 0s 56us/step - loss: 6.5893e-04 - val_loss: 6.8873e-04\n",
      "Epoch 187/500\n",
      "5056/5056 [==============================] - 0s 60us/step - loss: 6.5859e-04 - val_loss: 6.8843e-04\n",
      "Epoch 188/500\n",
      "5056/5056 [==============================] - 0s 74us/step - loss: 6.5832e-04 - val_loss: 6.8805e-04\n",
      "Epoch 189/500\n",
      "5056/5056 [==============================] - 0s 75us/step - loss: 6.5789e-04 - val_loss: 6.8821e-04\n",
      "Epoch 190/500\n",
      "5056/5056 [==============================] - 0s 73us/step - loss: 6.5773e-04 - val_loss: 6.8767e-04\n",
      "Epoch 191/500\n",
      "5056/5056 [==============================] - 0s 60us/step - loss: 6.5726e-04 - val_loss: 6.8755e-04\n",
      "Epoch 192/500\n",
      "5056/5056 [==============================] - 0s 53us/step - loss: 6.5696e-04 - val_loss: 6.8686e-04\n",
      "Epoch 193/500\n",
      "5056/5056 [==============================] - 0s 54us/step - loss: 6.5675e-04 - val_loss: 6.8681e-04\n",
      "Epoch 194/500\n",
      "5056/5056 [==============================] - 0s 61us/step - loss: 6.5644e-04 - val_loss: 6.8594e-04\n",
      "Epoch 195/500\n",
      "5056/5056 [==============================] - 0s 59us/step - loss: 6.5612e-04 - val_loss: 6.8604e-04\n",
      "Epoch 196/500\n",
      "5056/5056 [==============================] - 0s 56us/step - loss: 6.5591e-04 - val_loss: 6.8618e-04\n",
      "Epoch 197/500\n",
      "5056/5056 [==============================] - 0s 64us/step - loss: 6.5550e-04 - val_loss: 6.8620e-04\n",
      "Epoch 198/500\n",
      "5056/5056 [==============================] - 0s 54us/step - loss: 6.5538e-04 - val_loss: 6.8535e-04\n",
      "Epoch 199/500\n",
      "5056/5056 [==============================] - 0s 57us/step - loss: 6.5508e-04 - val_loss: 6.8527e-04\n",
      "Epoch 200/500\n",
      "5056/5056 [==============================] - 0s 57us/step - loss: 6.5484e-04 - val_loss: 6.8466e-04\n",
      "Epoch 201/500\n",
      "5056/5056 [==============================] - 0s 60us/step - loss: 6.5446e-04 - val_loss: 6.8465e-04\n",
      "Epoch 202/500\n",
      "5056/5056 [==============================] - 0s 58us/step - loss: 6.5431e-04 - val_loss: 6.8416e-04\n",
      "Epoch 203/500\n",
      "5056/5056 [==============================] - 0s 57us/step - loss: 6.5392e-04 - val_loss: 6.8454e-04\n",
      "Epoch 204/500\n",
      "5056/5056 [==============================] - 0s 56us/step - loss: 6.5371e-04 - val_loss: 6.8379e-04\n",
      "Epoch 205/500\n",
      "5056/5056 [==============================] - 0s 59us/step - loss: 6.5346e-04 - val_loss: 6.8360e-04\n",
      "Epoch 206/500\n",
      "5056/5056 [==============================] - 0s 67us/step - loss: 6.5330e-04 - val_loss: 6.8290e-04\n",
      "Epoch 207/500\n",
      "5056/5056 [==============================] - 0s 78us/step - loss: 6.5297e-04 - val_loss: 6.8311e-04\n",
      "Epoch 208/500\n",
      "5056/5056 [==============================] - 1s 150us/step - loss: 6.5277e-04 - val_loss: 6.8253e-04\n",
      "Epoch 209/500\n",
      "5056/5056 [==============================] - 0s 92us/step - loss: 6.5251e-04 - val_loss: 6.8235e-04\n",
      "Epoch 210/500\n",
      "5056/5056 [==============================] - 0s 92us/step - loss: 6.5230e-04 - val_loss: 6.8270e-04\n",
      "Epoch 211/500\n",
      "5056/5056 [==============================] - 0s 82us/step - loss: 6.5218e-04 - val_loss: 6.8192e-04\n",
      "Epoch 212/500\n",
      "5056/5056 [==============================] - 0s 65us/step - loss: 6.5185e-04 - val_loss: 6.8184e-04\n",
      "Epoch 213/500\n",
      "5056/5056 [==============================] - 0s 63us/step - loss: 6.5156e-04 - val_loss: 6.8227e-04\n",
      "Epoch 214/500\n",
      "5056/5056 [==============================] - 0s 51us/step - loss: 6.5147e-04 - val_loss: 6.8139e-04\n",
      "Epoch 215/500\n",
      "5056/5056 [==============================] - 0s 57us/step - loss: 6.5117e-04 - val_loss: 6.8123e-04\n",
      "Epoch 216/500\n",
      "5056/5056 [==============================] - 0s 50us/step - loss: 6.5097e-04 - val_loss: 6.8099e-04\n",
      "Epoch 217/500\n",
      "5056/5056 [==============================] - 0s 63us/step - loss: 6.5079e-04 - val_loss: 6.8078e-04\n",
      "Epoch 218/500\n",
      "5056/5056 [==============================] - 0s 50us/step - loss: 6.5059e-04 - val_loss: 6.8057e-04\n",
      "Epoch 219/500\n",
      "5056/5056 [==============================] - 0s 57us/step - loss: 6.5023e-04 - val_loss: 6.8063e-04\n",
      "Epoch 220/500\n",
      "5056/5056 [==============================] - 0s 50us/step - loss: 6.5031e-04 - val_loss: 6.8049e-04\n",
      "Epoch 221/500\n",
      "5056/5056 [==============================] - 0s 49us/step - loss: 6.4992e-04 - val_loss: 6.8012e-04\n",
      "Epoch 222/500\n",
      "5056/5056 [==============================] - 0s 48us/step - loss: 6.4990e-04 - val_loss: 6.7960e-04\n",
      "Epoch 223/500\n",
      "5056/5056 [==============================] - 0s 49us/step - loss: 6.4950e-04 - val_loss: 6.7987e-04\n",
      "Epoch 224/500\n",
      "5056/5056 [==============================] - 0s 52us/step - loss: 6.4942e-04 - val_loss: 6.7965e-04\n",
      "Epoch 225/500\n",
      "5056/5056 [==============================] - 0s 48us/step - loss: 6.4917e-04 - val_loss: 6.7935e-04\n",
      "Epoch 226/500\n",
      "5056/5056 [==============================] - 0s 48us/step - loss: 6.4902e-04 - val_loss: 6.7924e-04\n",
      "Epoch 227/500\n",
      "5056/5056 [==============================] - 0s 48us/step - loss: 6.4884e-04 - val_loss: 6.7925e-04\n",
      "Epoch 228/500\n",
      "5056/5056 [==============================] - 0s 47us/step - loss: 6.4865e-04 - val_loss: 6.7869e-04\n",
      "Epoch 229/500\n",
      "5056/5056 [==============================] - 0s 53us/step - loss: 6.4842e-04 - val_loss: 6.7853e-04\n",
      "Epoch 230/500\n",
      "5056/5056 [==============================] - 0s 46us/step - loss: 6.4823e-04 - val_loss: 6.7884e-04\n",
      "Epoch 231/500\n",
      "5056/5056 [==============================] - 0s 47us/step - loss: 6.4808e-04 - val_loss: 6.7890e-04\n",
      "Epoch 232/500\n",
      "5056/5056 [==============================] - 0s 49us/step - loss: 6.4790e-04 - val_loss: 6.7823e-04\n",
      "Epoch 233/500\n",
      "5056/5056 [==============================] - 0s 50us/step - loss: 6.4770e-04 - val_loss: 6.7836e-04\n",
      "Epoch 234/500\n",
      "5056/5056 [==============================] - 0s 59us/step - loss: 6.4758e-04 - val_loss: 6.7789e-04\n",
      "Epoch 235/500\n",
      "5056/5056 [==============================] - 0s 52us/step - loss: 6.4744e-04 - val_loss: 6.7751e-04\n",
      "Epoch 236/500\n",
      "5056/5056 [==============================] - 0s 44us/step - loss: 6.4726e-04 - val_loss: 6.7754e-04\n",
      "Epoch 237/500\n",
      "5056/5056 [==============================] - 0s 56us/step - loss: 6.4703e-04 - val_loss: 6.7748e-04\n",
      "Epoch 238/500\n",
      "5056/5056 [==============================] - 0s 50us/step - loss: 6.4696e-04 - val_loss: 6.7704e-04\n",
      "Epoch 239/500\n",
      "5056/5056 [==============================] - 0s 52us/step - loss: 6.4681e-04 - val_loss: 6.7713e-04\n",
      "Epoch 240/500\n",
      "5056/5056 [==============================] - 0s 57us/step - loss: 6.4659e-04 - val_loss: 6.7657e-04\n",
      "Epoch 241/500\n",
      "5056/5056 [==============================] - 0s 50us/step - loss: 6.4647e-04 - val_loss: 6.7650e-04\n",
      "Epoch 242/500\n",
      "5056/5056 [==============================] - 0s 70us/step - loss: 6.4632e-04 - val_loss: 6.7665e-04\n",
      "Epoch 243/500\n",
      "5056/5056 [==============================] - 0s 58us/step - loss: 6.4609e-04 - val_loss: 6.7649e-04\n",
      "Epoch 244/500\n",
      "5056/5056 [==============================] - 0s 56us/step - loss: 6.4598e-04 - val_loss: 6.7637e-04\n",
      "Epoch 245/500\n",
      "5056/5056 [==============================] - 0s 70us/step - loss: 6.4578e-04 - val_loss: 6.7619e-04\n",
      "Epoch 246/500\n",
      "5056/5056 [==============================] - 1s 113us/step - loss: 6.4573e-04 - val_loss: 6.7616e-04\n",
      "Epoch 247/500\n",
      "5056/5056 [==============================] - 0s 96us/step - loss: 6.4556e-04 - val_loss: 6.7573e-04\n",
      "Epoch 248/500\n",
      "5056/5056 [==============================] - 0s 78us/step - loss: 6.4541e-04 - val_loss: 6.7526e-04\n",
      "Epoch 249/500\n",
      "5056/5056 [==============================] - 0s 90us/step - loss: 6.4520e-04 - val_loss: 6.7567e-04\n",
      "Epoch 250/500\n",
      "5056/5056 [==============================] - 0s 88us/step - loss: 6.4507e-04 - val_loss: 6.7513e-04\n",
      "Epoch 251/500\n",
      "5056/5056 [==============================] - 0s 71us/step - loss: 6.4494e-04 - val_loss: 6.7505e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 252/500\n",
      "5056/5056 [==============================] - 0s 64us/step - loss: 6.4482e-04 - val_loss: 6.7514e-04\n",
      "Epoch 253/500\n",
      "5056/5056 [==============================] - 0s 79us/step - loss: 6.4466e-04 - val_loss: 6.7493e-04\n",
      "Epoch 254/500\n",
      "5056/5056 [==============================] - 0s 63us/step - loss: 6.4458e-04 - val_loss: 6.7458e-04\n",
      "Epoch 255/500\n",
      "5056/5056 [==============================] - 0s 64us/step - loss: 6.4436e-04 - val_loss: 6.7494e-04\n",
      "Epoch 256/500\n",
      "5056/5056 [==============================] - 0s 70us/step - loss: 6.4418e-04 - val_loss: 6.7485e-04\n",
      "Epoch 257/500\n",
      "5056/5056 [==============================] - 0s 76us/step - loss: 6.4413e-04 - val_loss: 6.7441e-04\n",
      "Epoch 258/500\n",
      "5056/5056 [==============================] - 0s 65us/step - loss: 6.4404e-04 - val_loss: 6.7404e-04\n",
      "Epoch 259/500\n",
      "5056/5056 [==============================] - 0s 79us/step - loss: 6.4385e-04 - val_loss: 6.7425e-04\n",
      "Epoch 260/500\n",
      "5056/5056 [==============================] - 0s 60us/step - loss: 6.4376e-04 - val_loss: 6.7411e-04\n",
      "Epoch 261/500\n",
      "5056/5056 [==============================] - 0s 71us/step - loss: 6.4363e-04 - val_loss: 6.7414e-04\n",
      "Epoch 262/500\n",
      "5056/5056 [==============================] - 0s 64us/step - loss: 6.4354e-04 - val_loss: 6.7390e-04\n",
      "Epoch 263/500\n",
      "5056/5056 [==============================] - 0s 65us/step - loss: 6.4337e-04 - val_loss: 6.7374e-04\n",
      "Epoch 264/500\n",
      "5056/5056 [==============================] - 0s 74us/step - loss: 6.4331e-04 - val_loss: 6.7383e-04\n",
      "Epoch 265/500\n",
      "5056/5056 [==============================] - 0s 60us/step - loss: 6.4313e-04 - val_loss: 6.7329e-04\n",
      "Epoch 266/500\n",
      "5056/5056 [==============================] - 0s 72us/step - loss: 6.4307e-04 - val_loss: 6.7311e-04\n",
      "Epoch 267/500\n",
      "5056/5056 [==============================] - 0s 74us/step - loss: 6.4294e-04 - val_loss: 6.7328e-04\n",
      "Epoch 268/500\n",
      "5056/5056 [==============================] - 0s 64us/step - loss: 6.4288e-04 - val_loss: 6.7320e-04\n",
      "Epoch 269/500\n",
      "5056/5056 [==============================] - 0s 72us/step - loss: 6.4264e-04 - val_loss: 6.7307e-04\n",
      "Epoch 270/500\n",
      "5056/5056 [==============================] - 0s 70us/step - loss: 6.4248e-04 - val_loss: 6.7350e-04\n",
      "Epoch 271/500\n",
      "5056/5056 [==============================] - 0s 78us/step - loss: 6.4244e-04 - val_loss: 6.7285e-04\n",
      "Epoch 272/500\n",
      "5056/5056 [==============================] - 0s 75us/step - loss: 6.4237e-04 - val_loss: 6.7279e-04\n",
      "Epoch 273/500\n",
      "5056/5056 [==============================] - 0s 73us/step - loss: 6.4220e-04 - val_loss: 6.7263e-04\n",
      "Epoch 274/500\n",
      "5056/5056 [==============================] - 0s 71us/step - loss: 6.4202e-04 - val_loss: 6.7219e-04\n",
      "Epoch 275/500\n",
      "5056/5056 [==============================] - 0s 58us/step - loss: 6.4203e-04 - val_loss: 6.7253e-04\n",
      "Epoch 276/500\n",
      "5056/5056 [==============================] - 0s 72us/step - loss: 6.4194e-04 - val_loss: 6.7228e-04\n",
      "Epoch 277/500\n",
      "5056/5056 [==============================] - 0s 59us/step - loss: 6.4172e-04 - val_loss: 6.7242e-04\n",
      "Epoch 278/500\n",
      "5056/5056 [==============================] - 0s 84us/step - loss: 6.4163e-04 - val_loss: 6.7230e-04\n",
      "Epoch 279/500\n",
      "5056/5056 [==============================] - 0s 59us/step - loss: 6.4157e-04 - val_loss: 6.7174e-04\n",
      "Epoch 280/500\n",
      "5056/5056 [==============================] - 0s 66us/step - loss: 6.4146e-04 - val_loss: 6.7208e-04\n",
      "Epoch 281/500\n",
      "5056/5056 [==============================] - 0s 53us/step - loss: 6.4140e-04 - val_loss: 6.7157e-04\n",
      "Epoch 282/500\n",
      "5056/5056 [==============================] - 0s 53us/step - loss: 6.4136e-04 - val_loss: 6.7135e-04\n",
      "Epoch 283/500\n",
      "5056/5056 [==============================] - 0s 52us/step - loss: 6.4119e-04 - val_loss: 6.7192e-04\n",
      "Epoch 284/500\n",
      "5056/5056 [==============================] - 0s 51us/step - loss: 6.4110e-04 - val_loss: 6.7125e-04\n",
      "Epoch 285/500\n",
      "5056/5056 [==============================] - 0s 53us/step - loss: 6.4090e-04 - val_loss: 6.7135e-04\n",
      "Epoch 286/500\n",
      "5056/5056 [==============================] - 0s 51us/step - loss: 6.4087e-04 - val_loss: 6.7183e-04\n",
      "Epoch 287/500\n",
      "5056/5056 [==============================] - 0s 51us/step - loss: 6.4082e-04 - val_loss: 6.7172e-04\n",
      "Epoch 288/500\n",
      "5056/5056 [==============================] - 0s 52us/step - loss: 6.4072e-04 - val_loss: 6.7139e-04\n",
      "Epoch 289/500\n",
      "5056/5056 [==============================] - 0s 51us/step - loss: 6.4060e-04 - val_loss: 6.7075e-04\n",
      "Epoch 290/500\n",
      "5056/5056 [==============================] - 0s 55us/step - loss: 6.4044e-04 - val_loss: 6.7093e-04\n",
      "Epoch 291/500\n",
      "5056/5056 [==============================] - 0s 54us/step - loss: 6.4051e-04 - val_loss: 6.7069e-04\n",
      "Epoch 292/500\n",
      "5056/5056 [==============================] - 0s 52us/step - loss: 6.4035e-04 - val_loss: 6.7084e-04\n",
      "Epoch 293/500\n",
      "5056/5056 [==============================] - 0s 58us/step - loss: 6.4025e-04 - val_loss: 6.7032e-04\n",
      "Epoch 294/500\n",
      "5056/5056 [==============================] - 0s 53us/step - loss: 6.4021e-04 - val_loss: 6.7024e-04\n",
      "Epoch 295/500\n",
      "5056/5056 [==============================] - 0s 53us/step - loss: 6.4010e-04 - val_loss: 6.7029e-04\n",
      "Epoch 296/500\n",
      "5056/5056 [==============================] - 0s 51us/step - loss: 6.3995e-04 - val_loss: 6.7035e-04\n",
      "Epoch 297/500\n",
      "5056/5056 [==============================] - 0s 51us/step - loss: 6.3995e-04 - val_loss: 6.7024e-04\n",
      "Epoch 298/500\n",
      "5056/5056 [==============================] - 0s 53us/step - loss: 6.3976e-04 - val_loss: 6.7040e-04\n",
      "Epoch 299/500\n",
      "5056/5056 [==============================] - 0s 56us/step - loss: 6.3973e-04 - val_loss: 6.7000e-04\n",
      "Epoch 300/500\n",
      "5056/5056 [==============================] - 0s 53us/step - loss: 6.3971e-04 - val_loss: 6.6986e-04\n",
      "Epoch 301/500\n",
      "5056/5056 [==============================] - 0s 56us/step - loss: 6.3948e-04 - val_loss: 6.6985e-04\n",
      "Epoch 302/500\n",
      "5056/5056 [==============================] - 0s 53us/step - loss: 6.3947e-04 - val_loss: 6.6958e-04\n",
      "Epoch 303/500\n",
      "5056/5056 [==============================] - 0s 52us/step - loss: 6.3931e-04 - val_loss: 6.6968e-04\n",
      "Epoch 304/500\n",
      "5056/5056 [==============================] - 0s 53us/step - loss: 6.3937e-04 - val_loss: 6.6956e-04\n",
      "Epoch 305/500\n",
      "5056/5056 [==============================] - 0s 54us/step - loss: 6.3915e-04 - val_loss: 6.6941e-04\n",
      "Epoch 306/500\n",
      "5056/5056 [==============================] - 0s 52us/step - loss: 6.3913e-04 - val_loss: 6.6993e-04\n",
      "Epoch 307/500\n",
      "5056/5056 [==============================] - 0s 51us/step - loss: 6.3902e-04 - val_loss: 6.6979e-04\n",
      "Epoch 308/500\n",
      "5056/5056 [==============================] - 0s 52us/step - loss: 6.3891e-04 - val_loss: 6.6949e-04\n",
      "Epoch 309/500\n",
      "5056/5056 [==============================] - 0s 52us/step - loss: 6.3887e-04 - val_loss: 6.6907e-04\n",
      "Epoch 310/500\n",
      "5056/5056 [==============================] - 0s 62us/step - loss: 6.3878e-04 - val_loss: 6.6942e-04\n",
      "Epoch 311/500\n",
      "5056/5056 [==============================] - 0s 63us/step - loss: 6.3874e-04 - val_loss: 6.6897e-04\n",
      "Epoch 312/500\n",
      "5056/5056 [==============================] - 0s 60us/step - loss: 6.3867e-04 - val_loss: 6.6919e-04\n",
      "Epoch 313/500\n",
      "5056/5056 [==============================] - 0s 59us/step - loss: 6.3852e-04 - val_loss: 6.6937e-04\n",
      "Epoch 314/500\n",
      "5056/5056 [==============================] - 0s 55us/step - loss: 6.3842e-04 - val_loss: 6.6903e-04\n",
      "Epoch 315/500\n",
      "5056/5056 [==============================] - 0s 52us/step - loss: 6.3835e-04 - val_loss: 6.6861e-04\n",
      "Epoch 316/500\n",
      "5056/5056 [==============================] - 0s 52us/step - loss: 6.3842e-04 - val_loss: 6.6847e-04\n",
      "Epoch 317/500\n",
      "5056/5056 [==============================] - 0s 53us/step - loss: 6.3820e-04 - val_loss: 6.6939e-04\n",
      "Epoch 318/500\n",
      "5056/5056 [==============================] - 0s 52us/step - loss: 6.3822e-04 - val_loss: 6.6870e-04\n",
      "Epoch 319/500\n",
      "5056/5056 [==============================] - 0s 52us/step - loss: 6.3808e-04 - val_loss: 6.6890e-04\n",
      "Epoch 320/500\n",
      "5056/5056 [==============================] - 0s 51us/step - loss: 6.3802e-04 - val_loss: 6.6850e-04\n",
      "Epoch 321/500\n",
      "5056/5056 [==============================] - 0s 52us/step - loss: 6.3789e-04 - val_loss: 6.6918e-04\n",
      "Epoch 322/500\n",
      "5056/5056 [==============================] - 0s 52us/step - loss: 6.3784e-04 - val_loss: 6.6839e-04\n",
      "Epoch 323/500\n",
      "5056/5056 [==============================] - 0s 52us/step - loss: 6.3782e-04 - val_loss: 6.6822e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 324/500\n",
      "5056/5056 [==============================] - 0s 51us/step - loss: 6.3780e-04 - val_loss: 6.6864e-04\n",
      "Epoch 325/500\n",
      "5056/5056 [==============================] - 0s 51us/step - loss: 6.3768e-04 - val_loss: 6.6796e-04\n",
      "Epoch 326/500\n",
      "5056/5056 [==============================] - 0s 50us/step - loss: 6.3763e-04 - val_loss: 6.6810e-04\n",
      "Epoch 327/500\n",
      "5056/5056 [==============================] - 0s 50us/step - loss: 6.3752e-04 - val_loss: 6.6808e-04\n",
      "Epoch 328/500\n",
      "5056/5056 [==============================] - 0s 56us/step - loss: 6.3749e-04 - val_loss: 6.6794e-04\n",
      "Epoch 329/500\n",
      "5056/5056 [==============================] - 0s 50us/step - loss: 6.3740e-04 - val_loss: 6.6771e-04\n",
      "Epoch 330/500\n",
      "5056/5056 [==============================] - 0s 51us/step - loss: 6.3744e-04 - val_loss: 6.6771e-04\n",
      "Epoch 331/500\n",
      "5056/5056 [==============================] - 0s 50us/step - loss: 6.3728e-04 - val_loss: 6.6762e-04\n",
      "Epoch 332/500\n",
      "5056/5056 [==============================] - 0s 51us/step - loss: 6.3721e-04 - val_loss: 6.6760e-04\n",
      "Epoch 333/500\n",
      "5056/5056 [==============================] - 0s 51us/step - loss: 6.3713e-04 - val_loss: 6.6766e-04\n",
      "Epoch 334/500\n",
      "5056/5056 [==============================] - 0s 51us/step - loss: 6.3706e-04 - val_loss: 6.6753e-04\n",
      "Epoch 335/500\n",
      "5056/5056 [==============================] - 0s 50us/step - loss: 6.3694e-04 - val_loss: 6.6795e-04\n",
      "Epoch 336/500\n",
      "5056/5056 [==============================] - 0s 52us/step - loss: 6.3700e-04 - val_loss: 6.6752e-04\n",
      "Epoch 337/500\n",
      "5056/5056 [==============================] - 0s 51us/step - loss: 6.3690e-04 - val_loss: 6.6757e-04\n",
      "Epoch 338/500\n",
      "5056/5056 [==============================] - 0s 52us/step - loss: 6.3690e-04 - val_loss: 6.6722e-04\n",
      "Epoch 339/500\n",
      "5056/5056 [==============================] - 0s 51us/step - loss: 6.3671e-04 - val_loss: 6.6704e-04\n",
      "Epoch 340/500\n",
      "5056/5056 [==============================] - 0s 50us/step - loss: 6.3664e-04 - val_loss: 6.6745e-04\n",
      "Epoch 341/500\n",
      "5056/5056 [==============================] - 0s 51us/step - loss: 6.3668e-04 - val_loss: 6.6662e-04\n",
      "Epoch 342/500\n",
      "5056/5056 [==============================] - 0s 51us/step - loss: 6.3658e-04 - val_loss: 6.6727e-04\n",
      "Epoch 343/500\n",
      "5056/5056 [==============================] - 0s 52us/step - loss: 6.3654e-04 - val_loss: 6.6672e-04\n",
      "Epoch 344/500\n",
      "5056/5056 [==============================] - 0s 51us/step - loss: 6.3633e-04 - val_loss: 6.6708e-04\n",
      "Epoch 345/500\n",
      "5056/5056 [==============================] - 0s 50us/step - loss: 6.3639e-04 - val_loss: 6.6670e-04\n",
      "Epoch 346/500\n",
      "5056/5056 [==============================] - 0s 50us/step - loss: 6.3629e-04 - val_loss: 6.6703e-04\n",
      "Epoch 347/500\n",
      "5056/5056 [==============================] - 0s 60us/step - loss: 6.3624e-04 - val_loss: 6.6683e-04\n",
      "Test MSE: 0.00069516624\n",
      "Total MSE: 0.00065136582\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input-layer (InputLayer)     (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "hidden-layer-1 (Dense)       (None, 150)               45150     \n",
      "_________________________________________________________________\n",
      "hidden-layer-2 (Dense)       (None, 75)                11325     \n",
      "_________________________________________________________________\n",
      "hidden-layer-3 (Dense)       (None, 150)               11400     \n",
      "_________________________________________________________________\n",
      "output-layer (Dense)         (None, 300)               45300     \n",
      "=================================================================\n",
      "Total params: 113,175\n",
      "Trainable params: 113,175\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 5056 samples, validate on 1264 samples\n",
      "Epoch 1/500\n",
      "5056/5056 [==============================] - 1s 130us/step - loss: 0.0182 - val_loss: 0.0087\n",
      "Epoch 2/500\n",
      "5056/5056 [==============================] - 0s 79us/step - loss: 0.0073 - val_loss: 0.0065\n",
      "Epoch 3/500\n",
      "5056/5056 [==============================] - 0s 79us/step - loss: 0.0058 - val_loss: 0.0055\n",
      "Epoch 4/500\n",
      "5056/5056 [==============================] - 0s 83us/step - loss: 0.0050 - val_loss: 0.0048\n",
      "Epoch 5/500\n",
      "5056/5056 [==============================] - 0s 84us/step - loss: 0.0044 - val_loss: 0.0044\n",
      "Epoch 6/500\n",
      "5056/5056 [==============================] - 0s 79us/step - loss: 0.0040 - val_loss: 0.0040\n",
      "Epoch 7/500\n",
      "5056/5056 [==============================] - 0s 80us/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 8/500\n",
      "5056/5056 [==============================] - 0s 79us/step - loss: 0.0034 - val_loss: 0.0035\n",
      "Epoch 9/500\n",
      "5056/5056 [==============================] - 0s 80us/step - loss: 0.0032 - val_loss: 0.0033\n",
      "Epoch 10/500\n",
      "5056/5056 [==============================] - 0s 80us/step - loss: 0.0030 - val_loss: 0.0031\n",
      "Epoch 11/500\n",
      "5056/5056 [==============================] - 0s 81us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 12/500\n",
      "5056/5056 [==============================] - 0s 77us/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 13/500\n",
      "5056/5056 [==============================] - 0s 78us/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 14/500\n",
      "5056/5056 [==============================] - 0s 76us/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 15/500\n",
      "5056/5056 [==============================] - 0s 79us/step - loss: 0.0025 - val_loss: 0.0026\n",
      "Epoch 16/500\n",
      "5056/5056 [==============================] - 0s 78us/step - loss: 0.0024 - val_loss: 0.0025\n",
      "Epoch 17/500\n",
      "5056/5056 [==============================] - 0s 83us/step - loss: 0.0023 - val_loss: 0.0024\n",
      "Epoch 18/500\n",
      "5056/5056 [==============================] - 0s 81us/step - loss: 0.0022 - val_loss: 0.0023\n",
      "Epoch 19/500\n",
      "5056/5056 [==============================] - 0s 81us/step - loss: 0.0022 - val_loss: 0.0023\n",
      "Epoch 20/500\n",
      "5056/5056 [==============================] - 0s 80us/step - loss: 0.0021 - val_loss: 0.0022\n",
      "Epoch 21/500\n",
      "5056/5056 [==============================] - 0s 80us/step - loss: 0.0021 - val_loss: 0.0022\n",
      "Epoch 22/500\n",
      "5056/5056 [==============================] - 0s 80us/step - loss: 0.0020 - val_loss: 0.0021\n",
      "Epoch 23/500\n",
      "5056/5056 [==============================] - 0s 77us/step - loss: 0.0020 - val_loss: 0.0021\n",
      "Epoch 24/500\n",
      "5056/5056 [==============================] - 0s 81us/step - loss: 0.0019 - val_loss: 0.0020\n",
      "Epoch 25/500\n",
      "5056/5056 [==============================] - 0s 86us/step - loss: 0.0019 - val_loss: 0.0020\n",
      "Epoch 26/500\n",
      "5056/5056 [==============================] - 0s 94us/step - loss: 0.0019 - val_loss: 0.0020\n",
      "Epoch 27/500\n",
      "5056/5056 [==============================] - 0s 83us/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 28/500\n",
      "5056/5056 [==============================] - 0s 77us/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 29/500\n",
      "5056/5056 [==============================] - 0s 83us/step - loss: 0.0017 - val_loss: 0.0019\n",
      "Epoch 30/500\n",
      "5056/5056 [==============================] - 0s 81us/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 31/500\n",
      "5056/5056 [==============================] - 0s 78us/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 32/500\n",
      "5056/5056 [==============================] - 0s 79us/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 33/500\n",
      "5056/5056 [==============================] - 0s 80us/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 34/500\n",
      "5056/5056 [==============================] - 0s 85us/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 35/500\n",
      "5056/5056 [==============================] - 0s 83us/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 36/500\n",
      "5056/5056 [==============================] - 0s 82us/step - loss: 0.0015 - val_loss: 0.0017\n",
      "Epoch 37/500\n",
      "5056/5056 [==============================] - 0s 79us/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 38/500\n",
      "5056/5056 [==============================] - 0s 76us/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 39/500\n",
      "5056/5056 [==============================] - 0s 82us/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 40/500\n",
      "5056/5056 [==============================] - 0s 94us/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 41/500\n",
      "5056/5056 [==============================] - 0s 82us/step - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 42/500\n",
      "5056/5056 [==============================] - 0s 81us/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 43/500\n",
      "5056/5056 [==============================] - 0s 78us/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 44/500\n",
      "5056/5056 [==============================] - 0s 80us/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 45/500\n",
      "5056/5056 [==============================] - 0s 79us/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 46/500\n",
      "5056/5056 [==============================] - 0s 80us/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 47/500\n",
      "5056/5056 [==============================] - 0s 75us/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 48/500\n",
      "5056/5056 [==============================] - 0s 76us/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 49/500\n",
      "5056/5056 [==============================] - 0s 79us/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 50/500\n",
      "5056/5056 [==============================] - 0s 75us/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 51/500\n",
      "5056/5056 [==============================] - 0s 77us/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 52/500\n",
      "5056/5056 [==============================] - 0s 83us/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 53/500\n",
      "5056/5056 [==============================] - 0s 85us/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 54/500\n",
      "5056/5056 [==============================] - 0s 83us/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 55/500\n",
      "5056/5056 [==============================] - 0s 78us/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 56/500\n",
      "5056/5056 [==============================] - 0s 79us/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 57/500\n",
      "5056/5056 [==============================] - 0s 78us/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 58/500\n",
      "5056/5056 [==============================] - 0s 78us/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 59/500\n",
      "5056/5056 [==============================] - 0s 78us/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 60/500\n",
      "5056/5056 [==============================] - 0s 81us/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 61/500\n",
      "5056/5056 [==============================] - 0s 76us/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 62/500\n",
      "5056/5056 [==============================] - 0s 78us/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 63/500\n",
      "5056/5056 [==============================] - 0s 76us/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 64/500\n",
      "5056/5056 [==============================] - 0s 79us/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 65/500\n",
      "5056/5056 [==============================] - 0s 79us/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 66/500\n",
      "5056/5056 [==============================] - 0s 78us/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 67/500\n",
      "5056/5056 [==============================] - 0s 79us/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 68/500\n",
      "5056/5056 [==============================] - 0s 75us/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 69/500\n",
      "5056/5056 [==============================] - 0s 77us/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 70/500\n",
      "5056/5056 [==============================] - 0s 79us/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 71/500\n",
      "5056/5056 [==============================] - 0s 85us/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 72/500\n",
      "5056/5056 [==============================] - 0s 79us/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 73/500\n",
      "5056/5056 [==============================] - 0s 75us/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 74/500\n",
      "5056/5056 [==============================] - 0s 80us/step - loss: 0.0010 - val_loss: 0.0012\n",
      "Epoch 75/500\n",
      "5056/5056 [==============================] - 0s 75us/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 76/500\n",
      "5056/5056 [==============================] - 0s 77us/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 77/500\n",
      "5056/5056 [==============================] - 0s 81us/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 78/500\n",
      "5056/5056 [==============================] - 0s 81us/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 79/500\n",
      "5056/5056 [==============================] - 0s 94us/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 80/500\n",
      "5056/5056 [==============================] - 0s 90us/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 81/500\n",
      "5056/5056 [==============================] - 0s 79us/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 82/500\n",
      "5056/5056 [==============================] - 0s 79us/step - loss: 9.9599e-04 - val_loss: 0.0011\n",
      "Epoch 83/500\n",
      "5056/5056 [==============================] - 0s 79us/step - loss: 9.8995e-04 - val_loss: 0.0011\n",
      "Epoch 84/500\n",
      "5056/5056 [==============================] - 0s 79us/step - loss: 9.8433e-04 - val_loss: 0.0011\n",
      "Epoch 85/500\n",
      "5056/5056 [==============================] - 0s 83us/step - loss: 9.7874e-04 - val_loss: 0.0011\n",
      "Epoch 86/500\n",
      "5056/5056 [==============================] - 0s 80us/step - loss: 9.7337e-04 - val_loss: 0.0011\n",
      "Epoch 87/500\n",
      "5056/5056 [==============================] - 0s 81us/step - loss: 9.6818e-04 - val_loss: 0.0011\n",
      "Epoch 88/500\n",
      "5056/5056 [==============================] - 0s 92us/step - loss: 9.6291e-04 - val_loss: 0.0011\n",
      "Epoch 89/500\n",
      "5056/5056 [==============================] - 0s 88us/step - loss: 9.5778e-04 - val_loss: 0.0011\n",
      "Epoch 90/500\n",
      "5056/5056 [==============================] - 0s 85us/step - loss: 9.5282e-04 - val_loss: 0.0011\n",
      "Epoch 91/500\n",
      "5056/5056 [==============================] - 0s 87us/step - loss: 9.4797e-04 - val_loss: 0.0011\n",
      "Epoch 92/500\n",
      "5056/5056 [==============================] - 0s 83us/step - loss: 9.4320e-04 - val_loss: 0.0010\n",
      "Epoch 93/500\n",
      "5056/5056 [==============================] - 0s 86us/step - loss: 9.3847e-04 - val_loss: 0.0010\n",
      "Epoch 94/500\n",
      "5056/5056 [==============================] - 0s 92us/step - loss: 9.3391e-04 - val_loss: 0.0010\n",
      "Epoch 95/500\n",
      "5056/5056 [==============================] - 0s 82us/step - loss: 9.2930e-04 - val_loss: 0.0010\n",
      "Epoch 96/500\n",
      "5056/5056 [==============================] - 0s 80us/step - loss: 9.2494e-04 - val_loss: 0.0010\n",
      "Epoch 97/500\n",
      "5056/5056 [==============================] - 0s 85us/step - loss: 9.2066e-04 - val_loss: 0.0010\n",
      "Epoch 98/500\n",
      "5056/5056 [==============================] - 0s 80us/step - loss: 9.1616e-04 - val_loss: 0.0010\n",
      "Epoch 99/500\n",
      "5056/5056 [==============================] - 0s 84us/step - loss: 9.1208e-04 - val_loss: 0.0010\n",
      "Epoch 100/500\n",
      "5056/5056 [==============================] - 0s 92us/step - loss: 9.0791e-04 - val_loss: 0.0010\n",
      "Epoch 101/500\n",
      "5056/5056 [==============================] - 0s 80us/step - loss: 9.0406e-04 - val_loss: 0.0010\n",
      "Epoch 102/500\n",
      "5056/5056 [==============================] - 0s 91us/step - loss: 8.9994e-04 - val_loss: 0.0010\n",
      "Epoch 103/500\n",
      "5056/5056 [==============================] - 0s 86us/step - loss: 8.9597e-04 - val_loss: 0.0010\n",
      "Epoch 104/500\n",
      "5056/5056 [==============================] - 0s 89us/step - loss: 8.9225e-04 - val_loss: 9.9476e-04\n",
      "Epoch 105/500\n",
      "5056/5056 [==============================] - 1s 102us/step - loss: 8.8851e-04 - val_loss: 9.9104e-04\n",
      "Epoch 106/500\n",
      "5056/5056 [==============================] - 0s 98us/step - loss: 8.8487e-04 - val_loss: 9.8712e-04\n",
      "Epoch 107/500\n",
      "5056/5056 [==============================] - 0s 91us/step - loss: 8.8112e-04 - val_loss: 9.8379e-04\n",
      "Epoch 108/500\n",
      "5056/5056 [==============================] - 0s 82us/step - loss: 8.7767e-04 - val_loss: 9.7979e-04\n",
      "Epoch 109/500\n",
      "5056/5056 [==============================] - 0s 89us/step - loss: 8.7390e-04 - val_loss: 9.7622e-04\n",
      "Epoch 110/500\n",
      "5056/5056 [==============================] - 0s 81us/step - loss: 8.7043e-04 - val_loss: 9.7266e-04\n",
      "Epoch 111/500\n",
      "5056/5056 [==============================] - 0s 80us/step - loss: 8.6717e-04 - val_loss: 9.6818e-04\n",
      "Epoch 112/500\n",
      "5056/5056 [==============================] - 0s 82us/step - loss: 8.6354e-04 - val_loss: 9.6550e-04\n",
      "Epoch 113/500\n",
      "5056/5056 [==============================] - 0s 80us/step - loss: 8.6032e-04 - val_loss: 9.6380e-04\n",
      "Epoch 114/500\n",
      "5056/5056 [==============================] - 0s 86us/step - loss: 8.5727e-04 - val_loss: 9.5869e-04\n",
      "Epoch 115/500\n",
      "5056/5056 [==============================] - 0s 85us/step - loss: 8.5391e-04 - val_loss: 9.5484e-04\n",
      "Epoch 116/500\n",
      "5056/5056 [==============================] - 0s 86us/step - loss: 8.5077e-04 - val_loss: 9.5141e-04\n",
      "Epoch 117/500\n",
      "5056/5056 [==============================] - 0s 84us/step - loss: 8.4756e-04 - val_loss: 9.4906e-04\n",
      "Epoch 118/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5056/5056 [==============================] - 0s 77us/step - loss: 8.4463e-04 - val_loss: 9.4525e-04\n",
      "Epoch 119/500\n",
      "5056/5056 [==============================] - 0s 76us/step - loss: 8.4151e-04 - val_loss: 9.4225e-04\n",
      "Epoch 120/500\n",
      "5056/5056 [==============================] - 0s 76us/step - loss: 8.3851e-04 - val_loss: 9.3881e-04\n",
      "Epoch 121/500\n",
      "5056/5056 [==============================] - 0s 79us/step - loss: 8.3555e-04 - val_loss: 9.3568e-04\n",
      "Epoch 122/500\n",
      "5056/5056 [==============================] - 0s 75us/step - loss: 8.3278e-04 - val_loss: 9.3256e-04\n",
      "Epoch 123/500\n",
      "5056/5056 [==============================] - 0s 77us/step - loss: 8.2978e-04 - val_loss: 9.2938e-04\n",
      "Epoch 124/500\n",
      "5056/5056 [==============================] - 0s 76us/step - loss: 8.2708e-04 - val_loss: 9.2663e-04\n",
      "Epoch 125/500\n",
      "5056/5056 [==============================] - 0s 76us/step - loss: 8.2425e-04 - val_loss: 9.2343e-04\n",
      "Epoch 126/500\n",
      "5056/5056 [==============================] - 0s 79us/step - loss: 8.2156e-04 - val_loss: 9.2057e-04\n",
      "Epoch 127/500\n",
      "5056/5056 [==============================] - 0s 76us/step - loss: 8.1885e-04 - val_loss: 9.1912e-04\n",
      "Epoch 128/500\n",
      "5056/5056 [==============================] - 1s 104us/step - loss: 8.1614e-04 - val_loss: 9.1537e-04\n",
      "Epoch 129/500\n",
      "5056/5056 [==============================] - 0s 83us/step - loss: 8.1350e-04 - val_loss: 9.1208e-04\n",
      "Epoch 130/500\n",
      "5056/5056 [==============================] - 0s 85us/step - loss: 8.1107e-04 - val_loss: 9.0890e-04\n",
      "Epoch 131/500\n",
      "5056/5056 [==============================] - 1s 155us/step - loss: 8.0841e-04 - val_loss: 9.0735e-04\n",
      "Epoch 132/500\n",
      "5056/5056 [==============================] - 1s 134us/step - loss: 8.0592e-04 - val_loss: 9.0426e-04\n",
      "Epoch 133/500\n",
      "5056/5056 [==============================] - 0s 92us/step - loss: 8.0338e-04 - val_loss: 9.0235e-04\n",
      "Epoch 134/500\n",
      "5056/5056 [==============================] - 0s 93us/step - loss: 8.0096e-04 - val_loss: 8.9957e-04\n",
      "Epoch 135/500\n",
      "5056/5056 [==============================] - 0s 85us/step - loss: 7.9854e-04 - val_loss: 8.9639e-04\n",
      "Epoch 136/500\n",
      "5056/5056 [==============================] - 0s 90us/step - loss: 7.9616e-04 - val_loss: 8.9328e-04\n",
      "Epoch 137/500\n",
      "5056/5056 [==============================] - 0s 87us/step - loss: 7.9380e-04 - val_loss: 8.9110e-04\n",
      "Epoch 138/500\n",
      "5056/5056 [==============================] - 0s 84us/step - loss: 7.9138e-04 - val_loss: 8.8988e-04\n",
      "Epoch 139/500\n",
      "5056/5056 [==============================] - 0s 84us/step - loss: 7.8926e-04 - val_loss: 8.8575e-04\n",
      "Epoch 140/500\n",
      "5056/5056 [==============================] - 1s 163us/step - loss: 7.8692e-04 - val_loss: 8.8455e-04\n",
      "Epoch 141/500\n",
      "5056/5056 [==============================] - 0s 95us/step - loss: 7.8473e-04 - val_loss: 8.8129e-04\n",
      "Epoch 142/500\n",
      "5056/5056 [==============================] - 0s 98us/step - loss: 7.8256e-04 - val_loss: 8.7941e-04\n",
      "Epoch 143/500\n",
      "5056/5056 [==============================] - 0s 83us/step - loss: 7.8031e-04 - val_loss: 8.7769e-04\n",
      "Epoch 144/500\n",
      "5056/5056 [==============================] - 0s 92us/step - loss: 7.7830e-04 - val_loss: 8.7481e-04\n",
      "Epoch 145/500\n",
      "5056/5056 [==============================] - 0s 85us/step - loss: 7.7607e-04 - val_loss: 8.7351e-04\n",
      "Epoch 146/500\n",
      "5056/5056 [==============================] - 0s 87us/step - loss: 7.7408e-04 - val_loss: 8.7156e-04\n",
      "Epoch 147/500\n",
      "5056/5056 [==============================] - 1s 116us/step - loss: 7.7211e-04 - val_loss: 8.6803e-04\n",
      "Epoch 148/500\n",
      "5056/5056 [==============================] - 0s 92us/step - loss: 7.6999e-04 - val_loss: 8.6647e-04\n",
      "Epoch 149/500\n",
      "5056/5056 [==============================] - 0s 86us/step - loss: 7.6802e-04 - val_loss: 8.6477e-04\n",
      "Epoch 150/500\n",
      "5056/5056 [==============================] - 0s 82us/step - loss: 7.6609e-04 - val_loss: 8.6249e-04\n",
      "Epoch 151/500\n",
      "5056/5056 [==============================] - 0s 92us/step - loss: 7.6408e-04 - val_loss: 8.5981e-04\n",
      "Epoch 152/500\n",
      "5056/5056 [==============================] - 1s 124us/step - loss: 7.6211e-04 - val_loss: 8.5796e-04\n",
      "Epoch 153/500\n",
      "5056/5056 [==============================] - 1s 140us/step - loss: 7.6038e-04 - val_loss: 8.5557e-04\n",
      "Epoch 154/500\n",
      "5056/5056 [==============================] - 1s 120us/step - loss: 7.5843e-04 - val_loss: 8.5414e-04\n",
      "Epoch 155/500\n",
      "5056/5056 [==============================] - 0s 98us/step - loss: 7.5664e-04 - val_loss: 8.5156e-04\n",
      "Epoch 156/500\n",
      "5056/5056 [==============================] - 0s 95us/step - loss: 7.5493e-04 - val_loss: 8.4942e-04\n",
      "Epoch 157/500\n",
      "5056/5056 [==============================] - 0s 82us/step - loss: 7.5312e-04 - val_loss: 8.4765e-04\n",
      "Epoch 158/500\n",
      "5056/5056 [==============================] - 0s 71us/step - loss: 7.5132e-04 - val_loss: 8.4606e-04\n",
      "Epoch 159/500\n",
      "5056/5056 [==============================] - 0s 71us/step - loss: 7.4973e-04 - val_loss: 8.4379e-04\n",
      "Epoch 160/500\n",
      "5056/5056 [==============================] - 0s 74us/step - loss: 7.4784e-04 - val_loss: 8.4207e-04\n",
      "Epoch 161/500\n",
      "5056/5056 [==============================] - 0s 79us/step - loss: 7.4629e-04 - val_loss: 8.4027e-04\n",
      "Epoch 162/500\n",
      "5056/5056 [==============================] - 0s 73us/step - loss: 7.4466e-04 - val_loss: 8.3965e-04\n",
      "Epoch 163/500\n",
      "5056/5056 [==============================] - 0s 71us/step - loss: 7.4298e-04 - val_loss: 8.3812e-04\n",
      "Epoch 164/500\n",
      "5056/5056 [==============================] - 0s 71us/step - loss: 7.4142e-04 - val_loss: 8.3586e-04\n",
      "Epoch 165/500\n",
      "5056/5056 [==============================] - 0s 71us/step - loss: 7.3972e-04 - val_loss: 8.3384e-04\n",
      "Epoch 166/500\n",
      "5056/5056 [==============================] - 0s 71us/step - loss: 7.3829e-04 - val_loss: 8.3209e-04\n",
      "Epoch 167/500\n",
      "5056/5056 [==============================] - 0s 77us/step - loss: 7.3686e-04 - val_loss: 8.3020e-04\n",
      "Epoch 168/500\n",
      "5056/5056 [==============================] - 0s 73us/step - loss: 7.3527e-04 - val_loss: 8.2937e-04\n",
      "Epoch 169/500\n",
      "5056/5056 [==============================] - 0s 78us/step - loss: 7.3380e-04 - val_loss: 8.2784e-04\n",
      "Epoch 170/500\n",
      "5056/5056 [==============================] - 0s 76us/step - loss: 7.3231e-04 - val_loss: 8.2582e-04\n",
      "Epoch 171/500\n",
      "5056/5056 [==============================] - 0s 83us/step - loss: 7.3091e-04 - val_loss: 8.2365e-04\n",
      "Epoch 172/500\n",
      "5056/5056 [==============================] - 0s 81us/step - loss: 7.2948e-04 - val_loss: 8.2278e-04\n",
      "Epoch 173/500\n",
      "5056/5056 [==============================] - 0s 80us/step - loss: 7.2795e-04 - val_loss: 8.2156e-04\n",
      "Epoch 174/500\n",
      "5056/5056 [==============================] - 0s 81us/step - loss: 7.2675e-04 - val_loss: 8.2005e-04\n",
      "Epoch 175/500\n",
      "5056/5056 [==============================] - 0s 87us/step - loss: 7.2537e-04 - val_loss: 8.1783e-04\n",
      "Epoch 176/500\n",
      "5056/5056 [==============================] - 0s 82us/step - loss: 7.2407e-04 - val_loss: 8.1672e-04\n",
      "Epoch 177/500\n",
      "5056/5056 [==============================] - 0s 72us/step - loss: 7.2271e-04 - val_loss: 8.1551e-04\n",
      "Epoch 178/500\n",
      "5056/5056 [==============================] - 0s 75us/step - loss: 7.2146e-04 - val_loss: 8.1376e-04\n",
      "Epoch 179/500\n",
      "5056/5056 [==============================] - 0s 71us/step - loss: 7.2016e-04 - val_loss: 8.1255e-04\n",
      "Epoch 180/500\n",
      "5056/5056 [==============================] - 0s 70us/step - loss: 7.1892e-04 - val_loss: 8.1121e-04\n",
      "Epoch 181/500\n",
      "5056/5056 [==============================] - 0s 81us/step - loss: 7.1767e-04 - val_loss: 8.0990e-04\n",
      "Epoch 182/500\n",
      "5056/5056 [==============================] - 0s 80us/step - loss: 7.1648e-04 - val_loss: 8.0832e-04\n",
      "Epoch 183/500\n",
      "5056/5056 [==============================] - 0s 84us/step - loss: 7.1527e-04 - val_loss: 8.0801e-04\n",
      "Epoch 184/500\n",
      "5056/5056 [==============================] - 0s 97us/step - loss: 7.1420e-04 - val_loss: 8.0682e-04\n",
      "Epoch 185/500\n",
      "5056/5056 [==============================] - 0s 85us/step - loss: 7.1307e-04 - val_loss: 8.0514e-04\n",
      "Epoch 186/500\n",
      "5056/5056 [==============================] - 1s 125us/step - loss: 7.1192e-04 - val_loss: 8.0370e-04\n",
      "Epoch 187/500\n",
      "5056/5056 [==============================] - 0s 94us/step - loss: 7.1083e-04 - val_loss: 8.0230e-04\n",
      "Epoch 188/500\n",
      "5056/5056 [==============================] - 1s 126us/step - loss: 7.0961e-04 - val_loss: 8.0146e-04\n",
      "Epoch 189/500\n",
      "5056/5056 [==============================] - 1s 155us/step - loss: 7.0865e-04 - val_loss: 8.0032e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 190/500\n",
      "5056/5056 [==============================] - 1s 105us/step - loss: 7.0759e-04 - val_loss: 7.9882e-04\n",
      "Epoch 191/500\n",
      "5056/5056 [==============================] - 0s 93us/step - loss: 7.0649e-04 - val_loss: 7.9859e-04\n",
      "Epoch 192/500\n",
      "5056/5056 [==============================] - 1s 133us/step - loss: 7.0545e-04 - val_loss: 7.9723e-04\n",
      "Epoch 193/500\n",
      "5056/5056 [==============================] - 1s 119us/step - loss: 7.0452e-04 - val_loss: 7.9640e-04\n",
      "Epoch 194/500\n",
      "5056/5056 [==============================] - 0s 91us/step - loss: 7.0349e-04 - val_loss: 7.9472e-04\n",
      "Epoch 195/500\n",
      "5056/5056 [==============================] - 0s 82us/step - loss: 7.0259e-04 - val_loss: 7.9440e-04\n",
      "Epoch 196/500\n",
      "5056/5056 [==============================] - 1s 139us/step - loss: 7.0153e-04 - val_loss: 7.9291e-04\n",
      "Epoch 197/500\n",
      "5056/5056 [==============================] - 1s 137us/step - loss: 7.0056e-04 - val_loss: 7.9190e-04\n",
      "Epoch 198/500\n",
      "5056/5056 [==============================] - 1s 150us/step - loss: 6.9966e-04 - val_loss: 7.9061e-04\n",
      "Epoch 199/500\n",
      "5056/5056 [==============================] - 0s 88us/step - loss: 6.9894e-04 - val_loss: 7.9000e-04\n",
      "Epoch 200/500\n",
      "5056/5056 [==============================] - 0s 81us/step - loss: 6.9781e-04 - val_loss: 7.8896e-04\n",
      "Epoch 201/500\n",
      "5056/5056 [==============================] - 1s 128us/step - loss: 6.9697e-04 - val_loss: 7.8838e-04\n",
      "Epoch 202/500\n",
      "5056/5056 [==============================] - 1s 128us/step - loss: 6.9609e-04 - val_loss: 7.8697e-04\n",
      "Epoch 203/500\n",
      "5056/5056 [==============================] - 1s 122us/step - loss: 6.9529e-04 - val_loss: 7.8637e-04\n",
      "Epoch 204/500\n",
      "5056/5056 [==============================] - 1s 157us/step - loss: 6.9442e-04 - val_loss: 7.8509e-04\n",
      "Epoch 205/500\n",
      "5056/5056 [==============================] - 1s 122us/step - loss: 6.9359e-04 - val_loss: 7.8371e-04\n",
      "Epoch 206/500\n",
      "5056/5056 [==============================] - 1s 138us/step - loss: 6.9286e-04 - val_loss: 7.8327e-04\n",
      "Epoch 207/500\n",
      "5056/5056 [==============================] - 1s 134us/step - loss: 6.9208e-04 - val_loss: 7.8198e-04\n",
      "Epoch 208/500\n",
      "5056/5056 [==============================] - 1s 105us/step - loss: 6.9126e-04 - val_loss: 7.8159e-04\n",
      "Epoch 209/500\n",
      "5056/5056 [==============================] - 1s 123us/step - loss: 6.9040e-04 - val_loss: 7.8099e-04\n",
      "Epoch 210/500\n",
      "5056/5056 [==============================] - 1s 103us/step - loss: 6.8967e-04 - val_loss: 7.8003e-04\n",
      "Epoch 211/500\n",
      "5056/5056 [==============================] - 1s 99us/step - loss: 6.8898e-04 - val_loss: 7.7904e-04\n",
      "Epoch 212/500\n",
      "5056/5056 [==============================] - 1s 112us/step - loss: 6.8823e-04 - val_loss: 7.7883e-04\n",
      "Epoch 213/500\n",
      "5056/5056 [==============================] - 1s 116us/step - loss: 6.8740e-04 - val_loss: 7.7797e-04\n",
      "Epoch 214/500\n",
      "5056/5056 [==============================] - 1s 163us/step - loss: 6.8681e-04 - val_loss: 7.7642e-04\n",
      "Epoch 215/500\n",
      "5056/5056 [==============================] - 1s 118us/step - loss: 6.8607e-04 - val_loss: 7.7694e-04\n",
      "Epoch 216/500\n",
      "5056/5056 [==============================] - 1s 203us/step - loss: 6.8534e-04 - val_loss: 7.7491e-04\n",
      "Epoch 217/500\n",
      "5056/5056 [==============================] - 1s 140us/step - loss: 6.8472e-04 - val_loss: 7.7437e-04\n",
      "Epoch 218/500\n",
      "5056/5056 [==============================] - 0s 96us/step - loss: 6.8397e-04 - val_loss: 7.7412e-04\n",
      "Epoch 219/500\n",
      "5056/5056 [==============================] - 1s 114us/step - loss: 6.8326e-04 - val_loss: 7.7352e-04\n",
      "Epoch 220/500\n",
      "5056/5056 [==============================] - 0s 86us/step - loss: 6.8270e-04 - val_loss: 7.7230e-04\n",
      "Epoch 221/500\n",
      "5056/5056 [==============================] - 0s 97us/step - loss: 6.8189e-04 - val_loss: 7.7223e-04\n",
      "Epoch 222/500\n",
      "5056/5056 [==============================] - 0s 86us/step - loss: 6.8144e-04 - val_loss: 7.7085e-04\n",
      "Epoch 223/500\n",
      "5056/5056 [==============================] - 0s 94us/step - loss: 6.8077e-04 - val_loss: 7.7029e-04\n",
      "Epoch 224/500\n",
      "5056/5056 [==============================] - 0s 91us/step - loss: 6.8017e-04 - val_loss: 7.6934e-04\n",
      "Epoch 225/500\n",
      "5056/5056 [==============================] - 1s 99us/step - loss: 6.7952e-04 - val_loss: 7.6877e-04\n",
      "Epoch 226/500\n",
      "5056/5056 [==============================] - 1s 124us/step - loss: 6.7903e-04 - val_loss: 7.6825e-04\n",
      "Epoch 227/500\n",
      "5056/5056 [==============================] - 0s 97us/step - loss: 6.7825e-04 - val_loss: 7.6864e-04\n",
      "Epoch 228/500\n",
      "5056/5056 [==============================] - 1s 135us/step - loss: 6.7772e-04 - val_loss: 7.6653e-04\n",
      "Epoch 229/500\n",
      "5056/5056 [==============================] - ETA: 0s - loss: 6.7717e-0 - 1s 123us/step - loss: 6.7720e-04 - val_loss: 7.6615e-04\n",
      "Epoch 230/500\n",
      "5056/5056 [==============================] - 1s 181us/step - loss: 6.7672e-04 - val_loss: 7.6624e-04\n",
      "Epoch 231/500\n",
      "5056/5056 [==============================] - 1s 186us/step - loss: 6.7597e-04 - val_loss: 7.6541e-04\n",
      "Epoch 232/500\n",
      "5056/5056 [==============================] - 1s 107us/step - loss: 6.7560e-04 - val_loss: 7.6453e-04\n",
      "Epoch 233/500\n",
      "5056/5056 [==============================] - 0s 98us/step - loss: 6.7502e-04 - val_loss: 7.6417e-04\n",
      "Epoch 234/500\n",
      "5056/5056 [==============================] - 0s 95us/step - loss: 6.7454e-04 - val_loss: 7.6329e-04\n",
      "Epoch 235/500\n",
      "5056/5056 [==============================] - 0s 84us/step - loss: 6.7392e-04 - val_loss: 7.6323e-04\n",
      "Epoch 236/500\n",
      "5056/5056 [==============================] - 0s 88us/step - loss: 6.7338e-04 - val_loss: 7.6275e-04\n",
      "Epoch 237/500\n",
      "5056/5056 [==============================] - 0s 97us/step - loss: 6.7303e-04 - val_loss: 7.6192e-04\n",
      "Epoch 238/500\n",
      "5056/5056 [==============================] - 1s 104us/step - loss: 6.7235e-04 - val_loss: 7.6161e-04\n",
      "Epoch 239/500\n",
      "5056/5056 [==============================] - 0s 83us/step - loss: 6.7196e-04 - val_loss: 7.6090e-04\n",
      "Epoch 240/500\n",
      "5056/5056 [==============================] - 0s 90us/step - loss: 6.7154e-04 - val_loss: 7.6024e-04\n",
      "Epoch 241/500\n",
      "5056/5056 [==============================] - 0s 86us/step - loss: 6.7094e-04 - val_loss: 7.5936e-04\n",
      "Epoch 242/500\n",
      "5056/5056 [==============================] - 1s 139us/step - loss: 6.7055e-04 - val_loss: 7.5922e-04\n",
      "Epoch 243/500\n",
      "5056/5056 [==============================] - 0s 92us/step - loss: 6.7008e-04 - val_loss: 7.5910e-04\n",
      "Epoch 244/500\n",
      "5056/5056 [==============================] - 0s 92us/step - loss: 6.6960e-04 - val_loss: 7.5810e-04\n",
      "Epoch 245/500\n",
      "5056/5056 [==============================] - 1s 100us/step - loss: 6.6912e-04 - val_loss: 7.5734e-04\n",
      "Epoch 246/500\n",
      "5056/5056 [==============================] - 0s 93us/step - loss: 6.6867e-04 - val_loss: 7.5750e-04\n",
      "Epoch 247/500\n",
      "5056/5056 [==============================] - 0s 91us/step - loss: 6.6822e-04 - val_loss: 7.5685e-04\n",
      "Epoch 248/500\n",
      "5056/5056 [==============================] - 1s 106us/step - loss: 6.6776e-04 - val_loss: 7.5634e-04\n",
      "Epoch 249/500\n",
      "5056/5056 [==============================] - 0s 87us/step - loss: 6.6737e-04 - val_loss: 7.5554e-04\n",
      "Epoch 250/500\n",
      "5056/5056 [==============================] - 1s 108us/step - loss: 6.6695e-04 - val_loss: 7.5608e-04\n",
      "Epoch 251/500\n",
      "5056/5056 [==============================] - 1s 127us/step - loss: 6.6657e-04 - val_loss: 7.5500e-04\n",
      "Epoch 252/500\n",
      "5056/5056 [==============================] - 0s 90us/step - loss: 6.6609e-04 - val_loss: 7.5460e-04\n",
      "Epoch 253/500\n",
      "5056/5056 [==============================] - 0s 94us/step - loss: 6.6582e-04 - val_loss: 7.5422e-04\n",
      "Epoch 254/500\n",
      "5056/5056 [==============================] - 0s 97us/step - loss: 6.6531e-04 - val_loss: 7.5398e-04\n",
      "Epoch 255/500\n",
      "5056/5056 [==============================] - 0s 93us/step - loss: 6.6485e-04 - val_loss: 7.5283e-04\n",
      "Epoch 256/500\n",
      "5056/5056 [==============================] - 0s 94us/step - loss: 6.6449e-04 - val_loss: 7.5284e-04\n",
      "Epoch 257/500\n",
      "5056/5056 [==============================] - 0s 91us/step - loss: 6.6414e-04 - val_loss: 7.5227e-04\n",
      "Epoch 258/500\n",
      "5056/5056 [==============================] - 0s 98us/step - loss: 6.6371e-04 - val_loss: 7.5206e-04\n",
      "Epoch 259/500\n",
      "5056/5056 [==============================] - 0s 99us/step - loss: 6.6335e-04 - val_loss: 7.5142e-04\n",
      "Epoch 260/500\n",
      "5056/5056 [==============================] - 0s 94us/step - loss: 6.6299e-04 - val_loss: 7.5111e-04\n",
      "Epoch 261/500\n",
      "5056/5056 [==============================] - 0s 97us/step - loss: 6.6263e-04 - val_loss: 7.5087e-04\n",
      "Epoch 262/500\n",
      "5056/5056 [==============================] - 0s 91us/step - loss: 6.6234e-04 - val_loss: 7.4996e-04\n",
      "Epoch 263/500\n",
      "5056/5056 [==============================] - 0s 84us/step - loss: 6.6195e-04 - val_loss: 7.4968e-04\n",
      "Epoch 264/500\n",
      "5056/5056 [==============================] - 1s 100us/step - loss: 6.6165e-04 - val_loss: 7.4934e-04\n",
      "Epoch 265/500\n",
      "5056/5056 [==============================] - 0s 96us/step - loss: 6.6129e-04 - val_loss: 7.4963e-04\n",
      "Epoch 266/500\n",
      "5056/5056 [==============================] - 0s 92us/step - loss: 6.6092e-04 - val_loss: 7.4857e-04\n",
      "Epoch 267/500\n",
      "5056/5056 [==============================] - 0s 96us/step - loss: 6.6052e-04 - val_loss: 7.4882e-04\n",
      "Epoch 268/500\n",
      "5056/5056 [==============================] - 1s 104us/step - loss: 6.6023e-04 - val_loss: 7.4828e-04\n",
      "Epoch 269/500\n",
      "5056/5056 [==============================] - 0s 95us/step - loss: 6.5985e-04 - val_loss: 7.4770e-04\n",
      "Epoch 270/500\n",
      "5056/5056 [==============================] - 0s 98us/step - loss: 6.5968e-04 - val_loss: 7.4704e-04\n",
      "Epoch 271/500\n",
      "5056/5056 [==============================] - 1s 109us/step - loss: 6.5924e-04 - val_loss: 7.4688e-04\n",
      "Epoch 272/500\n",
      "5056/5056 [==============================] - 1s 112us/step - loss: 6.5897e-04 - val_loss: 7.4724e-04\n",
      "Epoch 273/500\n",
      "5056/5056 [==============================] - 0s 88us/step - loss: 6.5870e-04 - val_loss: 7.4631e-04\n",
      "Epoch 274/500\n",
      "5056/5056 [==============================] - 0s 95us/step - loss: 6.5834e-04 - val_loss: 7.4594e-04\n",
      "Epoch 275/500\n",
      "5056/5056 [==============================] - 0s 95us/step - loss: 6.5799e-04 - val_loss: 7.4615e-04\n",
      "Epoch 276/500\n",
      "5056/5056 [==============================] - 0s 93us/step - loss: 6.5775e-04 - val_loss: 7.4491e-04\n",
      "Epoch 277/500\n",
      "5056/5056 [==============================] - 0s 94us/step - loss: 6.5736e-04 - val_loss: 7.4518e-04\n",
      "Epoch 278/500\n",
      "5056/5056 [==============================] - 0s 96us/step - loss: 6.5708e-04 - val_loss: 7.4478e-04\n",
      "Epoch 279/500\n",
      "5056/5056 [==============================] - 0s 91us/step - loss: 6.5685e-04 - val_loss: 7.4438e-04\n",
      "Epoch 280/500\n",
      "5056/5056 [==============================] - 0s 91us/step - loss: 6.5650e-04 - val_loss: 7.4406e-04\n",
      "Epoch 281/500\n",
      "5056/5056 [==============================] - 0s 95us/step - loss: 6.5622e-04 - val_loss: 7.4405e-04\n",
      "Epoch 282/500\n",
      "5056/5056 [==============================] - 0s 93us/step - loss: 6.5605e-04 - val_loss: 7.4290e-04\n",
      "Epoch 283/500\n",
      "5056/5056 [==============================] - 0s 91us/step - loss: 6.5571e-04 - val_loss: 7.4322e-04\n",
      "Epoch 284/500\n",
      "5056/5056 [==============================] - 0s 88us/step - loss: 6.5544e-04 - val_loss: 7.4239e-04\n",
      "Epoch 285/500\n",
      "5056/5056 [==============================] - 0s 92us/step - loss: 6.5513e-04 - val_loss: 7.4253e-04\n",
      "Epoch 286/500\n",
      "5056/5056 [==============================] - 0s 86us/step - loss: 6.5494e-04 - val_loss: 7.4172e-04\n",
      "Epoch 287/500\n",
      "5056/5056 [==============================] - 1s 106us/step - loss: 6.5465e-04 - val_loss: 7.4186e-04\n",
      "Epoch 288/500\n",
      "5056/5056 [==============================] - 1s 100us/step - loss: 6.5436e-04 - val_loss: 7.4146e-04\n",
      "Epoch 289/500\n",
      "5056/5056 [==============================] - 0s 84us/step - loss: 6.5407e-04 - val_loss: 7.4116e-04\n",
      "Epoch 290/500\n",
      "5056/5056 [==============================] - 1s 100us/step - loss: 6.5381e-04 - val_loss: 7.4145e-04\n",
      "Epoch 291/500\n",
      "5056/5056 [==============================] - 0s 85us/step - loss: 6.5358e-04 - val_loss: 7.4059e-04\n",
      "Epoch 292/500\n",
      "5056/5056 [==============================] - 0s 96us/step - loss: 6.5330e-04 - val_loss: 7.4065e-04\n",
      "Epoch 293/500\n",
      "5056/5056 [==============================] - 0s 97us/step - loss: 6.5307e-04 - val_loss: 7.4047e-04\n",
      "Epoch 294/500\n",
      "5056/5056 [==============================] - 1s 111us/step - loss: 6.5284e-04 - val_loss: 7.3993e-04\n",
      "Epoch 295/500\n",
      "5056/5056 [==============================] - 1s 107us/step - loss: 6.5259e-04 - val_loss: 7.4014e-04\n",
      "Epoch 296/500\n",
      "5056/5056 [==============================] - 0s 96us/step - loss: 6.5237e-04 - val_loss: 7.3876e-04\n",
      "Epoch 297/500\n",
      "5056/5056 [==============================] - 0s 92us/step - loss: 6.5207e-04 - val_loss: 7.3906e-04\n",
      "Epoch 298/500\n",
      "5056/5056 [==============================] - 1s 102us/step - loss: 6.5185e-04 - val_loss: 7.3941e-04\n",
      "Epoch 299/500\n",
      "5056/5056 [==============================] - 0s 95us/step - loss: 6.5161e-04 - val_loss: 7.3878e-04\n",
      "Epoch 300/500\n",
      "5056/5056 [==============================] - 0s 89us/step - loss: 6.5151e-04 - val_loss: 7.3836e-04\n",
      "Epoch 301/500\n",
      "5056/5056 [==============================] - 0s 93us/step - loss: 6.5110e-04 - val_loss: 7.3866e-04\n",
      "Epoch 302/500\n",
      "5056/5056 [==============================] - 0s 83us/step - loss: 6.5095e-04 - val_loss: 7.3822e-04\n",
      "Epoch 303/500\n",
      "5056/5056 [==============================] - 1s 127us/step - loss: 6.5071e-04 - val_loss: 7.3841e-04\n",
      "Epoch 304/500\n",
      "5056/5056 [==============================] - 1s 204us/step - loss: 6.5050e-04 - val_loss: 7.3752e-04\n",
      "Epoch 305/500\n",
      "5056/5056 [==============================] - 1s 126us/step - loss: 6.5027e-04 - val_loss: 7.3757e-04\n",
      "Epoch 306/500\n",
      "5056/5056 [==============================] - 1s 116us/step - loss: 6.5013e-04 - val_loss: 7.3705e-04\n",
      "Epoch 307/500\n",
      "5056/5056 [==============================] - 1s 144us/step - loss: 6.4986e-04 - val_loss: 7.3727e-04\n",
      "Epoch 308/500\n",
      "5056/5056 [==============================] - 1s 152us/step - loss: 6.4962e-04 - val_loss: 7.3654e-04\n",
      "Epoch 309/500\n",
      "5056/5056 [==============================] - 1s 148us/step - loss: 6.4944e-04 - val_loss: 7.3622e-04\n",
      "Epoch 310/500\n",
      "5056/5056 [==============================] - 1s 148us/step - loss: 6.4919e-04 - val_loss: 7.3598e-04\n",
      "Epoch 311/500\n",
      "5056/5056 [==============================] - 1s 128us/step - loss: 6.4901e-04 - val_loss: 7.3609e-04\n",
      "Epoch 312/500\n",
      "5056/5056 [==============================] - 1s 155us/step - loss: 6.4881e-04 - val_loss: 7.3585e-04\n",
      "Epoch 313/500\n",
      "5056/5056 [==============================] - 1s 148us/step - loss: 6.4858e-04 - val_loss: 7.3542e-04\n",
      "Epoch 314/500\n",
      "5056/5056 [==============================] - 1s 148us/step - loss: 6.4840e-04 - val_loss: 7.3573e-04\n",
      "Epoch 315/500\n",
      "5056/5056 [==============================] - 1s 127us/step - loss: 6.4820e-04 - val_loss: 7.3467e-04\n",
      "Epoch 316/500\n",
      "5056/5056 [==============================] - 1s 156us/step - loss: 6.4816e-04 - val_loss: 7.3461e-04\n",
      "Epoch 317/500\n",
      "5056/5056 [==============================] - 1s 141us/step - loss: 6.4778e-04 - val_loss: 7.3448e-04\n",
      "Epoch 318/500\n",
      "5056/5056 [==============================] - 1s 127us/step - loss: 6.4766e-04 - val_loss: 7.3466e-04\n",
      "Epoch 319/500\n",
      "5056/5056 [==============================] - 1s 130us/step - loss: 6.4750e-04 - val_loss: 7.3402e-04\n",
      "Epoch 320/500\n",
      "5056/5056 [==============================] - 1s 136us/step - loss: 6.4732e-04 - val_loss: 7.3377e-04\n",
      "Epoch 321/500\n",
      "5056/5056 [==============================] - 1s 132us/step - loss: 6.4713e-04 - val_loss: 7.3336e-04\n",
      "Epoch 322/500\n",
      "5056/5056 [==============================] - 1s 132us/step - loss: 6.4686e-04 - val_loss: 7.3418e-04\n",
      "Epoch 323/500\n",
      "5056/5056 [==============================] - 1s 145us/step - loss: 6.4682e-04 - val_loss: 7.3315e-04\n",
      "Epoch 324/500\n",
      "5056/5056 [==============================] - 1s 135us/step - loss: 6.4656e-04 - val_loss: 7.3291e-04\n",
      "Epoch 325/500\n",
      "5056/5056 [==============================] - 1s 116us/step - loss: 6.4630e-04 - val_loss: 7.3312e-04\n",
      "Epoch 326/500\n",
      "5056/5056 [==============================] - 1s 156us/step - loss: 6.4621e-04 - val_loss: 7.3290e-04\n",
      "Epoch 327/500\n",
      "5056/5056 [==============================] - 1s 141us/step - loss: 6.4600e-04 - val_loss: 7.3257e-04\n",
      "Epoch 328/500\n",
      "5056/5056 [==============================] - 1s 104us/step - loss: 6.4579e-04 - val_loss: 7.3285e-04\n",
      "Epoch 329/500\n",
      "5056/5056 [==============================] - 1s 110us/step - loss: 6.4566e-04 - val_loss: 7.3229e-04\n",
      "Epoch 330/500\n",
      "5056/5056 [==============================] - 1s 102us/step - loss: 6.4550e-04 - val_loss: 7.3193e-04\n",
      "Epoch 331/500\n",
      "5056/5056 [==============================] - 1s 104us/step - loss: 6.4537e-04 - val_loss: 7.3158e-04\n",
      "Epoch 332/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5056/5056 [==============================] - 0s 86us/step - loss: 6.4514e-04 - val_loss: 7.3178e-04\n",
      "Epoch 333/500\n",
      "5056/5056 [==============================] - 0s 90us/step - loss: 6.4498e-04 - val_loss: 7.3117e-04\n",
      "Epoch 334/500\n",
      "5056/5056 [==============================] - 0s 86us/step - loss: 6.4478e-04 - val_loss: 7.3130e-04\n",
      "Epoch 335/500\n",
      "5056/5056 [==============================] - 1s 109us/step - loss: 6.4480e-04 - val_loss: 7.3076e-04\n",
      "Epoch 336/500\n",
      "5056/5056 [==============================] - 0s 89us/step - loss: 6.4451e-04 - val_loss: 7.3046e-04\n",
      "Epoch 337/500\n",
      "5056/5056 [==============================] - 0s 94us/step - loss: 6.4428e-04 - val_loss: 7.3101e-04\n",
      "Epoch 338/500\n",
      "5056/5056 [==============================] - 0s 86us/step - loss: 6.4425e-04 - val_loss: 7.3064e-04\n",
      "Epoch 339/500\n",
      "5056/5056 [==============================] - 0s 87us/step - loss: 6.4414e-04 - val_loss: 7.3083e-04\n",
      "Epoch 340/500\n",
      "5056/5056 [==============================] - 0s 92us/step - loss: 6.4393e-04 - val_loss: 7.3009e-04\n",
      "Epoch 341/500\n",
      "5056/5056 [==============================] - 0s 85us/step - loss: 6.4374e-04 - val_loss: 7.2989e-04\n",
      "Epoch 342/500\n",
      "5056/5056 [==============================] - 0s 87us/step - loss: 6.4363e-04 - val_loss: 7.2986e-04\n",
      "Epoch 343/500\n",
      "5056/5056 [==============================] - 0s 90us/step - loss: 6.4337e-04 - val_loss: 7.3030e-04\n",
      "Epoch 344/500\n",
      "5056/5056 [==============================] - 0s 89us/step - loss: 6.4323e-04 - val_loss: 7.2948e-04\n",
      "Epoch 345/500\n",
      "5056/5056 [==============================] - 0s 90us/step - loss: 6.4308e-04 - val_loss: 7.3066e-04\n",
      "Epoch 346/500\n",
      "5056/5056 [==============================] - 0s 92us/step - loss: 6.4305e-04 - val_loss: 7.2922e-04\n",
      "Epoch 347/500\n",
      "5056/5056 [==============================] - 0s 87us/step - loss: 6.4281e-04 - val_loss: 7.2954e-04\n",
      "Epoch 348/500\n",
      "5056/5056 [==============================] - 0s 90us/step - loss: 6.4257e-04 - val_loss: 7.2902e-04\n",
      "Epoch 349/500\n",
      "5056/5056 [==============================] - 0s 96us/step - loss: 6.4246e-04 - val_loss: 7.2833e-04\n",
      "Epoch 350/500\n",
      "5056/5056 [==============================] - 1s 101us/step - loss: 6.4238e-04 - val_loss: 7.2898e-04\n",
      "Epoch 351/500\n",
      "5056/5056 [==============================] - 0s 90us/step - loss: 6.4216e-04 - val_loss: 7.2874e-04\n",
      "Epoch 352/500\n",
      "5056/5056 [==============================] - 0s 94us/step - loss: 6.4206e-04 - val_loss: 7.2850e-04\n",
      "Epoch 353/500\n",
      "5056/5056 [==============================] - 0s 93us/step - loss: 6.4198e-04 - val_loss: 7.2829e-04\n",
      "Epoch 354/500\n",
      "5056/5056 [==============================] - 0s 92us/step - loss: 6.4189e-04 - val_loss: 7.2836e-04\n",
      "Epoch 355/500\n",
      "5056/5056 [==============================] - 0s 91us/step - loss: 6.4173e-04 - val_loss: 7.2793e-04\n",
      "Epoch 356/500\n",
      "5056/5056 [==============================] - 0s 90us/step - loss: 6.4143e-04 - val_loss: 7.2831e-04\n",
      "Epoch 357/500\n",
      "5056/5056 [==============================] - 0s 94us/step - loss: 6.4136e-04 - val_loss: 7.2751e-04\n",
      "Epoch 358/500\n",
      "5056/5056 [==============================] - 0s 88us/step - loss: 6.4128e-04 - val_loss: 7.2749e-04\n",
      "Epoch 359/500\n",
      "5056/5056 [==============================] - 0s 90us/step - loss: 6.4109e-04 - val_loss: 7.2715e-04\n",
      "Epoch 360/500\n",
      "5056/5056 [==============================] - 0s 88us/step - loss: 6.4098e-04 - val_loss: 7.2726e-04\n",
      "Epoch 361/500\n",
      "5056/5056 [==============================] - 0s 91us/step - loss: 6.4080e-04 - val_loss: 7.2702e-04\n",
      "Epoch 362/500\n",
      "5056/5056 [==============================] - 0s 87us/step - loss: 6.4070e-04 - val_loss: 7.2677e-04\n",
      "Epoch 363/500\n",
      "5056/5056 [==============================] - 0s 90us/step - loss: 6.4063e-04 - val_loss: 7.2666e-04\n",
      "Epoch 364/500\n",
      "5056/5056 [==============================] - 0s 93us/step - loss: 6.4052e-04 - val_loss: 7.2699e-04\n",
      "Epoch 365/500\n",
      "5056/5056 [==============================] - 0s 92us/step - loss: 6.4038e-04 - val_loss: 7.2628e-04\n",
      "Epoch 366/500\n",
      "5056/5056 [==============================] - 0s 92us/step - loss: 6.4027e-04 - val_loss: 7.2597e-04\n",
      "Epoch 367/500\n",
      "5056/5056 [==============================] - 0s 90us/step - loss: 6.4022e-04 - val_loss: 7.2612e-04\n",
      "Epoch 368/500\n",
      "5056/5056 [==============================] - 0s 92us/step - loss: 6.3997e-04 - val_loss: 7.2645e-04\n",
      "Epoch 369/500\n",
      "5056/5056 [==============================] - 0s 92us/step - loss: 6.3989e-04 - val_loss: 7.2583e-04\n",
      "Epoch 370/500\n",
      "5056/5056 [==============================] - 1s 101us/step - loss: 6.3972e-04 - val_loss: 7.2554e-04\n",
      "Epoch 371/500\n",
      "5056/5056 [==============================] - 1s 107us/step - loss: 6.3954e-04 - val_loss: 7.2539e-04\n",
      "Epoch 372/500\n",
      "5056/5056 [==============================] - 1s 104us/step - loss: 6.3950e-04 - val_loss: 7.2539e-04\n",
      "Epoch 373/500\n",
      "5056/5056 [==============================] - 0s 86us/step - loss: 6.3932e-04 - val_loss: 7.2515e-04\n",
      "Epoch 374/500\n",
      "5056/5056 [==============================] - 0s 93us/step - loss: 6.3927e-04 - val_loss: 7.2494e-04\n",
      "Epoch 375/500\n",
      "5056/5056 [==============================] - 0s 98us/step - loss: 6.3918e-04 - val_loss: 7.2470e-04\n",
      "Epoch 376/500\n",
      "5056/5056 [==============================] - 0s 94us/step - loss: 6.3901e-04 - val_loss: 7.2471e-04\n",
      "Epoch 377/500\n",
      "5056/5056 [==============================] - 0s 87us/step - loss: 6.3895e-04 - val_loss: 7.2457e-04\n",
      "Epoch 378/500\n",
      "5056/5056 [==============================] - 0s 94us/step - loss: 6.3878e-04 - val_loss: 7.2440e-04\n",
      "Epoch 379/500\n",
      "5056/5056 [==============================] - 0s 91us/step - loss: 6.3868e-04 - val_loss: 7.2468e-04\n",
      "Epoch 380/500\n",
      "5056/5056 [==============================] - 0s 98us/step - loss: 6.3852e-04 - val_loss: 7.2395e-04\n",
      "Epoch 381/500\n",
      "5056/5056 [==============================] - 0s 92us/step - loss: 6.3851e-04 - val_loss: 7.2413e-04\n",
      "Epoch 382/500\n",
      "5056/5056 [==============================] - 0s 93us/step - loss: 6.3834e-04 - val_loss: 7.2420e-04\n",
      "Epoch 383/500\n",
      "5056/5056 [==============================] - 0s 88us/step - loss: 6.3821e-04 - val_loss: 7.2428e-04\n",
      "Epoch 384/500\n",
      "5056/5056 [==============================] - 0s 95us/step - loss: 6.3820e-04 - val_loss: 7.2366e-04\n",
      "Epoch 385/500\n",
      "5056/5056 [==============================] - 0s 95us/step - loss: 6.3806e-04 - val_loss: 7.2358e-04\n",
      "Epoch 386/500\n",
      "5056/5056 [==============================] - 0s 90us/step - loss: 6.3790e-04 - val_loss: 7.2359e-04\n",
      "Epoch 387/500\n",
      "5056/5056 [==============================] - 0s 94us/step - loss: 6.3787e-04 - val_loss: 7.2347e-04\n",
      "Epoch 388/500\n",
      "5056/5056 [==============================] - 0s 91us/step - loss: 6.3768e-04 - val_loss: 7.2331e-04\n",
      "Epoch 389/500\n",
      "5056/5056 [==============================] - 0s 95us/step - loss: 6.3759e-04 - val_loss: 7.2354e-04\n",
      "Epoch 390/500\n",
      "5056/5056 [==============================] - 0s 97us/step - loss: 6.3747e-04 - val_loss: 7.2300e-04\n",
      "Epoch 391/500\n",
      "5056/5056 [==============================] - 1s 106us/step - loss: 6.3727e-04 - val_loss: 7.2356e-04\n",
      "Epoch 392/500\n",
      "5056/5056 [==============================] - 1s 99us/step - loss: 6.3735e-04 - val_loss: 7.2277e-04\n",
      "Epoch 393/500\n",
      "5056/5056 [==============================] - 1s 118us/step - loss: 6.3727e-04 - val_loss: 7.2291e-04\n",
      "Epoch 394/500\n",
      "5056/5056 [==============================] - 1s 142us/step - loss: 6.3708e-04 - val_loss: 7.2266e-04\n",
      "Epoch 395/500\n",
      "5056/5056 [==============================] - 1s 113us/step - loss: 6.3693e-04 - val_loss: 7.2279e-04\n",
      "Epoch 396/500\n",
      "5056/5056 [==============================] - 1s 112us/step - loss: 6.3685e-04 - val_loss: 7.2234e-04\n",
      "Epoch 397/500\n",
      "5056/5056 [==============================] - 1s 127us/step - loss: 6.3666e-04 - val_loss: 7.2221e-04\n",
      "Epoch 398/500\n",
      "5056/5056 [==============================] - 1s 118us/step - loss: 6.3675e-04 - val_loss: 7.2294e-04\n",
      "Epoch 399/500\n",
      "5056/5056 [==============================] - 1s 123us/step - loss: 6.3659e-04 - val_loss: 7.2243e-04\n",
      "Epoch 400/500\n",
      "5056/5056 [==============================] - 1s 107us/step - loss: 6.3652e-04 - val_loss: 7.2225e-04\n",
      "Epoch 401/500\n",
      "5056/5056 [==============================] - 1s 107us/step - loss: 6.3638e-04 - val_loss: 7.2167e-04\n",
      "Epoch 402/500\n",
      "5056/5056 [==============================] - 1s 109us/step - loss: 6.3628e-04 - val_loss: 7.2207e-04\n",
      "Epoch 403/500\n",
      "5056/5056 [==============================] - 1s 102us/step - loss: 6.3624e-04 - val_loss: 7.2157e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 404/500\n",
      "5056/5056 [==============================] - 1s 114us/step - loss: 6.3605e-04 - val_loss: 7.2182e-04\n",
      "Epoch 405/500\n",
      "5056/5056 [==============================] - 0s 96us/step - loss: 6.3602e-04 - val_loss: 7.2137e-04\n",
      "Epoch 406/500\n",
      "5056/5056 [==============================] - 1s 102us/step - loss: 6.3587e-04 - val_loss: 7.2216e-04\n",
      "Epoch 407/500\n",
      "5056/5056 [==============================] - 1s 100us/step - loss: 6.3575e-04 - val_loss: 7.2186e-04\n",
      "Epoch 408/500\n",
      "5056/5056 [==============================] - 0s 90us/step - loss: 6.3582e-04 - val_loss: 7.2104e-04\n",
      "Epoch 409/500\n",
      "5056/5056 [==============================] - 0s 90us/step - loss: 6.3562e-04 - val_loss: 7.2070e-04\n",
      "Epoch 410/500\n",
      "5056/5056 [==============================] - 0s 90us/step - loss: 6.3559e-04 - val_loss: 7.2093e-04\n",
      "Epoch 411/500\n",
      "5056/5056 [==============================] - 1s 106us/step - loss: 6.3547e-04 - val_loss: 7.2081e-04\n",
      "Epoch 412/500\n",
      "5056/5056 [==============================] - 1s 101us/step - loss: 6.3525e-04 - val_loss: 7.2138e-04\n",
      "Epoch 413/500\n",
      "5056/5056 [==============================] - 0s 91us/step - loss: 6.3522e-04 - val_loss: 7.2095e-04\n",
      "Epoch 414/500\n",
      "5056/5056 [==============================] - 0s 97us/step - loss: 6.3515e-04 - val_loss: 7.2087e-04\n",
      "Epoch 415/500\n",
      "5056/5056 [==============================] - 0s 93us/step - loss: 6.3503e-04 - val_loss: 7.2036e-04\n",
      "Epoch 416/500\n",
      "5056/5056 [==============================] - 1s 106us/step - loss: 6.3500e-04 - val_loss: 7.2078e-04\n",
      "Epoch 417/500\n",
      "5056/5056 [==============================] - 1s 115us/step - loss: 6.3489e-04 - val_loss: 7.2048e-04\n",
      "Epoch 418/500\n",
      "5056/5056 [==============================] - 1s 120us/step - loss: 6.3481e-04 - val_loss: 7.2079e-04\n",
      "Epoch 419/500\n",
      "5056/5056 [==============================] - 1s 119us/step - loss: 6.3478e-04 - val_loss: 7.2029e-04\n",
      "Epoch 420/500\n",
      "5056/5056 [==============================] - 1s 149us/step - loss: 6.3461e-04 - val_loss: 7.2036e-04\n",
      "Epoch 421/500\n",
      "5056/5056 [==============================] - 1s 109us/step - loss: 6.3460e-04 - val_loss: 7.2013e-04\n",
      "Epoch 422/500\n",
      "5056/5056 [==============================] - 1s 106us/step - loss: 6.3447e-04 - val_loss: 7.1982e-04\n",
      "Epoch 423/500\n",
      "5056/5056 [==============================] - 1s 113us/step - loss: 6.3436e-04 - val_loss: 7.1963e-04\n",
      "Epoch 424/500\n",
      "5056/5056 [==============================] - 1s 140us/step - loss: 6.3431e-04 - val_loss: 7.2000e-04\n",
      "Epoch 425/500\n",
      "5056/5056 [==============================] - 1s 128us/step - loss: 6.3420e-04 - val_loss: 7.1972e-04\n",
      "Epoch 426/500\n",
      "5056/5056 [==============================] - 1s 119us/step - loss: 6.3419e-04 - val_loss: 7.1991e-04\n",
      "Epoch 427/500\n",
      "5056/5056 [==============================] - 1s 128us/step - loss: 6.3401e-04 - val_loss: 7.1911e-04\n",
      "Epoch 428/500\n",
      "5056/5056 [==============================] - 1s 150us/step - loss: 6.3402e-04 - val_loss: 7.1929e-04\n",
      "Epoch 429/500\n",
      "5056/5056 [==============================] - 1s 130us/step - loss: 6.3385e-04 - val_loss: 7.1931e-04\n",
      "Epoch 430/500\n",
      "5056/5056 [==============================] - 1s 123us/step - loss: 6.3381e-04 - val_loss: 7.1896e-04\n",
      "Epoch 431/500\n",
      "5056/5056 [==============================] - 1s 115us/step - loss: 6.3381e-04 - val_loss: 7.1879e-04\n",
      "Epoch 432/500\n",
      "5056/5056 [==============================] - 1s 115us/step - loss: 6.3362e-04 - val_loss: 7.1877e-04\n",
      "Epoch 433/500\n",
      "5056/5056 [==============================] - 1s 120us/step - loss: 6.3349e-04 - val_loss: 7.1884e-04\n",
      "Epoch 434/500\n",
      "5056/5056 [==============================] - 1s 101us/step - loss: 6.3356e-04 - val_loss: 7.1861e-04\n",
      "Epoch 435/500\n",
      "5056/5056 [==============================] - 1s 110us/step - loss: 6.3349e-04 - val_loss: 7.1836e-04\n",
      "Epoch 436/500\n",
      "5056/5056 [==============================] - 1s 114us/step - loss: 6.3340e-04 - val_loss: 7.1865e-04\n",
      "Epoch 437/500\n",
      "5056/5056 [==============================] - 1s 102us/step - loss: 6.3326e-04 - val_loss: 7.1833e-04\n",
      "Epoch 438/500\n",
      "5056/5056 [==============================] - 0s 98us/step - loss: 6.3314e-04 - val_loss: 7.1894e-04\n",
      "Epoch 439/500\n",
      "5056/5056 [==============================] - 1s 122us/step - loss: 6.3307e-04 - val_loss: 7.1910e-04\n",
      "Epoch 440/500\n",
      "5056/5056 [==============================] - 1s 109us/step - loss: 6.3303e-04 - val_loss: 7.1793e-04\n",
      "Epoch 441/500\n",
      "5056/5056 [==============================] - 1s 109us/step - loss: 6.3287e-04 - val_loss: 7.1824e-04\n",
      "Epoch 442/500\n",
      "5056/5056 [==============================] - 1s 109us/step - loss: 6.3294e-04 - val_loss: 7.1776e-04\n",
      "Epoch 443/500\n",
      "5056/5056 [==============================] - 0s 87us/step - loss: 6.3282e-04 - val_loss: 7.1799e-04\n",
      "Epoch 444/500\n",
      "5056/5056 [==============================] - 0s 91us/step - loss: 6.3273e-04 - val_loss: 7.1799e-04\n",
      "Epoch 445/500\n",
      "5056/5056 [==============================] - 0s 95us/step - loss: 6.3262e-04 - val_loss: 7.1765e-04\n",
      "Epoch 446/500\n",
      "5056/5056 [==============================] - 1s 106us/step - loss: 6.3259e-04 - val_loss: 7.1770e-04\n",
      "Epoch 447/500\n",
      "5056/5056 [==============================] - 1s 102us/step - loss: 6.3256e-04 - val_loss: 7.1748e-04\n",
      "Epoch 448/500\n",
      "5056/5056 [==============================] - 1s 123us/step - loss: 6.3236e-04 - val_loss: 7.1743e-04\n",
      "Epoch 449/500\n",
      "5056/5056 [==============================] - 1s 112us/step - loss: 6.3232e-04 - val_loss: 7.1726e-04\n",
      "Epoch 450/500\n",
      "5056/5056 [==============================] - 0s 94us/step - loss: 6.3231e-04 - val_loss: 7.1739e-04\n",
      "Epoch 451/500\n",
      "5056/5056 [==============================] - 0s 96us/step - loss: 6.3224e-04 - val_loss: 7.1708e-04\n",
      "Epoch 452/500\n",
      "5056/5056 [==============================] - 0s 93us/step - loss: 6.3209e-04 - val_loss: 7.1785e-04\n",
      "Epoch 453/500\n",
      "5056/5056 [==============================] - 0s 96us/step - loss: 6.3214e-04 - val_loss: 7.1705e-04\n",
      "Epoch 454/500\n",
      "5056/5056 [==============================] - 1s 102us/step - loss: 6.3199e-04 - val_loss: 7.1669e-04\n",
      "Epoch 455/500\n",
      "5056/5056 [==============================] - 0s 96us/step - loss: 6.3195e-04 - val_loss: 7.1712e-04\n",
      "Epoch 456/500\n",
      "5056/5056 [==============================] - 0s 94us/step - loss: 6.3191e-04 - val_loss: 7.1662e-04\n",
      "Epoch 457/500\n",
      "5056/5056 [==============================] - 1s 99us/step - loss: 6.3173e-04 - val_loss: 7.1686e-04\n",
      "Epoch 458/500\n",
      "5056/5056 [==============================] - 1s 99us/step - loss: 6.3174e-04 - val_loss: 7.1671e-04\n",
      "Epoch 459/500\n",
      "5056/5056 [==============================] - 1s 101us/step - loss: 6.3172e-04 - val_loss: 7.1630e-04\n",
      "Epoch 460/500\n",
      "5056/5056 [==============================] - 1s 117us/step - loss: 6.3156e-04 - val_loss: 7.1657e-04\n",
      "Epoch 461/500\n",
      "5056/5056 [==============================] - 1s 105us/step - loss: 6.3154e-04 - val_loss: 7.1684e-04\n",
      "Epoch 462/500\n",
      "5056/5056 [==============================] - 1s 99us/step - loss: 6.3152e-04 - val_loss: 7.1731e-04\n",
      "Epoch 463/500\n",
      "5056/5056 [==============================] - 0s 93us/step - loss: 6.3138e-04 - val_loss: 7.1660e-04\n",
      "Epoch 464/500\n",
      "5056/5056 [==============================] - 1s 107us/step - loss: 6.3137e-04 - val_loss: 7.1627e-04\n",
      "Epoch 465/500\n",
      "5056/5056 [==============================] - 1s 104us/step - loss: 6.3129e-04 - val_loss: 7.1611e-04\n",
      "Epoch 466/500\n",
      "5056/5056 [==============================] - 1s 111us/step - loss: 6.3113e-04 - val_loss: 7.1610e-04\n",
      "Epoch 467/500\n",
      "5056/5056 [==============================] - 1s 106us/step - loss: 6.3114e-04 - val_loss: 7.1584e-04\n",
      "Epoch 468/500\n",
      "5056/5056 [==============================] - 1s 110us/step - loss: 6.3110e-04 - val_loss: 7.1606e-04\n",
      "Epoch 469/500\n",
      "5056/5056 [==============================] - 1s 137us/step - loss: 6.3102e-04 - val_loss: 7.1571e-04\n",
      "Epoch 470/500\n",
      "5056/5056 [==============================] - 1s 119us/step - loss: 6.3094e-04 - val_loss: 7.1629e-04\n",
      "Epoch 471/500\n",
      "5056/5056 [==============================] - 1s 111us/step - loss: 6.3081e-04 - val_loss: 7.1584e-04\n",
      "Epoch 472/500\n",
      "5056/5056 [==============================] - 1s 115us/step - loss: 6.3080e-04 - val_loss: 7.1566e-04\n",
      "Epoch 473/500\n",
      "5056/5056 [==============================] - 1s 105us/step - loss: 6.3078e-04 - val_loss: 7.1553e-04\n",
      "Epoch 474/500\n",
      "5056/5056 [==============================] - 1s 105us/step - loss: 6.3064e-04 - val_loss: 7.1550e-04\n",
      "Epoch 475/500\n",
      "5056/5056 [==============================] - 1s 101us/step - loss: 6.3073e-04 - val_loss: 7.1556e-04\n",
      "Epoch 476/500\n",
      "5056/5056 [==============================] - 1s 100us/step - loss: 6.3053e-04 - val_loss: 7.1556e-04\n",
      "Epoch 477/500\n",
      "5056/5056 [==============================] - 1s 119us/step - loss: 6.3057e-04 - val_loss: 7.1544e-04\n",
      "Epoch 478/500\n",
      "5056/5056 [==============================] - 1s 106us/step - loss: 6.3040e-04 - val_loss: 7.1566e-04\n",
      "Epoch 479/500\n",
      "5056/5056 [==============================] - 1s 108us/step - loss: 6.3042e-04 - val_loss: 7.1507e-04\n",
      "Epoch 480/500\n",
      "5056/5056 [==============================] - 0s 95us/step - loss: 6.3033e-04 - val_loss: 7.1518e-04\n",
      "Epoch 481/500\n",
      "5056/5056 [==============================] - 1s 106us/step - loss: 6.3026e-04 - val_loss: 7.1463e-04\n",
      "Epoch 482/500\n",
      "5056/5056 [==============================] - 0s 85us/step - loss: 6.3021e-04 - val_loss: 7.1498e-04\n",
      "Epoch 483/500\n",
      "5056/5056 [==============================] - 0s 92us/step - loss: 6.3010e-04 - val_loss: 7.1500e-04\n",
      "Epoch 484/500\n",
      "5056/5056 [==============================] - 0s 87us/step - loss: 6.3003e-04 - val_loss: 7.1515e-04\n",
      "Epoch 485/500\n",
      "5056/5056 [==============================] - 0s 95us/step - loss: 6.3004e-04 - val_loss: 7.1503e-04\n",
      "Epoch 486/500\n",
      "5056/5056 [==============================] - 0s 92us/step - loss: 6.3004e-04 - val_loss: 7.1465e-04\n",
      "Epoch 487/500\n",
      "5056/5056 [==============================] - 1s 121us/step - loss: 6.2992e-04 - val_loss: 7.1461e-04\n",
      "Epoch 488/500\n",
      "5056/5056 [==============================] - 1s 112us/step - loss: 6.2978e-04 - val_loss: 7.1434e-04\n",
      "Epoch 489/500\n",
      "5056/5056 [==============================] - 1s 130us/step - loss: 6.2976e-04 - val_loss: 7.1438e-04\n",
      "Epoch 490/500\n",
      "5056/5056 [==============================] - 1s 114us/step - loss: 6.2969e-04 - val_loss: 7.1468e-04\n",
      "Epoch 491/500\n",
      "5056/5056 [==============================] - 1s 130us/step - loss: 6.2974e-04 - val_loss: 7.1430e-04\n",
      "Epoch 492/500\n",
      "5056/5056 [==============================] - 1s 115us/step - loss: 6.2960e-04 - val_loss: 7.1438e-04\n",
      "Epoch 493/500\n",
      "5056/5056 [==============================] - 1s 140us/step - loss: 6.2951e-04 - val_loss: 7.1402e-04\n",
      "Epoch 494/500\n",
      "5056/5056 [==============================] - 1s 114us/step - loss: 6.2944e-04 - val_loss: 7.1413e-04\n",
      "Epoch 495/500\n",
      "5056/5056 [==============================] - 1s 102us/step - loss: 6.2949e-04 - val_loss: 7.1419e-04\n",
      "Epoch 496/500\n",
      "5056/5056 [==============================] - 1s 150us/step - loss: 6.2944e-04 - val_loss: 7.1384e-04\n",
      "Epoch 497/500\n",
      "5056/5056 [==============================] - 1s 128us/step - loss: 6.2935e-04 - val_loss: 7.1414e-04\n",
      "Epoch 498/500\n",
      "5056/5056 [==============================] - 1s 142us/step - loss: 6.2925e-04 - val_loss: 7.1391e-04\n",
      "Epoch 499/500\n",
      "5056/5056 [==============================] - 1s 135us/step - loss: 6.2931e-04 - val_loss: 7.1382e-04\n",
      "Epoch 500/500\n",
      "5056/5056 [==============================] - 1s 118us/step - loss: 6.2907e-04 - val_loss: 7.1400e-04\n",
      "Test MSE: 0.00069436767\n",
      "Total MSE: 0.00065414003\n",
      "c.jpg\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 75)                22575     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 300)               22800     \n",
      "=================================================================\n",
      "Total params: 45,375\n",
      "Trainable params: 45,375\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 23592 samples, validate on 5899 samples\n",
      "Epoch 1/500\n",
      "23592/23592 [==============================] - 2s 90us/step - loss: 0.0021 - val_loss: 0.0011\n",
      "Epoch 2/500\n",
      "23592/23592 [==============================] - 2s 79us/step - loss: 9.2268e-04 - val_loss: 8.1696e-04\n",
      "Epoch 3/500\n",
      "23592/23592 [==============================] - 2s 66us/step - loss: 7.5463e-04 - val_loss: 6.9995e-04\n",
      "Epoch 4/500\n",
      "23592/23592 [==============================] - 2s 68us/step - loss: 6.5944e-04 - val_loss: 6.3601e-04\n",
      "Epoch 5/500\n",
      "23592/23592 [==============================] - 2s 85us/step - loss: 5.9634e-04 - val_loss: 5.7010e-04\n",
      "Epoch 6/500\n",
      "23592/23592 [==============================] - 2s 70us/step - loss: 5.4994e-04 - val_loss: 5.3140e-04\n",
      "Epoch 7/500\n",
      "23592/23592 [==============================] - 2s 66us/step - loss: 5.1305e-04 - val_loss: 4.9613e-04\n",
      "Test MSE: 0.00052835097\n",
      "Total MSE: 0.00050366826\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input-layer (InputLayer)     (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "hidden-layer-1 (Dense)       (None, 150)               45150     \n",
      "_________________________________________________________________\n",
      "hidden-layer-2 (Dense)       (None, 75)                11325     \n",
      "_________________________________________________________________\n",
      "hidden-layer-3 (Dense)       (None, 150)               11400     \n",
      "_________________________________________________________________\n",
      "output-layer (Dense)         (None, 300)               45300     \n",
      "=================================================================\n",
      "Total params: 113,175\n",
      "Trainable params: 113,175\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 23592 samples, validate on 5899 samples\n",
      "Epoch 1/500\n",
      "23592/23592 [==============================] - 3s 146us/step - loss: 0.0145 - val_loss: 0.0016\n",
      "Epoch 2/500\n",
      "23592/23592 [==============================] - 2s 85us/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 3/500\n",
      "23592/23592 [==============================] - 2s 88us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 4/500\n",
      "23592/23592 [==============================] - 3s 134us/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 5/500\n",
      "23592/23592 [==============================] - 2s 98us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 6/500\n",
      "23592/23592 [==============================] - 2s 88us/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 7/500\n",
      "23592/23592 [==============================] - 2s 88us/step - loss: 0.0010 - val_loss: 9.6418e-04\n",
      "Epoch 8/500\n",
      "23592/23592 [==============================] - 2s 91us/step - loss: 9.5845e-04 - val_loss: 9.2470e-04\n",
      "Epoch 9/500\n",
      "23592/23592 [==============================] - 2s 82us/step - loss: 9.1834e-04 - val_loss: 8.8585e-04\n",
      "Epoch 10/500\n",
      "23592/23592 [==============================] - 2s 85us/step - loss: 8.8139e-04 - val_loss: 8.5112e-04\n",
      "Epoch 11/500\n",
      "23592/23592 [==============================] - 2s 82us/step - loss: 8.4726e-04 - val_loss: 8.1914e-04\n",
      "Epoch 12/500\n",
      "23592/23592 [==============================] - 2s 93us/step - loss: 8.1622e-04 - val_loss: 8.0684e-04\n",
      "Epoch 13/500\n",
      "23592/23592 [==============================] - 2s 88us/step - loss: 7.8805e-04 - val_loss: 7.6445e-04\n",
      "Epoch 14/500\n",
      "23592/23592 [==============================] - 2s 90us/step - loss: 7.6248e-04 - val_loss: 7.4099e-04\n",
      "Epoch 15/500\n",
      "23592/23592 [==============================] - 2s 86us/step - loss: 7.3940e-04 - val_loss: 7.2469e-04\n",
      "Epoch 16/500\n",
      "23592/23592 [==============================] - 2s 86us/step - loss: 7.1879e-04 - val_loss: 7.0056e-04\n",
      "Epoch 17/500\n",
      "23592/23592 [==============================] - 2s 83us/step - loss: 6.9997e-04 - val_loss: 6.8320e-04\n",
      "Epoch 18/500\n",
      "23592/23592 [==============================] - 2s 84us/step - loss: 6.8300e-04 - val_loss: 6.8693e-04\n",
      "Epoch 19/500\n",
      "23592/23592 [==============================] - 2s 95us/step - loss: 6.6762e-04 - val_loss: 6.5191e-04\n",
      "Epoch 20/500\n",
      "23592/23592 [==============================] - 2s 85us/step - loss: 6.5353e-04 - val_loss: 6.4073e-04\n",
      "Epoch 21/500\n",
      "23592/23592 [==============================] - 2s 85us/step - loss: 6.4074e-04 - val_loss: 6.4134e-04\n",
      "Epoch 22/500\n",
      "23592/23592 [==============================] - 2s 85us/step - loss: 6.2875e-04 - val_loss: 6.1691e-04\n",
      "Epoch 23/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23592/23592 [==============================] - 2s 76us/step - loss: 6.1774e-04 - val_loss: 6.0726e-04\n",
      "Epoch 24/500\n",
      "23592/23592 [==============================] - 2s 84us/step - loss: 6.0751e-04 - val_loss: 5.9725e-04\n",
      "Epoch 25/500\n",
      "23592/23592 [==============================] - 2s 79us/step - loss: 5.9783e-04 - val_loss: 5.8721e-04\n",
      "Epoch 26/500\n",
      "23592/23592 [==============================] - 2s 83us/step - loss: 5.8888e-04 - val_loss: 5.7837e-04\n",
      "Epoch 27/500\n",
      "23592/23592 [==============================] - 2s 81us/step - loss: 5.8033e-04 - val_loss: 5.7259e-04\n",
      "Epoch 28/500\n",
      "23592/23592 [==============================] - 3s 121us/step - loss: 5.7224e-04 - val_loss: 5.6342e-04\n",
      "Epoch 29/500\n",
      "23592/23592 [==============================] - 2s 91us/step - loss: 5.6462e-04 - val_loss: 5.5480e-04\n",
      "Epoch 30/500\n",
      "23592/23592 [==============================] - 2s 104us/step - loss: 5.5726e-04 - val_loss: 5.4742e-04\n",
      "Epoch 31/500\n",
      "23592/23592 [==============================] - 3s 141us/step - loss: 5.5028e-04 - val_loss: 5.4580e-04\n",
      "Epoch 32/500\n",
      "23592/23592 [==============================] - 4s 152us/step - loss: 5.4359e-04 - val_loss: 5.3577e-04\n",
      "Epoch 33/500\n",
      "23592/23592 [==============================] - 5s 231us/step - loss: 5.3717e-04 - val_loss: 5.2903e-04\n",
      "Epoch 34/500\n",
      "23592/23592 [==============================] - 4s 168us/step - loss: 5.3100e-04 - val_loss: 5.2197e-04\n",
      "Epoch 35/500\n",
      "23592/23592 [==============================] - 3s 116us/step - loss: 5.2507e-04 - val_loss: 5.2551e-04\n",
      "Epoch 36/500\n",
      "23592/23592 [==============================] - 4s 158us/step - loss: 5.1935e-04 - val_loss: 5.1076e-04\n",
      "Epoch 37/500\n",
      "23592/23592 [==============================] - 3s 112us/step - loss: 5.1378e-04 - val_loss: 5.0506e-04\n",
      "Epoch 38/500\n",
      "23592/23592 [==============================] - 6s 258us/step - loss: 5.0845e-04 - val_loss: 4.9983e-04\n",
      "Test MSE: 0.00051227102\n",
      "Total MSE: 0.00050601681\n",
      "d.jpg\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 75)                22575     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 300)               22800     \n",
      "=================================================================\n",
      "Total params: 45,375\n",
      "Trainable params: 45,375\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 15219 samples, validate on 3805 samples\n",
      "Epoch 1/500\n",
      "15219/15219 [==============================] - 2s 132us/step - loss: 0.0056 - val_loss: 0.0025\n",
      "Epoch 2/500\n",
      "15219/15219 [==============================] - 2s 107us/step - loss: 0.0022 - val_loss: 0.0020\n",
      "Epoch 3/500\n",
      "15219/15219 [==============================] - 2s 109us/step - loss: 0.0019 - val_loss: 0.0017\n",
      "Epoch 4/500\n",
      "15219/15219 [==============================] - 2s 105us/step - loss: 0.0016 - val_loss: 0.0015\n",
      "Epoch 5/500\n",
      "15219/15219 [==============================] - 1s 89us/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 6/500\n",
      "15219/15219 [==============================] - 1s 65us/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 7/500\n",
      "15219/15219 [==============================] - 1s 88us/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 8/500\n",
      "15219/15219 [==============================] - 2s 120us/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 9/500\n",
      "15219/15219 [==============================] - 2s 114us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 10/500\n",
      "15219/15219 [==============================] - 1s 97us/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 11/500\n",
      "15219/15219 [==============================] - 1s 72us/step - loss: 0.0010 - val_loss: 9.8343e-04\n",
      "Epoch 12/500\n",
      "15219/15219 [==============================] - 1s 76us/step - loss: 9.5948e-04 - val_loss: 9.4191e-04\n",
      "Epoch 13/500\n",
      "15219/15219 [==============================] - 1s 90us/step - loss: 9.2120e-04 - val_loss: 9.0261e-04\n",
      "Epoch 14/500\n",
      "15219/15219 [==============================] - 2s 100us/step - loss: 8.8694e-04 - val_loss: 8.7314e-04\n",
      "Epoch 15/500\n",
      "15219/15219 [==============================] - 1s 95us/step - loss: 8.5584e-04 - val_loss: 8.4570e-04\n",
      "Epoch 16/500\n",
      "15219/15219 [==============================] - 2s 114us/step - loss: 8.2767e-04 - val_loss: 8.1931e-04\n",
      "Epoch 17/500\n",
      "15219/15219 [==============================] - 1s 75us/step - loss: 8.0181e-04 - val_loss: 7.9503e-04\n",
      "Epoch 18/500\n",
      "15219/15219 [==============================] - 1s 73us/step - loss: 7.7796e-04 - val_loss: 7.7009e-04\n",
      "Epoch 19/500\n",
      "15219/15219 [==============================] - 1s 69us/step - loss: 7.5593e-04 - val_loss: 7.4789e-04\n",
      "Epoch 20/500\n",
      "15219/15219 [==============================] - 1s 76us/step - loss: 7.3555e-04 - val_loss: 7.2865e-04\n",
      "Epoch 21/500\n",
      "15219/15219 [==============================] - 2s 108us/step - loss: 7.1649e-04 - val_loss: 7.0753e-04\n",
      "Epoch 22/500\n",
      "15219/15219 [==============================] - 1s 94us/step - loss: 6.9843e-04 - val_loss: 6.9328e-04\n",
      "Epoch 23/500\n",
      "15219/15219 [==============================] - 1s 71us/step - loss: 6.8180e-04 - val_loss: 6.7645e-04\n",
      "Epoch 24/500\n",
      "15219/15219 [==============================] - 2s 106us/step - loss: 6.6617e-04 - val_loss: 6.5955e-04\n",
      "Epoch 25/500\n",
      "15219/15219 [==============================] - 2s 99us/step - loss: 6.5115e-04 - val_loss: 6.4471e-04\n",
      "Epoch 26/500\n",
      "15219/15219 [==============================] - 2s 99us/step - loss: 6.3745e-04 - val_loss: 6.3250e-04\n",
      "Epoch 27/500\n",
      "15219/15219 [==============================] - 1s 64us/step - loss: 6.2410e-04 - val_loss: 6.1996e-04\n",
      "Epoch 28/500\n",
      "15219/15219 [==============================] - 1s 64us/step - loss: 6.1158e-04 - val_loss: 6.0958e-04\n",
      "Epoch 29/500\n",
      "15219/15219 [==============================] - 1s 63us/step - loss: 5.9981e-04 - val_loss: 5.9657e-04\n",
      "Epoch 30/500\n",
      "15219/15219 [==============================] - 1s 64us/step - loss: 5.8843e-04 - val_loss: 5.8715e-04\n",
      "Epoch 31/500\n",
      "15219/15219 [==============================] - 1s 67us/step - loss: 5.7766e-04 - val_loss: 5.7897e-04\n",
      "Epoch 32/500\n",
      "15219/15219 [==============================] - 1s 63us/step - loss: 5.6736e-04 - val_loss: 5.6338e-04\n",
      "Epoch 33/500\n",
      "15219/15219 [==============================] - 1s 59us/step - loss: 5.5765e-04 - val_loss: 5.5468e-04\n",
      "Epoch 34/500\n",
      "15219/15219 [==============================] - 1s 66us/step - loss: 5.4815e-04 - val_loss: 5.4937e-04\n",
      "Epoch 35/500\n",
      "15219/15219 [==============================] - 1s 63us/step - loss: 5.3915e-04 - val_loss: 5.3745e-04\n",
      "Epoch 36/500\n",
      "15219/15219 [==============================] - 1s 77us/step - loss: 5.3053e-04 - val_loss: 5.2912e-04\n",
      "Epoch 37/500\n",
      "15219/15219 [==============================] - 1s 64us/step - loss: 5.2227e-04 - val_loss: 5.2234e-04\n",
      "Epoch 38/500\n",
      "15219/15219 [==============================] - 1s 63us/step - loss: 5.1430e-04 - val_loss: 5.1580e-04\n",
      "Epoch 39/500\n",
      "15219/15219 [==============================] - 1s 61us/step - loss: 5.0667e-04 - val_loss: 5.0612e-04\n",
      "Epoch 40/500\n",
      "15219/15219 [==============================] - 1s 66us/step - loss: 4.9931e-04 - val_loss: 5.0129e-04\n",
      "Epoch 41/500\n",
      "15219/15219 [==============================] - 1s 81us/step - loss: 4.9223e-04 - val_loss: 4.9249e-04\n",
      "Test MSE: 0.00052592577\n",
      "Total MSE: 0.00049638295\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input-layer (InputLayer)     (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "hidden-layer-1 (Dense)       (None, 150)               45150     \n",
      "_________________________________________________________________\n",
      "hidden-layer-2 (Dense)       (None, 75)                11325     \n",
      "_________________________________________________________________\n",
      "hidden-layer-3 (Dense)       (None, 150)               11400     \n",
      "_________________________________________________________________\n",
      "output-layer (Dense)         (None, 300)               45300     \n",
      "=================================================================\n",
      "Total params: 113,175\n",
      "Trainable params: 113,175\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15219 samples, validate on 3805 samples\n",
      "Epoch 1/500\n",
      "15219/15219 [==============================] - 2s 123us/step - loss: 0.0367 - val_loss: 0.0041\n",
      "Epoch 2/500\n",
      "15219/15219 [==============================] - 1s 90us/step - loss: 0.0033 - val_loss: 0.0030\n",
      "Epoch 3/500\n",
      "15219/15219 [==============================] - 1s 88us/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 4/500\n",
      "15219/15219 [==============================] - 1s 86us/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 5/500\n",
      "15219/15219 [==============================] - 2s 105us/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 6/500\n",
      "15219/15219 [==============================] - 1s 84us/step - loss: 0.0023 - val_loss: 0.0024\n",
      "Epoch 7/500\n",
      "15219/15219 [==============================] - 2s 129us/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 8/500\n",
      "15219/15219 [==============================] - 2s 144us/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 9/500\n",
      "15219/15219 [==============================] - 2s 138us/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 10/500\n",
      "15219/15219 [==============================] - 2s 144us/step - loss: 0.0020 - val_loss: 0.0021\n",
      "Epoch 11/500\n",
      "15219/15219 [==============================] - 2s 144us/step - loss: 0.0019 - val_loss: 0.0020\n",
      "Epoch 12/500\n",
      "15219/15219 [==============================] - 2s 155us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 13/500\n",
      "15219/15219 [==============================] - 1s 93us/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 14/500\n",
      "15219/15219 [==============================] - 2s 142us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 15/500\n",
      "15219/15219 [==============================] - 3s 197us/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 16/500\n",
      "15219/15219 [==============================] - 1s 90us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 17/500\n",
      "15219/15219 [==============================] - 1s 93us/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 18/500\n",
      "15219/15219 [==============================] - 2s 143us/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 19/500\n",
      "15219/15219 [==============================] - 3s 195us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 20/500\n",
      "15219/15219 [==============================] - 2s 151us/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 21/500\n",
      "15219/15219 [==============================] - 3s 172us/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 22/500\n",
      "15219/15219 [==============================] - 3s 207us/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 23/500\n",
      "15219/15219 [==============================] - 4s 278us/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 24/500\n",
      "15219/15219 [==============================] - 3s 226us/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 25/500\n",
      "15219/15219 [==============================] - 3s 174us/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 26/500\n",
      "15219/15219 [==============================] - 2s 127us/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 27/500\n",
      "15219/15219 [==============================] - 2s 128us/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 28/500\n",
      "15219/15219 [==============================] - 2s 106us/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 29/500\n",
      "15219/15219 [==============================] - 2s 102us/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 30/500\n",
      "15219/15219 [==============================] - 2s 105us/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 31/500\n",
      "15219/15219 [==============================] - 2s 133us/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 32/500\n",
      "15219/15219 [==============================] - 2s 141us/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 33/500\n",
      "15219/15219 [==============================] - 2s 151us/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 34/500\n",
      "15219/15219 [==============================] - 2s 121us/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 35/500\n",
      "15219/15219 [==============================] - 2s 164us/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 36/500\n",
      "15219/15219 [==============================] - 3s 174us/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 37/500\n",
      "15219/15219 [==============================] - 4s 238us/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 38/500\n",
      "15219/15219 [==============================] - 3s 192us/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 39/500\n",
      "15219/15219 [==============================] - 3s 171us/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 40/500\n",
      "15219/15219 [==============================] - 2s 110us/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 41/500\n",
      "15219/15219 [==============================] - 2s 124us/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 42/500\n",
      "15219/15219 [==============================] - 2s 116us/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 43/500\n",
      "15219/15219 [==============================] - 2s 150us/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 44/500\n",
      "15219/15219 [==============================] - 2s 143us/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 45/500\n",
      "15219/15219 [==============================] - 2s 118us/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 46/500\n",
      "15219/15219 [==============================] - 1s 96us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 47/500\n",
      "15219/15219 [==============================] - 2s 117us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 48/500\n",
      "15219/15219 [==============================] - 2s 99us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 49/500\n",
      "15219/15219 [==============================] - 2s 104us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 50/500\n",
      "15219/15219 [==============================] - 2s 115us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 51/500\n",
      "15219/15219 [==============================] - 2s 101us/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 52/500\n",
      "15219/15219 [==============================] - 1s 95us/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 53/500\n",
      "15219/15219 [==============================] - 2s 102us/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 54/500\n",
      "15219/15219 [==============================] - 2s 108us/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 55/500\n",
      "15219/15219 [==============================] - 1s 94us/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 56/500\n",
      "15219/15219 [==============================] - 2s 107us/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 57/500\n",
      "15219/15219 [==============================] - 1s 91us/step - loss: 9.9281e-04 - val_loss: 0.0010\n",
      "Epoch 58/500\n",
      "15219/15219 [==============================] - 2s 107us/step - loss: 9.8477e-04 - val_loss: 0.0010\n",
      "Epoch 59/500\n",
      "15219/15219 [==============================] - 1s 90us/step - loss: 9.7689e-04 - val_loss: 0.0010\n",
      "Epoch 60/500\n",
      "15219/15219 [==============================] - 1s 87us/step - loss: 9.6917e-04 - val_loss: 0.0010\n",
      "Epoch 61/500\n",
      "15219/15219 [==============================] - 2s 104us/step - loss: 9.6144e-04 - val_loss: 0.0010\n",
      "Epoch 62/500\n",
      "15219/15219 [==============================] - 1s 90us/step - loss: 9.5418e-04 - val_loss: 9.9920e-04\n",
      "Epoch 63/500\n",
      "15219/15219 [==============================] - 1s 91us/step - loss: 9.4678e-04 - val_loss: 9.9173e-04\n",
      "Epoch 64/500\n",
      "15219/15219 [==============================] - 1s 96us/step - loss: 9.3962e-04 - val_loss: 9.8496e-04\n",
      "Epoch 65/500\n",
      "15219/15219 [==============================] - 1s 86us/step - loss: 9.3255e-04 - val_loss: 9.7670e-04\n",
      "Epoch 66/500\n",
      "15219/15219 [==============================] - 1s 86us/step - loss: 9.2566e-04 - val_loss: 9.7046e-04\n",
      "Epoch 67/500\n",
      "15219/15219 [==============================] - 1s 87us/step - loss: 9.1883e-04 - val_loss: 9.6395e-04\n",
      "Epoch 68/500\n",
      "15219/15219 [==============================] - 1s 94us/step - loss: 9.1227e-04 - val_loss: 9.5575e-04\n",
      "Epoch 69/500\n",
      "15219/15219 [==============================] - 1s 97us/step - loss: 9.0577e-04 - val_loss: 9.4979e-04\n",
      "Epoch 70/500\n",
      "15219/15219 [==============================] - 1s 91us/step - loss: 8.9933e-04 - val_loss: 9.4427e-04\n",
      "Epoch 71/500\n",
      "15219/15219 [==============================] - 1s 88us/step - loss: 8.9292e-04 - val_loss: 9.3759e-04\n",
      "Epoch 72/500\n",
      "15219/15219 [==============================] - 1s 85us/step - loss: 8.8683e-04 - val_loss: 9.3071e-04\n",
      "Epoch 73/500\n",
      "15219/15219 [==============================] - 1s 89us/step - loss: 8.8072e-04 - val_loss: 9.2412e-04\n",
      "Epoch 74/500\n",
      "15219/15219 [==============================] - 1s 85us/step - loss: 8.7475e-04 - val_loss: 9.1835e-04\n",
      "Epoch 75/500\n",
      "15219/15219 [==============================] - 1s 81us/step - loss: 8.6894e-04 - val_loss: 9.1196e-04\n",
      "Epoch 76/500\n",
      "15219/15219 [==============================] - 1s 89us/step - loss: 8.6311e-04 - val_loss: 9.0652e-04\n",
      "Epoch 77/500\n",
      "15219/15219 [==============================] - 1s 79us/step - loss: 8.5750e-04 - val_loss: 9.0088e-04\n",
      "Epoch 78/500\n",
      "15219/15219 [==============================] - 1s 77us/step - loss: 8.5192e-04 - val_loss: 8.9539e-04\n",
      "Epoch 79/500\n",
      "15219/15219 [==============================] - 1s 85us/step - loss: 8.4638e-04 - val_loss: 8.8963e-04\n",
      "Epoch 80/500\n",
      "15219/15219 [==============================] - 1s 82us/step - loss: 8.4106e-04 - val_loss: 8.8485e-04\n",
      "Epoch 81/500\n",
      "15219/15219 [==============================] - 1s 84us/step - loss: 8.3577e-04 - val_loss: 8.7811e-04\n",
      "Epoch 82/500\n",
      "15219/15219 [==============================] - 1s 77us/step - loss: 8.3058e-04 - val_loss: 8.7322e-04\n",
      "Epoch 83/500\n",
      "15219/15219 [==============================] - 1s 81us/step - loss: 8.2548e-04 - val_loss: 8.6790e-04\n",
      "Epoch 84/500\n",
      "15219/15219 [==============================] - 1s 89us/step - loss: 8.2040e-04 - val_loss: 8.6268e-04\n",
      "Epoch 85/500\n",
      "15219/15219 [==============================] - 1s 83us/step - loss: 8.1551e-04 - val_loss: 8.5775e-04\n",
      "Epoch 86/500\n",
      "15219/15219 [==============================] - 1s 82us/step - loss: 8.1059e-04 - val_loss: 8.5270e-04\n",
      "Epoch 87/500\n",
      "15219/15219 [==============================] - 1s 77us/step - loss: 8.0588e-04 - val_loss: 8.4796e-04\n",
      "Epoch 88/500\n",
      "15219/15219 [==============================] - 1s 87us/step - loss: 8.0113e-04 - val_loss: 8.4266e-04\n",
      "Epoch 89/500\n",
      "15219/15219 [==============================] - 1s 81us/step - loss: 7.9659e-04 - val_loss: 8.3761e-04\n",
      "Epoch 90/500\n",
      "15219/15219 [==============================] - 1s 91us/step - loss: 7.9203e-04 - val_loss: 8.3292e-04\n",
      "Epoch 91/500\n",
      "15219/15219 [==============================] - 1s 82us/step - loss: 7.8757e-04 - val_loss: 8.2915e-04\n",
      "Epoch 92/500\n",
      "15219/15219 [==============================] - 2s 104us/step - loss: 7.8319e-04 - val_loss: 8.2527e-04\n",
      "Epoch 93/500\n",
      "15219/15219 [==============================] - 1s 87us/step - loss: 7.7881e-04 - val_loss: 8.2123e-04\n",
      "Epoch 94/500\n",
      "15219/15219 [==============================] - 1s 80us/step - loss: 7.7462e-04 - val_loss: 8.1534e-04\n",
      "Epoch 95/500\n",
      "15219/15219 [==============================] - 1s 84us/step - loss: 7.7039e-04 - val_loss: 8.1122e-04\n",
      "Epoch 96/500\n",
      "15219/15219 [==============================] - 1s 79us/step - loss: 7.6629e-04 - val_loss: 8.0677e-04\n",
      "Epoch 97/500\n",
      "15219/15219 [==============================] - 1s 78us/step - loss: 7.6223e-04 - val_loss: 8.0412e-04\n",
      "Epoch 98/500\n",
      "15219/15219 [==============================] - 1s 82us/step - loss: 7.5827e-04 - val_loss: 7.9885e-04\n",
      "Epoch 99/500\n",
      "15219/15219 [==============================] - 1s 78us/step - loss: 7.5434e-04 - val_loss: 7.9492e-04\n",
      "Epoch 100/500\n",
      "15219/15219 [==============================] - 1s 78us/step - loss: 7.5048e-04 - val_loss: 7.9113e-04\n",
      "Epoch 101/500\n",
      "15219/15219 [==============================] - 1s 91us/step - loss: 7.4668e-04 - val_loss: 7.8772e-04\n",
      "Epoch 102/500\n",
      "15219/15219 [==============================] - 1s 83us/step - loss: 7.4291e-04 - val_loss: 7.8408e-04\n",
      "Epoch 103/500\n",
      "15219/15219 [==============================] - 1s 83us/step - loss: 7.3924e-04 - val_loss: 7.8060e-04\n",
      "Epoch 104/500\n",
      "15219/15219 [==============================] - 1s 94us/step - loss: 7.3557e-04 - val_loss: 7.7536e-04\n",
      "Epoch 105/500\n",
      "15219/15219 [==============================] - 1s 84us/step - loss: 7.3197e-04 - val_loss: 7.7232e-04\n",
      "Epoch 106/500\n",
      "15219/15219 [==============================] - 1s 82us/step - loss: 7.2849e-04 - val_loss: 7.6915e-04\n",
      "Epoch 107/500\n",
      "15219/15219 [==============================] - 1s 83us/step - loss: 7.2502e-04 - val_loss: 7.6575e-04\n",
      "Epoch 108/500\n",
      "15219/15219 [==============================] - 1s 84us/step - loss: 7.2161e-04 - val_loss: 7.6136e-04\n",
      "Epoch 109/500\n",
      "15219/15219 [==============================] - 2s 109us/step - loss: 7.1828e-04 - val_loss: 7.5751e-04\n",
      "Epoch 110/500\n",
      "15219/15219 [==============================] - 1s 96us/step - loss: 7.1494e-04 - val_loss: 7.5461e-04\n",
      "Epoch 111/500\n",
      "15219/15219 [==============================] - 1s 91us/step - loss: 7.1169e-04 - val_loss: 7.5089e-04\n",
      "Epoch 112/500\n",
      "15219/15219 [==============================] - 1s 84us/step - loss: 7.0843e-04 - val_loss: 7.4775e-04\n",
      "Epoch 113/500\n",
      "15219/15219 [==============================] - 1s 90us/step - loss: 7.0529e-04 - val_loss: 7.4465e-04\n",
      "Epoch 114/500\n",
      "15219/15219 [==============================] - 1s 84us/step - loss: 7.0215e-04 - val_loss: 7.4132e-04\n",
      "Epoch 115/500\n",
      "15219/15219 [==============================] - 2s 107us/step - loss: 6.9906e-04 - val_loss: 7.3813e-04\n",
      "Epoch 116/500\n",
      "15219/15219 [==============================] - 2s 107us/step - loss: 6.9609e-04 - val_loss: 7.3525e-04\n",
      "Epoch 117/500\n",
      "15219/15219 [==============================] - 1s 85us/step - loss: 6.9305e-04 - val_loss: 7.3221e-04\n",
      "Epoch 118/500\n",
      "15219/15219 [==============================] - 1s 80us/step - loss: 6.9011e-04 - val_loss: 7.2858e-04\n",
      "Epoch 119/500\n",
      "15219/15219 [==============================] - 1s 84us/step - loss: 6.8716e-04 - val_loss: 7.2616e-04\n",
      "Epoch 120/500\n",
      "15219/15219 [==============================] - 1s 94us/step - loss: 6.8430e-04 - val_loss: 7.2324e-04\n",
      "Epoch 121/500\n",
      "15219/15219 [==============================] - 1s 85us/step - loss: 6.8148e-04 - val_loss: 7.2120e-04\n",
      "Epoch 122/500\n",
      "15219/15219 [==============================] - 1s 87us/step - loss: 6.7862e-04 - val_loss: 7.1693e-04\n",
      "Epoch 123/500\n",
      "15219/15219 [==============================] - 1s 88us/step - loss: 6.7592e-04 - val_loss: 7.1473e-04\n",
      "Epoch 124/500\n",
      "15219/15219 [==============================] - 1s 93us/step - loss: 6.7314e-04 - val_loss: 7.1124e-04\n",
      "Epoch 125/500\n",
      "15219/15219 [==============================] - 1s 95us/step - loss: 6.7048e-04 - val_loss: 7.0905e-04\n",
      "Epoch 126/500\n",
      "15219/15219 [==============================] - 1s 94us/step - loss: 6.6783e-04 - val_loss: 7.0701e-04\n",
      "Epoch 127/500\n",
      "15219/15219 [==============================] - 1s 81us/step - loss: 6.6522e-04 - val_loss: 7.0365e-04\n",
      "Epoch 128/500\n",
      "15219/15219 [==============================] - 1s 81us/step - loss: 6.6264e-04 - val_loss: 7.0103e-04\n",
      "Epoch 129/500\n",
      "15219/15219 [==============================] - 1s 83us/step - loss: 6.6008e-04 - val_loss: 6.9767e-04\n",
      "Epoch 130/500\n",
      "15219/15219 [==============================] - 1s 90us/step - loss: 6.5754e-04 - val_loss: 6.9629e-04\n",
      "Epoch 131/500\n",
      "15219/15219 [==============================] - 1s 95us/step - loss: 6.5506e-04 - val_loss: 6.9286e-04\n",
      "Epoch 132/500\n",
      "15219/15219 [==============================] - 2s 103us/step - loss: 6.5257e-04 - val_loss: 6.9062e-04\n",
      "Epoch 133/500\n",
      "15219/15219 [==============================] - 2s 111us/step - loss: 6.5014e-04 - val_loss: 6.8916e-04\n",
      "Epoch 134/500\n",
      "15219/15219 [==============================] - 1s 91us/step - loss: 6.4771e-04 - val_loss: 6.8559e-04\n",
      "Epoch 135/500\n",
      "15219/15219 [==============================] - 1s 88us/step - loss: 6.4534e-04 - val_loss: 6.8351e-04\n",
      "Epoch 136/500\n",
      "15219/15219 [==============================] - 1s 95us/step - loss: 6.4298e-04 - val_loss: 6.8133e-04\n",
      "Epoch 137/500\n",
      "15219/15219 [==============================] - 1s 86us/step - loss: 6.4070e-04 - val_loss: 6.7771e-04\n",
      "Epoch 138/500\n",
      "15219/15219 [==============================] - 1s 97us/step - loss: 6.3838e-04 - val_loss: 6.7541e-04\n",
      "Epoch 139/500\n",
      "15219/15219 [==============================] - 2s 99us/step - loss: 6.3612e-04 - val_loss: 6.7411e-04\n",
      "Epoch 140/500\n",
      "15219/15219 [==============================] - 1s 97us/step - loss: 6.3391e-04 - val_loss: 6.7159e-04\n",
      "Epoch 141/500\n",
      "15219/15219 [==============================] - 1s 89us/step - loss: 6.3167e-04 - val_loss: 6.6897e-04\n",
      "Epoch 142/500\n",
      "15219/15219 [==============================] - 1s 85us/step - loss: 6.2942e-04 - val_loss: 6.6680e-04\n",
      "Epoch 143/500\n",
      "15219/15219 [==============================] - 2s 100us/step - loss: 6.2729e-04 - val_loss: 6.6410e-04\n",
      "Epoch 144/500\n",
      "15219/15219 [==============================] - 1s 89us/step - loss: 6.2509e-04 - val_loss: 6.6267e-04\n",
      "Epoch 145/500\n",
      "15219/15219 [==============================] - 1s 83us/step - loss: 6.2298e-04 - val_loss: 6.6083e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 146/500\n",
      "15219/15219 [==============================] - 2s 101us/step - loss: 6.2087e-04 - val_loss: 6.5803e-04\n",
      "Epoch 147/500\n",
      "15219/15219 [==============================] - 1s 85us/step - loss: 6.1878e-04 - val_loss: 6.5630e-04\n",
      "Epoch 148/500\n",
      "15219/15219 [==============================] - 1s 80us/step - loss: 6.1670e-04 - val_loss: 6.5336e-04\n",
      "Epoch 149/500\n",
      "15219/15219 [==============================] - 1s 80us/step - loss: 6.1468e-04 - val_loss: 6.5118e-04\n",
      "Epoch 150/500\n",
      "15219/15219 [==============================] - 1s 82us/step - loss: 6.1267e-04 - val_loss: 6.4925e-04\n",
      "Epoch 151/500\n",
      "15219/15219 [==============================] - 2s 105us/step - loss: 6.1064e-04 - val_loss: 6.4834e-04\n",
      "Epoch 152/500\n",
      "15219/15219 [==============================] - 2s 101us/step - loss: 6.0868e-04 - val_loss: 6.4520e-04\n",
      "Epoch 153/500\n",
      "15219/15219 [==============================] - 1s 91us/step - loss: 6.0672e-04 - val_loss: 6.4354e-04\n",
      "Epoch 154/500\n",
      "15219/15219 [==============================] - 1s 92us/step - loss: 6.0472e-04 - val_loss: 6.4126e-04\n",
      "Epoch 155/500\n",
      "15219/15219 [==============================] - 1s 82us/step - loss: 6.0283e-04 - val_loss: 6.3888e-04\n",
      "Epoch 156/500\n",
      "15219/15219 [==============================] - 1s 80us/step - loss: 6.0092e-04 - val_loss: 6.3697e-04\n",
      "Epoch 157/500\n",
      "15219/15219 [==============================] - 1s 82us/step - loss: 5.9904e-04 - val_loss: 6.3531e-04\n",
      "Epoch 158/500\n",
      "15219/15219 [==============================] - 1s 81us/step - loss: 5.9714e-04 - val_loss: 6.3369e-04\n",
      "Epoch 159/500\n",
      "15219/15219 [==============================] - 1s 89us/step - loss: 5.9525e-04 - val_loss: 6.3103e-04\n",
      "Epoch 160/500\n",
      "15219/15219 [==============================] - 1s 85us/step - loss: 5.9343e-04 - val_loss: 6.2938e-04\n",
      "Epoch 161/500\n",
      "15219/15219 [==============================] - 1s 86us/step - loss: 5.9162e-04 - val_loss: 6.2821e-04\n",
      "Epoch 162/500\n",
      "15219/15219 [==============================] - 2s 99us/step - loss: 5.8978e-04 - val_loss: 6.2553e-04\n",
      "Epoch 163/500\n",
      "15219/15219 [==============================] - 1s 85us/step - loss: 5.8795e-04 - val_loss: 6.2436e-04\n",
      "Epoch 164/500\n",
      "15219/15219 [==============================] - 1s 79us/step - loss: 5.8626e-04 - val_loss: 6.2192e-04\n",
      "Epoch 165/500\n",
      "15219/15219 [==============================] - 1s 82us/step - loss: 5.8445e-04 - val_loss: 6.2031e-04\n",
      "Epoch 166/500\n",
      "15219/15219 [==============================] - 1s 85us/step - loss: 5.8275e-04 - val_loss: 6.1841e-04\n",
      "Epoch 167/500\n",
      "15219/15219 [==============================] - 1s 90us/step - loss: 5.8101e-04 - val_loss: 6.1610e-04\n",
      "Epoch 168/500\n",
      "15219/15219 [==============================] - 1s 92us/step - loss: 5.7926e-04 - val_loss: 6.1457e-04\n",
      "Epoch 169/500\n",
      "15219/15219 [==============================] - 1s 85us/step - loss: 5.7756e-04 - val_loss: 6.1297e-04\n",
      "Epoch 170/500\n",
      "15219/15219 [==============================] - 1s 94us/step - loss: 5.7588e-04 - val_loss: 6.1126e-04\n",
      "Epoch 171/500\n",
      "15219/15219 [==============================] - 1s 84us/step - loss: 5.7416e-04 - val_loss: 6.0957e-04\n",
      "Epoch 172/500\n",
      "15219/15219 [==============================] - 1s 83us/step - loss: 5.7250e-04 - val_loss: 6.0772e-04\n",
      "Epoch 173/500\n",
      "15219/15219 [==============================] - 1s 86us/step - loss: 5.7089e-04 - val_loss: 6.0601e-04\n",
      "Epoch 174/500\n",
      "15219/15219 [==============================] - 1s 84us/step - loss: 5.6922e-04 - val_loss: 6.0423e-04\n",
      "Epoch 175/500\n",
      "15219/15219 [==============================] - 1s 86us/step - loss: 5.6760e-04 - val_loss: 6.0301e-04\n",
      "Epoch 176/500\n",
      "15219/15219 [==============================] - 1s 88us/step - loss: 5.6593e-04 - val_loss: 6.0111e-04\n",
      "Epoch 177/500\n",
      "15219/15219 [==============================] - 1s 97us/step - loss: 5.6440e-04 - val_loss: 5.9947e-04\n",
      "Epoch 178/500\n",
      "15219/15219 [==============================] - 1s 95us/step - loss: 5.6279e-04 - val_loss: 5.9777e-04\n",
      "Epoch 179/500\n",
      "15219/15219 [==============================] - 1s 89us/step - loss: 5.6121e-04 - val_loss: 5.9572e-04\n",
      "Epoch 180/500\n",
      "15219/15219 [==============================] - 2s 139us/step - loss: 5.5968e-04 - val_loss: 5.9437e-04\n",
      "Epoch 181/500\n",
      "15219/15219 [==============================] - 2s 136us/step - loss: 5.5808e-04 - val_loss: 5.9253e-04\n",
      "Epoch 182/500\n",
      "15219/15219 [==============================] - 2s 99us/step - loss: 5.5655e-04 - val_loss: 5.9098e-04\n",
      "Epoch 183/500\n",
      "15219/15219 [==============================] - 1s 92us/step - loss: 5.5501e-04 - val_loss: 5.8978e-04\n",
      "Epoch 184/500\n",
      "15219/15219 [==============================] - 2s 115us/step - loss: 5.5352e-04 - val_loss: 5.8802e-04\n",
      "Epoch 185/500\n",
      "15219/15219 [==============================] - 2s 100us/step - loss: 5.5197e-04 - val_loss: 5.8641e-04\n",
      "Epoch 186/500\n",
      "15219/15219 [==============================] - 2s 134us/step - loss: 5.5049e-04 - val_loss: 5.8477e-04\n",
      "Epoch 187/500\n",
      "15219/15219 [==============================] - 2s 139us/step - loss: 5.4900e-04 - val_loss: 5.8304e-04\n",
      "Epoch 188/500\n",
      "15219/15219 [==============================] - 1s 96us/step - loss: 5.4751e-04 - val_loss: 5.8133e-04\n",
      "Epoch 189/500\n",
      "15219/15219 [==============================] - 1s 93us/step - loss: 5.4601e-04 - val_loss: 5.8055e-04\n",
      "Epoch 190/500\n",
      "15219/15219 [==============================] - 1s 94us/step - loss: 5.4458e-04 - val_loss: 5.7919e-04\n",
      "Epoch 191/500\n",
      "15219/15219 [==============================] - 1s 86us/step - loss: 5.4314e-04 - val_loss: 5.7732e-04\n",
      "Epoch 192/500\n",
      "15219/15219 [==============================] - 1s 87us/step - loss: 5.4171e-04 - val_loss: 5.7581e-04\n",
      "Epoch 193/500\n",
      "15219/15219 [==============================] - 1s 88us/step - loss: 5.4026e-04 - val_loss: 5.7435e-04\n",
      "Epoch 194/500\n",
      "15219/15219 [==============================] - 1s 91us/step - loss: 5.3884e-04 - val_loss: 5.7318e-04\n",
      "Epoch 195/500\n",
      "15219/15219 [==============================] - 1s 83us/step - loss: 5.3744e-04 - val_loss: 5.7192e-04\n",
      "Epoch 196/500\n",
      "15219/15219 [==============================] - 1s 81us/step - loss: 5.3601e-04 - val_loss: 5.6979e-04\n",
      "Epoch 197/500\n",
      "15219/15219 [==============================] - 1s 83us/step - loss: 5.3461e-04 - val_loss: 5.6918e-04\n",
      "Epoch 198/500\n",
      "15219/15219 [==============================] - 1s 91us/step - loss: 5.3329e-04 - val_loss: 5.6730e-04\n",
      "Epoch 199/500\n",
      "15219/15219 [==============================] - 1s 92us/step - loss: 5.3189e-04 - val_loss: 5.6520e-04\n",
      "Epoch 200/500\n",
      "15219/15219 [==============================] - 1s 85us/step - loss: 5.3053e-04 - val_loss: 5.6422e-04\n",
      "Epoch 201/500\n",
      "15219/15219 [==============================] - 1s 83us/step - loss: 5.2917e-04 - val_loss: 5.6249e-04\n",
      "Epoch 202/500\n",
      "15219/15219 [==============================] - 1s 83us/step - loss: 5.2787e-04 - val_loss: 5.6127e-04\n",
      "Epoch 203/500\n",
      "15219/15219 [==============================] - 1s 73us/step - loss: 5.2649e-04 - val_loss: 5.5956e-04\n",
      "Epoch 204/500\n",
      "15219/15219 [==============================] - 1s 76us/step - loss: 5.2514e-04 - val_loss: 5.5862e-04\n",
      "Epoch 205/500\n",
      "15219/15219 [==============================] - 1s 77us/step - loss: 5.2386e-04 - val_loss: 5.5729e-04\n",
      "Epoch 206/500\n",
      "15219/15219 [==============================] - 1s 73us/step - loss: 5.2255e-04 - val_loss: 5.5621e-04\n",
      "Epoch 207/500\n",
      "15219/15219 [==============================] - 2s 100us/step - loss: 5.2121e-04 - val_loss: 5.5456e-04\n",
      "Epoch 208/500\n",
      "15219/15219 [==============================] - 1s 91us/step - loss: 5.1994e-04 - val_loss: 5.5345e-04\n",
      "Epoch 209/500\n",
      "15219/15219 [==============================] - 1s 87us/step - loss: 5.1863e-04 - val_loss: 5.5154e-04\n",
      "Epoch 210/500\n",
      "15219/15219 [==============================] - 1s 83us/step - loss: 5.1738e-04 - val_loss: 5.5033e-04\n",
      "Epoch 211/500\n",
      "15219/15219 [==============================] - 1s 94us/step - loss: 5.1608e-04 - val_loss: 5.4863e-04\n",
      "Epoch 212/500\n",
      "15219/15219 [==============================] - 1s 88us/step - loss: 5.1487e-04 - val_loss: 5.4720e-04\n",
      "Epoch 213/500\n",
      "15219/15219 [==============================] - 1s 77us/step - loss: 5.1358e-04 - val_loss: 5.4635e-04\n",
      "Epoch 214/500\n",
      "15219/15219 [==============================] - 1s 88us/step - loss: 5.1235e-04 - val_loss: 5.4487e-04\n",
      "Epoch 215/500\n",
      "15219/15219 [==============================] - 1s 91us/step - loss: 5.1110e-04 - val_loss: 5.4365e-04\n",
      "Epoch 216/500\n",
      "15219/15219 [==============================] - 1s 85us/step - loss: 5.0988e-04 - val_loss: 5.4261e-04\n",
      "Epoch 217/500\n",
      "15219/15219 [==============================] - 1s 86us/step - loss: 5.0862e-04 - val_loss: 5.4079e-04\n",
      "Epoch 218/500\n",
      "15219/15219 [==============================] - 1s 83us/step - loss: 5.0742e-04 - val_loss: 5.4003e-04\n",
      "Epoch 219/500\n",
      "15219/15219 [==============================] - 1s 75us/step - loss: 5.0623e-04 - val_loss: 5.3885e-04\n",
      "Epoch 220/500\n",
      "15219/15219 [==============================] - 1s 77us/step - loss: 5.0499e-04 - val_loss: 5.3733e-04\n",
      "Epoch 221/500\n",
      "15219/15219 [==============================] - 1s 72us/step - loss: 5.0384e-04 - val_loss: 5.3624e-04\n",
      "Epoch 222/500\n",
      "15219/15219 [==============================] - 1s 83us/step - loss: 5.0260e-04 - val_loss: 5.3502e-04\n",
      "Epoch 223/500\n",
      "15219/15219 [==============================] - 1s 86us/step - loss: 5.0147e-04 - val_loss: 5.3353e-04\n",
      "Epoch 224/500\n",
      "15219/15219 [==============================] - 1s 86us/step - loss: 5.0026e-04 - val_loss: 5.3222e-04\n",
      "Epoch 225/500\n",
      "15219/15219 [==============================] - 1s 84us/step - loss: 4.9909e-04 - val_loss: 5.3156e-04\n",
      "Epoch 226/500\n",
      "15219/15219 [==============================] - 1s 83us/step - loss: 4.9796e-04 - val_loss: 5.3039e-04\n",
      "Epoch 227/500\n",
      "15219/15219 [==============================] - 1s 78us/step - loss: 4.9675e-04 - val_loss: 5.2871e-04\n",
      "Epoch 228/500\n",
      "15219/15219 [==============================] - 1s 87us/step - loss: 4.9566e-04 - val_loss: 5.2749e-04\n",
      "Epoch 229/500\n",
      "15219/15219 [==============================] - 1s 79us/step - loss: 4.9449e-04 - val_loss: 5.2634e-04\n",
      "Epoch 230/500\n",
      "15219/15219 [==============================] - 1s 78us/step - loss: 4.9339e-04 - val_loss: 5.2495e-04\n",
      "Epoch 231/500\n",
      "15219/15219 [==============================] - 1s 76us/step - loss: 4.9225e-04 - val_loss: 5.2412e-04\n",
      "Epoch 232/500\n",
      "15219/15219 [==============================] - 1s 84us/step - loss: 4.9110e-04 - val_loss: 5.2270e-04\n",
      "Epoch 233/500\n",
      "15219/15219 [==============================] - 1s 74us/step - loss: 4.9000e-04 - val_loss: 5.2236e-04\n",
      "Epoch 234/500\n",
      "15219/15219 [==============================] - 1s 73us/step - loss: 4.8892e-04 - val_loss: 5.2028e-04\n",
      "Epoch 235/500\n",
      "15219/15219 [==============================] - 1s 84us/step - loss: 4.8781e-04 - val_loss: 5.1971e-04\n",
      "Epoch 236/500\n",
      "15219/15219 [==============================] - 1s 82us/step - loss: 4.8672e-04 - val_loss: 5.1824e-04\n",
      "Epoch 237/500\n",
      "15219/15219 [==============================] - 1s 87us/step - loss: 4.8563e-04 - val_loss: 5.1698e-04\n",
      "Epoch 238/500\n",
      "15219/15219 [==============================] - 1s 90us/step - loss: 4.8454e-04 - val_loss: 5.1614e-04\n",
      "Epoch 239/500\n",
      "15219/15219 [==============================] - 1s 77us/step - loss: 4.8347e-04 - val_loss: 5.1487e-04\n",
      "Epoch 240/500\n",
      "15219/15219 [==============================] - 1s 79us/step - loss: 4.8237e-04 - val_loss: 5.1441e-04\n",
      "Epoch 241/500\n",
      "15219/15219 [==============================] - 1s 76us/step - loss: 4.8132e-04 - val_loss: 5.1281e-04\n",
      "Epoch 242/500\n",
      "15219/15219 [==============================] - 1s 73us/step - loss: 4.8026e-04 - val_loss: 5.1145e-04\n",
      "Epoch 243/500\n",
      "15219/15219 [==============================] - 1s 75us/step - loss: 4.7921e-04 - val_loss: 5.1026e-04\n",
      "Epoch 244/500\n",
      "15219/15219 [==============================] - 1s 75us/step - loss: 4.7814e-04 - val_loss: 5.0936e-04\n",
      "Epoch 245/500\n",
      "15219/15219 [==============================] - 1s 74us/step - loss: 4.7710e-04 - val_loss: 5.0799e-04\n",
      "Epoch 246/500\n",
      "15219/15219 [==============================] - 1s 82us/step - loss: 4.7608e-04 - val_loss: 5.0728e-04\n",
      "Epoch 247/500\n",
      "15219/15219 [==============================] - 1s 76us/step - loss: 4.7504e-04 - val_loss: 5.0597e-04\n",
      "Epoch 248/500\n",
      "15219/15219 [==============================] - 1s 75us/step - loss: 4.7404e-04 - val_loss: 5.0494e-04\n",
      "Epoch 249/500\n",
      "15219/15219 [==============================] - 1s 76us/step - loss: 4.7300e-04 - val_loss: 5.0415e-04\n",
      "Epoch 250/500\n",
      "15219/15219 [==============================] - 1s 86us/step - loss: 4.7201e-04 - val_loss: 5.0270e-04\n",
      "Epoch 251/500\n",
      "15219/15219 [==============================] - 1s 74us/step - loss: 4.7099e-04 - val_loss: 5.0171e-04\n",
      "Epoch 252/500\n",
      "15219/15219 [==============================] - 1s 71us/step - loss: 4.6998e-04 - val_loss: 5.0091e-04\n",
      "Epoch 253/500\n",
      "15219/15219 [==============================] - 1s 76us/step - loss: 4.6897e-04 - val_loss: 4.9978e-04\n",
      "Test MSE: 0.00050452715\n",
      "Total MSE: 0.00048063869\n",
      "e.jpg\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 75)                22575     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 300)               22800     \n",
      "=================================================================\n",
      "Total params: 45,375\n",
      "Trainable params: 45,375\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 23483 samples, validate on 5871 samples\n",
      "Epoch 1/500\n",
      "23483/23483 [==============================] - 2s 73us/step - loss: 0.0010 - val_loss: 3.8143e-04\n",
      "Test MSE: 0.00036147165\n",
      "Total MSE: 0.00037381756\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input-layer (InputLayer)     (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "hidden-layer-1 (Dense)       (None, 150)               45150     \n",
      "_________________________________________________________________\n",
      "hidden-layer-2 (Dense)       (None, 75)                11325     \n",
      "_________________________________________________________________\n",
      "hidden-layer-3 (Dense)       (None, 150)               11400     \n",
      "_________________________________________________________________\n",
      "output-layer (Dense)         (None, 300)               45300     \n",
      "=================================================================\n",
      "Total params: 113,175\n",
      "Trainable params: 113,175\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 23483 samples, validate on 5871 samples\n",
      "Epoch 1/500\n",
      "23483/23483 [==============================] - 2s 96us/step - loss: 0.0021 - val_loss: 5.4648e-04\n",
      "Epoch 2/500\n",
      "23483/23483 [==============================] - 2s 77us/step - loss: 4.3735e-04 - val_loss: 3.4274e-04\n",
      "Test MSE: 0.00036182586\n",
      "Total MSE: 0.00035229229\n"
     ]
    }
   ],
   "source": [
    "image_epochs = {}\n",
    "image_epochs_deep = {}\n",
    "for image in ['a.jpg', 'b.jpg', 'c.jpg', 'd.jpg', 'e.jpg']:\n",
    "    print(image)\n",
    "    image_epochs[image] = process_image(image, 4, 5e-4, lr=1e-2)\n",
    "    image_epochs_deep[image] = process_image_deeply(image, 4, 5e-4, lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x468 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[3, 347, 7, 41, 1]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.rcParams['font.size'] = 15\n",
    "plt.rcParams['font.weight'] = 100\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(15, 6.5)\n",
    "\n",
    "n = len(image_epochs)\n",
    "\n",
    "ax.set(xlabel='Image name', ylabel='Epochs count')\n",
    "ax.bar(np.arange(n), [len(e['loss']) for e in image_epochs.values()], 0.3, label='Simple autoencoder')\n",
    "ax.bar(np.arange(n) + 0.3, [len(e['loss']) for e in image_epochs_deep.values()], 0.3, label='Deep autoencoder')\n",
    "\n",
    "ax.set_yscale('log')\n",
    "ax.set_xticks(np.arange(n) + 0.3 / 2)\n",
    "ax.set_xticklabels(list(image_epochs.keys()))\n",
    "\n",
    "ax.grid()\n",
    "ax.legend(loc='upper center')\n",
    "plt.show()\n",
    "[len(e['loss']) for e in image_epochs.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 150)               45150     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 300)               45300     \n",
      "=================================================================\n",
      "Total params: 90,450\n",
      "Trainable params: 90,450\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/200\n",
      "12800/12800 [==============================] - 1s 96us/step - loss: 0.0016 - val_loss: 7.0064e-04\n",
      "Epoch 2/200\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 5.3543e-04 - val_loss: 4.0832e-04\n",
      "Epoch 3/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 3.4472e-04 - val_loss: 2.8634e-04\n",
      "Epoch 4/200\n",
      "12800/12800 [==============================] - 1s 109us/step - loss: 2.5537e-04 - val_loss: 2.2301e-04\n",
      "Epoch 5/200\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 2.0390e-04 - val_loss: 1.8181e-04\n",
      "Epoch 6/200\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 1.7035e-04 - val_loss: 1.5450e-04\n",
      "Epoch 7/200\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 1.4693e-04 - val_loss: 1.3515e-04\n",
      "Epoch 8/200\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 1.2958e-04 - val_loss: 1.2074e-04\n",
      "Epoch 9/200\n",
      "12800/12800 [==============================] - 1s 98us/step - loss: 1.1628e-04 - val_loss: 1.0860e-04\n",
      "Epoch 10/200\n",
      "12800/12800 [==============================] - 1s 97us/step - loss: 1.0569e-04 - val_loss: 9.9466e-05\n",
      "Epoch 11/200\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 9.7011e-05 - val_loss: 9.2103e-05\n",
      "Epoch 12/200\n",
      "12800/12800 [==============================] - 1s 69us/step - loss: 8.9832e-05 - val_loss: 8.5389e-05\n",
      "Epoch 13/200\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 8.3759e-05 - val_loss: 8.0199e-05\n",
      "Epoch 14/200\n",
      "12800/12800 [==============================] - 1s 72us/step - loss: 7.8523e-05 - val_loss: 7.5090e-05\n",
      "Epoch 15/200\n",
      "12800/12800 [==============================] - 1s 101us/step - loss: 7.3965e-05 - val_loss: 7.0916e-05\n",
      "Epoch 16/200\n",
      "12800/12800 [==============================] - 1s 95us/step - loss: 6.9958e-05 - val_loss: 6.7243e-05\n",
      "Epoch 17/200\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 6.6400e-05 - val_loss: 6.4119e-05\n",
      "Epoch 18/200\n",
      "12800/12800 [==============================] - 1s 112us/step - loss: 6.3209e-05 - val_loss: 6.1223e-05\n",
      "Epoch 19/200\n",
      "12800/12800 [==============================] - 2s 144us/step - loss: 6.0330e-05 - val_loss: 5.8232e-05\n",
      "Epoch 20/200\n",
      "12800/12800 [==============================] - 1s 99us/step - loss: 5.7716e-05 - val_loss: 5.5766e-05\n",
      "Epoch 21/200\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 5.5328e-05 - val_loss: 5.4542e-05\n",
      "Epoch 22/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 5.3160e-05 - val_loss: 5.1513e-05\n",
      "Epoch 23/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 5.1141e-05 - val_loss: 4.9484e-05\n",
      "Epoch 24/200\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 4.9285e-05 - val_loss: 4.7743e-05\n",
      "Epoch 25/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 4.7562e-05 - val_loss: 4.6140e-05\n",
      "Epoch 26/200\n",
      "12800/12800 [==============================] - 1s 95us/step - loss: 4.5978e-05 - val_loss: 4.4683e-05\n",
      "Epoch 27/200\n",
      "12800/12800 [==============================] - 1s 107us/step - loss: 4.4487e-05 - val_loss: 4.3153e-05\n",
      "Epoch 28/200\n",
      "12800/12800 [==============================] - 1s 72us/step - loss: 4.3078e-05 - val_loss: 4.1900e-05\n",
      "Epoch 29/200\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 4.1812e-05 - val_loss: 4.0629e-05\n",
      "Epoch 30/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 4.0594e-05 - val_loss: 3.9429e-05\n",
      "Epoch 31/200\n",
      "12800/12800 [==============================] - 1s 100us/step - loss: 3.9437e-05 - val_loss: 3.8223e-05\n",
      "Epoch 32/200\n",
      "12800/12800 [==============================] - 2s 125us/step - loss: 3.8346e-05 - val_loss: 3.7328e-05\n",
      "Epoch 33/200\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 3.7330e-05 - val_loss: 3.6342e-05\n",
      "Epoch 34/200\n",
      "12800/12800 [==============================] - 1s 62us/step - loss: 3.6360e-05 - val_loss: 3.5397e-05\n",
      "Epoch 35/200\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 3.5464e-05 - val_loss: 3.4517e-05\n",
      "Epoch 36/200\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 3.4591e-05 - val_loss: 3.3708e-05\n",
      "Epoch 37/200\n",
      "12800/12800 [==============================] - 1s 68us/step - loss: 3.3773e-05 - val_loss: 3.2850e-05\n",
      "Epoch 38/200\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 3.2991e-05 - val_loss: 3.2062e-05\n",
      "Epoch 39/200\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 3.2244e-05 - val_loss: 3.1390e-05\n",
      "Epoch 40/200\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 3.1533e-05 - val_loss: 3.0712e-05\n",
      "Epoch 41/200\n",
      "12800/12800 [==============================] - 1s 65us/step - loss: 3.0856e-05 - val_loss: 3.0049e-05\n",
      "Epoch 42/200\n",
      "12800/12800 [==============================] - 1s 64us/step - loss: 3.0208e-05 - val_loss: 2.9650e-05\n",
      "Epoch 43/200\n",
      "12800/12800 [==============================] - 1s 63us/step - loss: 2.9591e-05 - val_loss: 2.8893e-05\n",
      "Epoch 44/200\n",
      "12800/12800 [==============================] - 1s 62us/step - loss: 2.9002e-05 - val_loss: 2.8312e-05\n",
      "Epoch 45/200\n",
      "12800/12800 [==============================] - 1s 64us/step - loss: 2.8438e-05 - val_loss: 2.7789e-05\n",
      "Epoch 46/200\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 2.7890e-05 - val_loss: 2.7188e-05\n",
      "Epoch 47/200\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 2.7368e-05 - val_loss: 2.6645e-05\n",
      "Epoch 48/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 2.6867e-05 - val_loss: 2.6280e-05\n",
      "Epoch 49/200\n",
      "12800/12800 [==============================] - 1s 68us/step - loss: 2.6384e-05 - val_loss: 2.5733e-05\n",
      "Epoch 50/200\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 2.5922e-05 - val_loss: 2.5301e-05\n",
      "Epoch 51/200\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 2.5475e-05 - val_loss: 2.4918e-05\n",
      "Epoch 52/200\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 2.5036e-05 - val_loss: 2.4423e-05\n",
      "Epoch 53/200\n",
      "12800/12800 [==============================] - 1s 65us/step - loss: 2.4623e-05 - val_loss: 2.4023e-05\n",
      "Epoch 54/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 2.4228e-05 - val_loss: 2.3614e-05\n",
      "Epoch 55/200\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 2.3843e-05 - val_loss: 2.3278e-05\n",
      "Epoch 56/200\n",
      "12800/12800 [==============================] - 1s 61us/step - loss: 2.3470e-05 - val_loss: 2.2881e-05\n",
      "Epoch 57/200\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 2.3109e-05 - val_loss: 2.2553e-05\n",
      "Epoch 58/200\n",
      "12800/12800 [==============================] - 2s 118us/step - loss: 2.2752e-05 - val_loss: 2.2179e-05\n",
      "Epoch 59/200\n",
      "12800/12800 [==============================] - 1s 112us/step - loss: 2.2415e-05 - val_loss: 2.1902e-05\n",
      "Epoch 60/200\n",
      "12800/12800 [==============================] - 1s 102us/step - loss: 2.2093e-05 - val_loss: 2.1630e-05\n",
      "Epoch 61/200\n",
      "12800/12800 [==============================] - 2s 118us/step - loss: 2.1768e-05 - val_loss: 2.1260e-05\n",
      "Epoch 62/200\n",
      "12800/12800 [==============================] - 2s 117us/step - loss: 2.1465e-05 - val_loss: 2.0976e-05\n",
      "Epoch 63/200\n",
      "12800/12800 [==============================] - 1s 112us/step - loss: 2.1161e-05 - val_loss: 2.0648e-05\n",
      "Epoch 64/200\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 2.0877e-05 - val_loss: 2.0356e-05\n",
      "Epoch 65/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 1s 67us/step - loss: 2.0591e-05 - val_loss: 2.0120e-05\n",
      "Epoch 66/200\n",
      "12800/12800 [==============================] - 1s 72us/step - loss: 2.0318e-05 - val_loss: 1.9852e-05\n",
      "Epoch 67/200\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 2.0058e-05 - val_loss: 1.9564e-05\n",
      "Epoch 68/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 1.9793e-05 - val_loss: 1.9316e-05\n",
      "Epoch 69/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 1.9548e-05 - val_loss: 1.9048e-05\n",
      "Epoch 70/200\n",
      "12800/12800 [==============================] - 1s 68us/step - loss: 1.9303e-05 - val_loss: 1.8822e-05\n",
      "Epoch 71/200\n",
      "12800/12800 [==============================] - 1s 64us/step - loss: 1.9060e-05 - val_loss: 1.8575e-05\n",
      "Epoch 72/200\n",
      "12800/12800 [==============================] - 1s 69us/step - loss: 1.8834e-05 - val_loss: 1.8370e-05\n",
      "Epoch 73/200\n",
      "12800/12800 [==============================] - 1s 65us/step - loss: 1.8606e-05 - val_loss: 1.8138e-05\n",
      "Epoch 74/200\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 1.8396e-05 - val_loss: 1.7924e-05\n",
      "Epoch 75/200\n",
      "12800/12800 [==============================] - 1s 64us/step - loss: 1.8168e-05 - val_loss: 1.7832e-05\n",
      "Epoch 76/200\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 1.7968e-05 - val_loss: 1.7513e-05\n",
      "Epoch 77/200\n",
      "12800/12800 [==============================] - 2s 119us/step - loss: 1.7760e-05 - val_loss: 1.7313e-05\n",
      "Epoch 78/200\n",
      "12800/12800 [==============================] - 1s 105us/step - loss: 1.7566e-05 - val_loss: 1.7116e-05\n",
      "Epoch 79/200\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 1.7367e-05 - val_loss: 1.6949e-05\n",
      "Epoch 80/200\n",
      "12800/12800 [==============================] - 1s 72us/step - loss: 1.7185e-05 - val_loss: 1.6786e-05\n",
      "Epoch 81/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 1.7000e-05 - val_loss: 1.6565e-05\n",
      "Epoch 82/200\n",
      "12800/12800 [==============================] - 1s 63us/step - loss: 1.6820e-05 - val_loss: 1.6429e-05\n",
      "Epoch 83/200\n",
      "12800/12800 [==============================] - 1s 72us/step - loss: 1.6648e-05 - val_loss: 1.6325e-05\n",
      "Epoch 84/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 1.6470e-05 - val_loss: 1.6076e-05\n",
      "Epoch 85/200\n",
      "12800/12800 [==============================] - 1s 101us/step - loss: 1.6310e-05 - val_loss: 1.5878e-05\n",
      "Epoch 86/200\n",
      "12800/12800 [==============================] - 1s 108us/step - loss: 1.6142e-05 - val_loss: 1.5719e-05\n",
      "Epoch 87/200\n",
      "12800/12800 [==============================] - 1s 99us/step - loss: 1.5986e-05 - val_loss: 1.5547e-05\n",
      "Epoch 88/200\n",
      "12800/12800 [==============================] - 1s 116us/step - loss: 1.5826e-05 - val_loss: 1.5424e-05\n",
      "Epoch 89/200\n",
      "12800/12800 [==============================] - 1s 114us/step - loss: 1.5671e-05 - val_loss: 1.5291e-05\n",
      "Epoch 90/200\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 1.5520e-05 - val_loss: 1.5152e-05\n",
      "Epoch 91/200\n",
      "12800/12800 [==============================] - 1s 69us/step - loss: 1.5375e-05 - val_loss: 1.4977e-05\n",
      "Epoch 92/200\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 1.5232e-05 - val_loss: 1.4948e-05\n",
      "Epoch 93/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 1.5090e-05 - val_loss: 1.4680e-05\n",
      "Epoch 94/200\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 1.4948e-05 - val_loss: 1.4577e-05\n",
      "Epoch 95/200\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 1.4822e-05 - val_loss: 1.4499e-05\n",
      "Epoch 96/200\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 1.4689e-05 - val_loss: 1.4289e-05\n",
      "Epoch 97/200\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 1.4554e-05 - val_loss: 1.4164e-05\n",
      "Epoch 98/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 1.4425e-05 - val_loss: 1.4065e-05\n",
      "Epoch 99/200\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 1.4306e-05 - val_loss: 1.3907e-05\n",
      "Epoch 100/200\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 1.4182e-05 - val_loss: 1.3776e-05\n",
      "Epoch 101/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 1.4062e-05 - val_loss: 1.3667e-05\n",
      "Epoch 102/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 1.3948e-05 - val_loss: 1.3545e-05\n",
      "Epoch 103/200\n",
      "12800/12800 [==============================] - 1s 63us/step - loss: 1.3828e-05 - val_loss: 1.3453e-05\n",
      "Epoch 104/200\n",
      "12800/12800 [==============================] - 1s 68us/step - loss: 1.3716e-05 - val_loss: 1.3400e-05\n",
      "Epoch 105/200\n",
      "12800/12800 [==============================] - 1s 61us/step - loss: 1.3604e-05 - val_loss: 1.3280e-05\n",
      "Epoch 106/200\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 1.3501e-05 - val_loss: 1.3135e-05\n",
      "Epoch 107/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 1.3389e-05 - val_loss: 1.3050e-05\n",
      "Epoch 108/200\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 1.3285e-05 - val_loss: 1.2924e-05\n",
      "Epoch 109/200\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 1.3185e-05 - val_loss: 1.2809e-05\n",
      "Epoch 110/200\n",
      "12800/12800 [==============================] - 1s 61us/step - loss: 1.3084e-05 - val_loss: 1.2706e-05\n",
      "Epoch 111/200\n",
      "12800/12800 [==============================] - 1s 62us/step - loss: 1.2985e-05 - val_loss: 1.2631e-05\n",
      "Epoch 112/200\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 1.2886e-05 - val_loss: 1.2516e-05\n",
      "Epoch 113/200\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 1.2794e-05 - val_loss: 1.2442e-05\n",
      "Epoch 114/200\n",
      "12800/12800 [==============================] - 1s 72us/step - loss: 1.2699e-05 - val_loss: 1.2356e-05\n",
      "Epoch 115/200\n",
      "12800/12800 [==============================] - 1s 64us/step - loss: 1.2608e-05 - val_loss: 1.2242e-05\n",
      "Epoch 116/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 1.2516e-05 - val_loss: 1.2182e-05\n",
      "Epoch 117/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 1.2430e-05 - val_loss: 1.2064e-05\n",
      "Epoch 118/200\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 1.2338e-05 - val_loss: 1.2041e-05\n",
      "Epoch 119/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 1.2258e-05 - val_loss: 1.1900e-05\n",
      "Epoch 120/200\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 1.2169e-05 - val_loss: 1.1807e-05\n",
      "Epoch 121/200\n",
      "12800/12800 [==============================] - 1s 105us/step - loss: 1.2088e-05 - val_loss: 1.1744e-05\n",
      "Epoch 122/200\n",
      "12800/12800 [==============================] - 1s 114us/step - loss: 1.2005e-05 - val_loss: 1.1679e-05\n",
      "Epoch 123/200\n",
      "12800/12800 [==============================] - 1s 103us/step - loss: 1.1925e-05 - val_loss: 1.1608e-05\n",
      "Epoch 124/200\n",
      "12800/12800 [==============================] - 1s 117us/step - loss: 1.1842e-05 - val_loss: 1.1484e-05\n",
      "Epoch 125/200\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 1.1770e-05 - val_loss: 1.1426e-05\n",
      "Epoch 126/200\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 1.1694e-05 - val_loss: 1.1354e-05\n",
      "Epoch 127/200\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 1.1613e-05 - val_loss: 1.1257e-05\n",
      "Epoch 128/200\n",
      "12800/12800 [==============================] - 1s 105us/step - loss: 1.1541e-05 - val_loss: 1.1249e-05\n",
      "Epoch 129/200\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 1.1468e-05 - val_loss: 1.1132e-05\n",
      "Epoch 130/200\n",
      "12800/12800 [==============================] - 1s 95us/step - loss: 1.1397e-05 - val_loss: 1.1088e-05\n",
      "Epoch 131/200\n",
      "12800/12800 [==============================] - 1s 92us/step - loss: 1.1325e-05 - val_loss: 1.1055e-05\n",
      "Epoch 132/200\n",
      "12800/12800 [==============================] - 1s 100us/step - loss: 1.1256e-05 - val_loss: 1.0943e-05\n",
      "Epoch 133/200\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 1.1187e-05 - val_loss: 1.0853e-05\n",
      "Epoch 134/200\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 1.1119e-05 - val_loss: 1.0787e-05\n",
      "Epoch 135/200\n",
      "12800/12800 [==============================] - 1s 93us/step - loss: 1.1052e-05 - val_loss: 1.0725e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 136/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 1.0986e-05 - val_loss: 1.0798e-05: 0s - loss: 1.11\n",
      "Epoch 137/200\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 1.0924e-05 - val_loss: 1.0609e-05\n",
      "Epoch 138/200\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 1.0862e-05 - val_loss: 1.0600e-05\n",
      "Epoch 139/200\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 1.0801e-05 - val_loss: 1.0479e-05\n",
      "Epoch 140/200\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 1.0740e-05 - val_loss: 1.0402e-05\n",
      "Epoch 141/200\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 1.0676e-05 - val_loss: 1.0395e-05\n",
      "Epoch 142/200\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 1.0615e-05 - val_loss: 1.0283e-05\n",
      "Epoch 143/200\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 1.0555e-05 - val_loss: 1.0240e-05\n",
      "Epoch 144/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 1.0498e-05 - val_loss: 1.0183e-05\n",
      "Epoch 145/200\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 1.0441e-05 - val_loss: 1.0133e-05\n",
      "Epoch 146/200\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 1.0386e-05 - val_loss: 1.0050e-05\n",
      "Epoch 147/200\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 1.0329e-05 - val_loss: 1.0008e-05\n",
      "Epoch 148/200\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 1.0272e-05 - val_loss: 9.9342e-06\n",
      "Epoch 149/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 1.0215e-05 - val_loss: 9.8962e-06\n",
      "Epoch 150/200\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 1.0167e-05 - val_loss: 9.8094e-06\n",
      "Epoch 151/200\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 1.0111e-05 - val_loss: 9.7819e-06\n",
      "Epoch 152/200\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 1.0060e-05 - val_loss: 9.7269e-06\n",
      "Epoch 153/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 1.0010e-05 - val_loss: 9.7072e-06\n",
      "Epoch 154/200\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 9.9563e-06 - val_loss: 9.6446e-06\n",
      "Epoch 155/200\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 9.9054e-06 - val_loss: 9.5897e-06\n",
      "Epoch 156/200\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 9.8577e-06 - val_loss: 9.5627e-06\n",
      "Epoch 157/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 9.8103e-06 - val_loss: 9.5272e-06\n",
      "Epoch 158/200\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 9.7637e-06 - val_loss: 9.4316e-06\n",
      "Epoch 159/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 9.7102e-06 - val_loss: 9.4079e-06\n",
      "Epoch 160/200\n",
      "12800/12800 [==============================] - 1s 72us/step - loss: 9.6671e-06 - val_loss: 9.3491e-06\n",
      "Epoch 161/200\n",
      "12800/12800 [==============================] - 1s 72us/step - loss: 9.6199e-06 - val_loss: 9.2979e-06\n",
      "Epoch 162/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 9.5778e-06 - val_loss: 9.2442e-06\n",
      "Epoch 163/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 9.5285e-06 - val_loss: 9.2204e-06\n",
      "Epoch 164/200\n",
      "12800/12800 [==============================] - 1s 88us/step - loss: 9.4848e-06 - val_loss: 9.1961e-06\n",
      "Epoch 165/200\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 9.4448e-06 - val_loss: 9.1309e-06\n",
      "Epoch 166/200\n",
      "12800/12800 [==============================] - 1s 88us/step - loss: 9.3982e-06 - val_loss: 9.0919e-06\n",
      "Epoch 167/200\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 9.3507e-06 - val_loss: 9.0373e-06\n",
      "Epoch 168/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 9.3129e-06 - val_loss: 9.0202e-06\n",
      "Epoch 169/200\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 9.2682e-06 - val_loss: 8.9636e-06\n",
      "Epoch 170/200\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 9.2293e-06 - val_loss: 8.9248e-06\n",
      "Epoch 171/200\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 9.1867e-06 - val_loss: 8.8845e-06\n",
      "Epoch 172/200\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 9.1473e-06 - val_loss: 8.8476e-06\n",
      "Epoch 173/200\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 9.1075e-06 - val_loss: 8.8010e-06\n",
      "Epoch 174/200\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 9.0700e-06 - val_loss: 8.7755e-06\n",
      "Epoch 175/200\n",
      "12800/12800 [==============================] - 1s 94us/step - loss: 9.0293e-06 - val_loss: 8.7436e-06\n",
      "Epoch 176/200\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 8.9900e-06 - val_loss: 8.7512e-06\n",
      "Epoch 177/200\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 8.9532e-06 - val_loss: 8.6406e-06\n",
      "Epoch 178/200\n",
      "12800/12800 [==============================] - 1s 92us/step - loss: 8.9159e-06 - val_loss: 8.6364e-06\n",
      "Epoch 179/200\n",
      "12800/12800 [==============================] - 2s 163us/step - loss: 8.8784e-06 - val_loss: 8.5673e-06\n",
      "Epoch 180/200\n",
      "12800/12800 [==============================] - 1s 97us/step - loss: 8.8406e-06 - val_loss: 8.5549e-06\n",
      "Epoch 181/200\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 8.8029e-06 - val_loss: 8.5139e-06\n",
      "Epoch 182/200\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 8.7670e-06 - val_loss: 8.4850e-06\n",
      "Epoch 183/200\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 8.7333e-06 - val_loss: 8.4343e-06\n",
      "Epoch 184/200\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 8.6961e-06 - val_loss: 8.4264e-06\n",
      "Epoch 185/200\n",
      "12800/12800 [==============================] - 2s 124us/step - loss: 8.6643e-06 - val_loss: 8.3545e-06\n",
      "Epoch 186/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 8.6274e-06 - val_loss: 8.3372e-06\n",
      "Epoch 187/200\n",
      "12800/12800 [==============================] - 1s 88us/step - loss: 8.5923e-06 - val_loss: 8.3087e-06\n",
      "Epoch 188/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 8.5615e-06 - val_loss: 8.2546e-06\n",
      "Epoch 189/200\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 8.5250e-06 - val_loss: 8.2343e-06\n",
      "Epoch 190/200\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 8.4933e-06 - val_loss: 8.1996e-06\n",
      "Epoch 191/200\n",
      "12800/12800 [==============================] - 1s 72us/step - loss: 8.4598e-06 - val_loss: 8.1848e-06\n",
      "Epoch 192/200\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 8.4279e-06 - val_loss: 8.1271e-06\n",
      "Epoch 193/200\n",
      "12800/12800 [==============================] - 1s 98us/step - loss: 8.3966e-06 - val_loss: 8.0836e-06\n",
      "Epoch 194/200\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 8.3630e-06 - val_loss: 8.0725e-06\n",
      "Epoch 195/200\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 8.3322e-06 - val_loss: 8.0885e-06\n",
      "Epoch 196/200\n",
      "12800/12800 [==============================] - 1s 111us/step - loss: 8.3026e-06 - val_loss: 8.0301e-06\n",
      "Epoch 197/200\n",
      "12800/12800 [==============================] - 1s 100us/step - loss: 8.2709e-06 - val_loss: 7.9758e-06\n",
      "Epoch 198/200\n",
      "12800/12800 [==============================] - 1s 95us/step - loss: 8.2410e-06 - val_loss: 7.9529e-06\n",
      "Epoch 199/200\n",
      "12800/12800 [==============================] - 2s 117us/step - loss: 8.2076e-06 - val_loss: 7.9197e-06\n",
      "Epoch 200/200\n",
      "12800/12800 [==============================] - 2s 124us/step - loss: 8.1785e-06 - val_loss: 7.8900e-06\n",
      "Test MSE: 8.2809631e-06\n",
      "Total MSE: 8.1040552e-06\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input-layer (InputLayer)     (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "hidden-layer-1 (Dense)       (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "hidden-layer-2 (Dense)       (None, 150)               45150     \n",
      "_________________________________________________________________\n",
      "hidden-layer-3 (Dense)       (None, 300)               45300     \n",
      "_________________________________________________________________\n",
      "output-layer (Dense)         (None, 300)               90300     \n",
      "=================================================================\n",
      "Total params: 271,050\n",
      "Trainable params: 271,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/200\n",
      "12800/12800 [==============================] - 3s 198us/step - loss: 0.0114 - val_loss: 0.0022\n",
      "Epoch 2/200\n",
      "12800/12800 [==============================] - 2s 164us/step - loss: 0.0019 - val_loss: 0.0018\n",
      "Epoch 3/200\n",
      "12800/12800 [==============================] - 2s 162us/step - loss: 0.0016 - val_loss: 0.0015\n",
      "Epoch 4/200\n",
      "12800/12800 [==============================] - 2s 169us/step - loss: 0.0014 - val_loss: 0.0013\n",
      "Epoch 5/200\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 6/200\n",
      "12800/12800 [==============================] - 2s 164us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 7/200\n",
      "12800/12800 [==============================] - 2s 155us/step - loss: 0.0010 - val_loss: 9.8691e-04\n",
      "Epoch 8/200\n",
      "12800/12800 [==============================] - 2s 158us/step - loss: 9.3901e-04 - val_loss: 9.1228e-04\n",
      "Epoch 9/200\n",
      "12800/12800 [==============================] - 2s 159us/step - loss: 8.7104e-04 - val_loss: 8.4876e-04\n",
      "Epoch 10/200\n",
      "12800/12800 [==============================] - 2s 162us/step - loss: 8.1308e-04 - val_loss: 7.9295e-04\n",
      "Epoch 11/200\n",
      "12800/12800 [==============================] - 2s 157us/step - loss: 7.6235e-04 - val_loss: 7.4492e-04\n",
      "Epoch 12/200\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 7.1768e-04 - val_loss: 7.0328e-04\n",
      "Epoch 13/200\n",
      "12800/12800 [==============================] - 4s 316us/step - loss: 6.7789e-04 - val_loss: 6.6418e-04\n",
      "Epoch 14/200\n",
      "12800/12800 [==============================] - 3s 266us/step - loss: 6.4210e-04 - val_loss: 6.2970e-04\n",
      "Epoch 15/200\n",
      "12800/12800 [==============================] - 2s 188us/step - loss: 6.0966e-04 - val_loss: 5.9879e-04\n",
      "Epoch 16/200\n",
      "12800/12800 [==============================] - 2s 179us/step - loss: 5.8022e-04 - val_loss: 5.7023e-04\n",
      "Epoch 17/200\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 5.5336e-04 - val_loss: 5.4469e-04\n",
      "Epoch 18/200\n",
      "12800/12800 [==============================] - 2s 186us/step - loss: 5.2871e-04 - val_loss: 5.2075e-04\n",
      "Epoch 19/200\n",
      "12800/12800 [==============================] - 3s 203us/step - loss: 5.0610e-04 - val_loss: 4.9913e-04\n",
      "Epoch 20/200\n",
      "12800/12800 [==============================] - 3s 200us/step - loss: 4.8522e-04 - val_loss: 4.7907e-04\n",
      "Epoch 21/200\n",
      "12800/12800 [==============================] - 2s 163us/step - loss: 4.6597e-04 - val_loss: 4.6027e-04\n",
      "Epoch 22/200\n",
      "12800/12800 [==============================] - 2s 155us/step - loss: 4.4815e-04 - val_loss: 4.4300e-04\n",
      "Epoch 23/200\n",
      "12800/12800 [==============================] - 2s 165us/step - loss: 4.3153e-04 - val_loss: 4.2691e-04\n",
      "Epoch 24/200\n",
      "12800/12800 [==============================] - 2s 152us/step - loss: 4.1615e-04 - val_loss: 4.1184e-04\n",
      "Epoch 25/200\n",
      "12800/12800 [==============================] - 2s 143us/step - loss: 4.0175e-04 - val_loss: 3.9797e-04\n",
      "Epoch 26/200\n",
      "12800/12800 [==============================] - 2s 156us/step - loss: 3.8833e-04 - val_loss: 3.8494e-04\n",
      "Epoch 27/200\n",
      "12800/12800 [==============================] - 2s 137us/step - loss: 3.7580e-04 - val_loss: 3.7301e-04\n",
      "Epoch 28/200\n",
      "12800/12800 [==============================] - 2s 159us/step - loss: 3.6404e-04 - val_loss: 3.6145e-04\n",
      "Epoch 29/200\n",
      "12800/12800 [==============================] - 2s 145us/step - loss: 3.5295e-04 - val_loss: 3.5076e-04\n",
      "Epoch 30/200\n",
      "12800/12800 [==============================] - 2s 161us/step - loss: 3.4255e-04 - val_loss: 3.4050e-04\n",
      "Epoch 31/200\n",
      "12800/12800 [==============================] - 3s 231us/step - loss: 3.3280e-04 - val_loss: 3.3088e-04\n",
      "Epoch 32/200\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 3.2354e-04 - val_loss: 3.2188e-04\n",
      "Epoch 33/200\n",
      "12800/12800 [==============================] - 3s 197us/step - loss: 3.1483e-04 - val_loss: 3.1347e-04\n",
      "Epoch 34/200\n",
      "12800/12800 [==============================] - 2s 174us/step - loss: 3.0654e-04 - val_loss: 3.0533e-04\n",
      "Epoch 35/200\n",
      "12800/12800 [==============================] - 2s 186us/step - loss: 2.9873e-04 - val_loss: 2.9775e-04\n",
      "Epoch 36/200\n",
      "12800/12800 [==============================] - 2s 180us/step - loss: 2.9132e-04 - val_loss: 2.9042e-04\n",
      "Epoch 37/200\n",
      "12800/12800 [==============================] - 2s 180us/step - loss: 2.8428e-04 - val_loss: 2.8394e-04\n",
      "Epoch 38/200\n",
      "12800/12800 [==============================] - 2s 192us/step - loss: 2.7756e-04 - val_loss: 2.7702e-04\n",
      "Epoch 39/200\n",
      "12800/12800 [==============================] - 2s 179us/step - loss: 2.7116e-04 - val_loss: 2.7073e-04\n",
      "Epoch 40/200\n",
      "12800/12800 [==============================] - 2s 187us/step - loss: 2.6511e-04 - val_loss: 2.6484e-04\n",
      "Epoch 41/200\n",
      "12800/12800 [==============================] - 3s 199us/step - loss: 2.5931e-04 - val_loss: 2.5913e-04\n",
      "Epoch 42/200\n",
      "12800/12800 [==============================] - 2s 184us/step - loss: 2.5378e-04 - val_loss: 2.5360e-04\n",
      "Epoch 43/200\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 2.4847e-04 - val_loss: 2.4848e-04\n",
      "Epoch 44/200\n",
      "12800/12800 [==============================] - 2s 167us/step - loss: 2.4340e-04 - val_loss: 2.4347e-04\n",
      "Epoch 45/200\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 2.3855e-04 - val_loss: 2.3861e-04\n",
      "Epoch 46/200\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 2.3390e-04 - val_loss: 2.3419e-04\n",
      "Epoch 47/200\n",
      "12800/12800 [==============================] - 3s 195us/step - loss: 2.2943e-04 - val_loss: 2.2974e-04\n",
      "Epoch 48/200\n",
      "12800/12800 [==============================] - 2s 188us/step - loss: 2.2516e-04 - val_loss: 2.2546e-04\n",
      "Epoch 49/200\n",
      "12800/12800 [==============================] - 2s 182us/step - loss: 2.2105e-04 - val_loss: 2.2146e-04\n",
      "Epoch 50/200\n",
      "12800/12800 [==============================] - 2s 182us/step - loss: 2.1707e-04 - val_loss: 2.1753e-04\n",
      "Epoch 51/200\n",
      "12800/12800 [==============================] - 3s 197us/step - loss: 2.1327e-04 - val_loss: 2.1380e-04\n",
      "Epoch 52/200\n",
      "12800/12800 [==============================] - 2s 179us/step - loss: 2.0962e-04 - val_loss: 2.1022e-04\n",
      "Epoch 53/200\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 2.0609e-04 - val_loss: 2.0681e-04\n",
      "Epoch 54/200\n",
      "12800/12800 [==============================] - 2s 182us/step - loss: 2.0272e-04 - val_loss: 2.0340e-04\n",
      "Epoch 55/200\n",
      "12800/12800 [==============================] - 2s 184us/step - loss: 1.9941e-04 - val_loss: 2.0022e-04\n",
      "Epoch 56/200\n",
      "12800/12800 [==============================] - 3s 213us/step - loss: 1.9624e-04 - val_loss: 1.9705e-04\n",
      "Epoch 57/200\n",
      "12800/12800 [==============================] - 2s 178us/step - loss: 1.9317e-04 - val_loss: 1.9406e-04\n",
      "Epoch 58/200\n",
      "12800/12800 [==============================] - 2s 181us/step - loss: 1.9023e-04 - val_loss: 1.9107e-04\n",
      "Epoch 59/200\n",
      "12800/12800 [==============================] - 2s 177us/step - loss: 1.8737e-04 - val_loss: 1.8829e-04\n",
      "Epoch 60/200\n",
      "12800/12800 [==============================] - 3s 209us/step - loss: 1.8461e-04 - val_loss: 1.8553e-04\n",
      "Epoch 61/200\n",
      "12800/12800 [==============================] - 3s 197us/step - loss: 1.8195e-04 - val_loss: 1.8286e-04\n",
      "Epoch 62/200\n",
      "12800/12800 [==============================] - 3s 216us/step - loss: 1.7935e-04 - val_loss: 1.8031e-04\n",
      "Epoch 63/200\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 1.7685e-04 - val_loss: 1.7780e-04\n",
      "Epoch 64/200\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 1.7443e-04 - val_loss: 1.7546e-04\n",
      "Epoch 65/200\n",
      "12800/12800 [==============================] - 2s 186us/step - loss: 1.7208e-04 - val_loss: 1.7308e-04\n",
      "Epoch 66/200\n",
      "12800/12800 [==============================] - 1s 112us/step - loss: 1.6979e-04 - val_loss: 1.7084e-04\n",
      "Epoch 67/200\n",
      "12800/12800 [==============================] - 1s 117us/step - loss: 1.6757e-04 - val_loss: 1.6868e-04\n",
      "Epoch 68/200\n",
      "12800/12800 [==============================] - 1s 108us/step - loss: 1.6542e-04 - val_loss: 1.6663e-04\n",
      "Epoch 69/200\n",
      "12800/12800 [==============================] - 1s 107us/step - loss: 1.6333e-04 - val_loss: 1.6452e-04\n",
      "Epoch 70/200\n",
      "12800/12800 [==============================] - 1s 110us/step - loss: 1.6131e-04 - val_loss: 1.6244e-04\n",
      "Epoch 71/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 1s 108us/step - loss: 1.5934e-04 - val_loss: 1.6052e-04\n",
      "Epoch 72/200\n",
      "12800/12800 [==============================] - 2s 127us/step - loss: 1.5741e-04 - val_loss: 1.5864e-04\n",
      "Epoch 73/200\n",
      "12800/12800 [==============================] - 1s 107us/step - loss: 1.5556e-04 - val_loss: 1.5682e-04\n",
      "Epoch 74/200\n",
      "12800/12800 [==============================] - 1s 110us/step - loss: 1.5372e-04 - val_loss: 1.5506e-04\n",
      "Epoch 75/200\n",
      "12800/12800 [==============================] - 1s 109us/step - loss: 1.5197e-04 - val_loss: 1.5325e-04\n",
      "Epoch 76/200\n",
      "12800/12800 [==============================] - 1s 107us/step - loss: 1.5024e-04 - val_loss: 1.5161e-04\n",
      "Epoch 77/200\n",
      "12800/12800 [==============================] - 1s 110us/step - loss: 1.4855e-04 - val_loss: 1.4992e-04\n",
      "Epoch 78/200\n",
      "12800/12800 [==============================] - 1s 112us/step - loss: 1.4691e-04 - val_loss: 1.4832e-04\n",
      "Epoch 79/200\n",
      "12800/12800 [==============================] - 2s 119us/step - loss: 1.4533e-04 - val_loss: 1.4668e-04\n",
      "Epoch 80/200\n",
      "12800/12800 [==============================] - 1s 111us/step - loss: 1.4378e-04 - val_loss: 1.4509e-04\n",
      "Epoch 81/200\n",
      "12800/12800 [==============================] - 1s 110us/step - loss: 1.4225e-04 - val_loss: 1.4366e-04\n",
      "Epoch 82/200\n",
      "12800/12800 [==============================] - 1s 109us/step - loss: 1.4078e-04 - val_loss: 1.4219e-04\n",
      "Epoch 83/200\n",
      "12800/12800 [==============================] - 1s 112us/step - loss: 1.3933e-04 - val_loss: 1.4076e-04\n",
      "Epoch 84/200\n",
      "12800/12800 [==============================] - 1s 108us/step - loss: 1.3792e-04 - val_loss: 1.3936e-04\n",
      "Epoch 85/200\n",
      "12800/12800 [==============================] - 1s 109us/step - loss: 1.3654e-04 - val_loss: 1.3803e-04\n",
      "Epoch 86/200\n",
      "12800/12800 [==============================] - 1s 108us/step - loss: 1.3520e-04 - val_loss: 1.3663e-04\n",
      "Epoch 87/200\n",
      "12800/12800 [==============================] - 2s 122us/step - loss: 1.3388e-04 - val_loss: 1.3542e-04\n",
      "Epoch 88/200\n",
      "12800/12800 [==============================] - 2s 121us/step - loss: 1.3258e-04 - val_loss: 1.3410e-04\n",
      "Epoch 89/200\n",
      "12800/12800 [==============================] - 1s 112us/step - loss: 1.3133e-04 - val_loss: 1.3288e-04\n",
      "Epoch 90/200\n",
      "12800/12800 [==============================] - 1s 109us/step - loss: 1.3011e-04 - val_loss: 1.3161e-04\n",
      "Epoch 91/200\n",
      "12800/12800 [==============================] - 1s 109us/step - loss: 1.2890e-04 - val_loss: 1.3042e-04\n",
      "Epoch 92/200\n",
      "12800/12800 [==============================] - 1s 107us/step - loss: 1.2773e-04 - val_loss: 1.2922e-04\n",
      "Epoch 93/200\n",
      "12800/12800 [==============================] - 1s 108us/step - loss: 1.2657e-04 - val_loss: 1.2816e-04\n",
      "Epoch 94/200\n",
      "12800/12800 [==============================] - 1s 107us/step - loss: 1.2545e-04 - val_loss: 1.2703e-04\n",
      "Epoch 95/200\n",
      "12800/12800 [==============================] - 2s 123us/step - loss: 1.2433e-04 - val_loss: 1.2595e-04\n",
      "Epoch 96/200\n",
      "12800/12800 [==============================] - 1s 109us/step - loss: 1.2325e-04 - val_loss: 1.2483e-04\n",
      "Epoch 97/200\n",
      "12800/12800 [==============================] - 1s 108us/step - loss: 1.2219e-04 - val_loss: 1.2386e-04\n",
      "Epoch 98/200\n",
      "12800/12800 [==============================] - 1s 109us/step - loss: 1.2115e-04 - val_loss: 1.2282e-04\n",
      "Epoch 99/200\n",
      "12800/12800 [==============================] - 1s 112us/step - loss: 1.2013e-04 - val_loss: 1.2171e-04\n",
      "Epoch 100/200\n",
      "12800/12800 [==============================] - 1s 108us/step - loss: 1.1912e-04 - val_loss: 1.2074e-04\n",
      "Epoch 101/200\n",
      "12800/12800 [==============================] - 1s 110us/step - loss: 1.1815e-04 - val_loss: 1.1980e-04\n",
      "Epoch 102/200\n",
      "12800/12800 [==============================] - 2s 119us/step - loss: 1.1719e-04 - val_loss: 1.1881e-04\n",
      "Epoch 103/200\n",
      "12800/12800 [==============================] - 1s 111us/step - loss: 1.1625e-04 - val_loss: 1.1788e-04\n",
      "Epoch 104/200\n",
      "12800/12800 [==============================] - 1s 108us/step - loss: 1.1532e-04 - val_loss: 1.1699e-04\n",
      "Epoch 105/200\n",
      "12800/12800 [==============================] - 1s 109us/step - loss: 1.1441e-04 - val_loss: 1.1609e-04\n",
      "Epoch 106/200\n",
      "12800/12800 [==============================] - 1s 107us/step - loss: 1.1350e-04 - val_loss: 1.1524e-04\n",
      "Epoch 107/200\n",
      "12800/12800 [==============================] - 1s 112us/step - loss: 1.1264e-04 - val_loss: 1.1439e-04\n",
      "Epoch 108/200\n",
      "12800/12800 [==============================] - 1s 109us/step - loss: 1.1177e-04 - val_loss: 1.1351e-04\n",
      "Epoch 109/200\n",
      "12800/12800 [==============================] - 2s 132us/step - loss: 1.1093e-04 - val_loss: 1.1267e-04\n",
      "Epoch 110/200\n",
      "12800/12800 [==============================] - 1s 110us/step - loss: 1.1009e-04 - val_loss: 1.1183e-04\n",
      "Epoch 111/200\n",
      "12800/12800 [==============================] - 1s 107us/step - loss: 1.0928e-04 - val_loss: 1.1105e-04\n",
      "Epoch 112/200\n",
      "12800/12800 [==============================] - 1s 109us/step - loss: 1.0847e-04 - val_loss: 1.1023e-04\n",
      "Epoch 113/200\n",
      "12800/12800 [==============================] - 1s 107us/step - loss: 1.0769e-04 - val_loss: 1.0954e-04\n",
      "Epoch 114/200\n",
      "12800/12800 [==============================] - 1s 109us/step - loss: 1.0691e-04 - val_loss: 1.0872e-04\n",
      "Epoch 115/200\n",
      "12800/12800 [==============================] - 1s 108us/step - loss: 1.0616e-04 - val_loss: 1.0795e-04\n",
      "Epoch 116/200\n",
      "12800/12800 [==============================] - 1s 108us/step - loss: 1.0540e-04 - val_loss: 1.0714e-04\n",
      "Epoch 117/200\n",
      "12800/12800 [==============================] - 2s 124us/step - loss: 1.0465e-04 - val_loss: 1.0645e-04\n",
      "Epoch 118/200\n",
      "12800/12800 [==============================] - 1s 107us/step - loss: 1.0394e-04 - val_loss: 1.0573e-04\n",
      "Epoch 119/200\n",
      "12800/12800 [==============================] - 1s 107us/step - loss: 1.0323e-04 - val_loss: 1.0501e-04\n",
      "Epoch 120/200\n",
      "12800/12800 [==============================] - 1s 115us/step - loss: 1.0252e-04 - val_loss: 1.0433e-04\n",
      "Epoch 121/200\n",
      "12800/12800 [==============================] - 1s 111us/step - loss: 1.0183e-04 - val_loss: 1.0364e-04\n",
      "Epoch 122/200\n",
      "12800/12800 [==============================] - 1s 110us/step - loss: 1.0114e-04 - val_loss: 1.0297e-04\n",
      "Epoch 123/200\n",
      "12800/12800 [==============================] - 1s 107us/step - loss: 1.0047e-04 - val_loss: 1.0227e-04\n",
      "Epoch 124/200\n",
      "12800/12800 [==============================] - 2s 126us/step - loss: 9.9806e-05 - val_loss: 1.0164e-04\n",
      "Epoch 125/200\n",
      "12800/12800 [==============================] - 1s 108us/step - loss: 9.9165e-05 - val_loss: 1.0100e-04\n",
      "Epoch 126/200\n",
      "12800/12800 [==============================] - 1s 111us/step - loss: 9.8519e-05 - val_loss: 1.0032e-04\n",
      "Epoch 127/200\n",
      "12800/12800 [==============================] - 1s 113us/step - loss: 9.7883e-05 - val_loss: 9.9744e-05\n",
      "Epoch 128/200\n",
      "12800/12800 [==============================] - 1s 111us/step - loss: 9.7260e-05 - val_loss: 9.9130e-05\n",
      "Epoch 129/200\n",
      "12800/12800 [==============================] - 1s 110us/step - loss: 9.6642e-05 - val_loss: 9.8496e-05\n",
      "Epoch 130/200\n",
      "12800/12800 [==============================] - 1s 112us/step - loss: 9.6039e-05 - val_loss: 9.7896e-05\n",
      "Epoch 131/200\n",
      "12800/12800 [==============================] - 2s 124us/step - loss: 9.5444e-05 - val_loss: 9.7269e-05\n",
      "Epoch 132/200\n",
      "12800/12800 [==============================] - 1s 108us/step - loss: 9.4852e-05 - val_loss: 9.6735e-05\n",
      "Epoch 133/200\n",
      "12800/12800 [==============================] - 1s 114us/step - loss: 9.4283e-05 - val_loss: 9.6130e-05\n",
      "Epoch 134/200\n",
      "12800/12800 [==============================] - 1s 109us/step - loss: 9.3709e-05 - val_loss: 9.5546e-05\n",
      "Epoch 135/200\n",
      "12800/12800 [==============================] - 1s 109us/step - loss: 9.3140e-05 - val_loss: 9.4982e-05\n",
      "Epoch 136/200\n",
      "12800/12800 [==============================] - 1s 110us/step - loss: 9.2581e-05 - val_loss: 9.4490e-05\n",
      "Epoch 137/200\n",
      "12800/12800 [==============================] - 1s 110us/step - loss: 9.2029e-05 - val_loss: 9.3959e-05\n",
      "Epoch 138/200\n",
      "12800/12800 [==============================] - 1s 110us/step - loss: 9.1491e-05 - val_loss: 9.3400e-05\n",
      "Epoch 139/200\n",
      "12800/12800 [==============================] - 2s 126us/step - loss: 9.0963e-05 - val_loss: 9.2820e-05\n",
      "Epoch 140/200\n",
      "12800/12800 [==============================] - 2s 118us/step - loss: 9.0432e-05 - val_loss: 9.2297e-05\n",
      "Epoch 141/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 2s 124us/step - loss: 8.9911e-05 - val_loss: 9.1807e-05\n",
      "Epoch 142/200\n",
      "12800/12800 [==============================] - 1s 114us/step - loss: 8.9390e-05 - val_loss: 9.1279e-05\n",
      "Epoch 143/200\n",
      "12800/12800 [==============================] - 1s 108us/step - loss: 8.8886e-05 - val_loss: 9.0778e-05\n",
      "Epoch 144/200\n",
      "12800/12800 [==============================] - 1s 108us/step - loss: 8.8383e-05 - val_loss: 9.0284e-05\n",
      "Epoch 145/200\n",
      "12800/12800 [==============================] - 1s 109us/step - loss: 8.7883e-05 - val_loss: 8.9834e-05\n",
      "Epoch 146/200\n",
      "12800/12800 [==============================] - 2s 125us/step - loss: 8.7398e-05 - val_loss: 8.9330e-05\n",
      "Epoch 147/200\n",
      "12800/12800 [==============================] - 1s 107us/step - loss: 8.6913e-05 - val_loss: 8.8890e-05\n",
      "Epoch 148/200\n",
      "12800/12800 [==============================] - 1s 114us/step - loss: 8.6443e-05 - val_loss: 8.8352e-05\n",
      "Epoch 149/200\n",
      "12800/12800 [==============================] - 1s 110us/step - loss: 8.5967e-05 - val_loss: 8.7888e-05\n",
      "Epoch 150/200\n",
      "12800/12800 [==============================] - 1s 107us/step - loss: 8.5499e-05 - val_loss: 8.7483e-05\n",
      "Epoch 151/200\n",
      "12800/12800 [==============================] - 1s 111us/step - loss: 8.5046e-05 - val_loss: 8.6976e-05\n",
      "Epoch 152/200\n",
      "12800/12800 [==============================] - 1s 107us/step - loss: 8.4587e-05 - val_loss: 8.6487e-05\n",
      "Epoch 153/200\n",
      "12800/12800 [==============================] - 2s 118us/step - loss: 8.4134e-05 - val_loss: 8.6090e-05\n",
      "Epoch 154/200\n",
      "12800/12800 [==============================] - 1s 114us/step - loss: 8.3693e-05 - val_loss: 8.5609e-05\n",
      "Epoch 155/200\n",
      "12800/12800 [==============================] - 1s 108us/step - loss: 8.3253e-05 - val_loss: 8.5160e-05\n",
      "Epoch 156/200\n",
      "12800/12800 [==============================] - 1s 109us/step - loss: 8.2823e-05 - val_loss: 8.4790e-05\n",
      "Epoch 157/200\n",
      "12800/12800 [==============================] - 1s 116us/step - loss: 8.2388e-05 - val_loss: 8.4381e-05\n",
      "Epoch 158/200\n",
      "12800/12800 [==============================] - 1s 109us/step - loss: 8.1968e-05 - val_loss: 8.3907e-05\n",
      "Epoch 159/200\n",
      "12800/12800 [==============================] - 1s 109us/step - loss: 8.1541e-05 - val_loss: 8.3490e-05\n",
      "Epoch 160/200\n",
      "12800/12800 [==============================] - 1s 114us/step - loss: 8.1136e-05 - val_loss: 8.3114e-05\n",
      "Epoch 161/200\n",
      "12800/12800 [==============================] - 2s 118us/step - loss: 8.0718e-05 - val_loss: 8.2695e-05\n",
      "Epoch 162/200\n",
      "12800/12800 [==============================] - 1s 111us/step - loss: 8.0320e-05 - val_loss: 8.2283e-05\n",
      "Epoch 163/200\n",
      "12800/12800 [==============================] - 1s 108us/step - loss: 7.9913e-05 - val_loss: 8.1878e-05\n",
      "Epoch 164/200\n",
      "12800/12800 [==============================] - 1s 107us/step - loss: 7.9514e-05 - val_loss: 8.1458e-05\n",
      "Epoch 165/200\n",
      "12800/12800 [==============================] - 1s 110us/step - loss: 7.9124e-05 - val_loss: 8.1061e-05\n",
      "Epoch 166/200\n",
      "12800/12800 [==============================] - 1s 111us/step - loss: 7.8729e-05 - val_loss: 8.0721e-05\n",
      "Epoch 167/200\n",
      "12800/12800 [==============================] - 1s 110us/step - loss: 7.8350e-05 - val_loss: 8.0312e-05\n",
      "Epoch 168/200\n",
      "12800/12800 [==============================] - 2s 118us/step - loss: 7.7979e-05 - val_loss: 7.9928e-05\n",
      "Epoch 169/200\n",
      "12800/12800 [==============================] - 2s 133us/step - loss: 7.7596e-05 - val_loss: 7.9612e-05\n",
      "Epoch 170/200\n",
      "12800/12800 [==============================] - 2s 122us/step - loss: 7.7220e-05 - val_loss: 7.9175e-05\n",
      "Epoch 171/200\n",
      "12800/12800 [==============================] - 1s 112us/step - loss: 7.6847e-05 - val_loss: 7.8829e-05\n",
      "Epoch 172/200\n",
      "12800/12800 [==============================] - 1s 113us/step - loss: 7.6491e-05 - val_loss: 7.8483e-05\n",
      "Epoch 173/200\n",
      "12800/12800 [==============================] - 1s 110us/step - loss: 7.6127e-05 - val_loss: 7.8115e-05\n",
      "Epoch 174/200\n",
      "12800/12800 [==============================] - 1s 109us/step - loss: 7.5769e-05 - val_loss: 7.7737e-05\n",
      "Epoch 175/200\n",
      "12800/12800 [==============================] - ETA: 0s - loss: 7.5544e-0 - 2s 126us/step - loss: 7.5413e-05 - val_loss: 7.7415e-05\n",
      "Epoch 176/200\n",
      "12800/12800 [==============================] - 1s 109us/step - loss: 7.5063e-05 - val_loss: 7.7059e-05\n",
      "Epoch 177/200\n",
      "12800/12800 [==============================] - 1s 111us/step - loss: 7.4716e-05 - val_loss: 7.6759e-05\n",
      "Epoch 178/200\n",
      "12800/12800 [==============================] - 1s 109us/step - loss: 7.4375e-05 - val_loss: 7.6358e-05\n",
      "Epoch 179/200\n",
      "12800/12800 [==============================] - 1s 110us/step - loss: 7.4034e-05 - val_loss: 7.6049e-05\n",
      "Epoch 180/200\n",
      "12800/12800 [==============================] - 1s 108us/step - loss: 7.3701e-05 - val_loss: 7.5668e-05\n",
      "Epoch 181/200\n",
      "12800/12800 [==============================] - 1s 107us/step - loss: 7.3358e-05 - val_loss: 7.5349e-05\n",
      "Epoch 182/200\n",
      "12800/12800 [==============================] - 2s 123us/step - loss: 7.3034e-05 - val_loss: 7.5065e-05\n",
      "Epoch 183/200\n",
      "12800/12800 [==============================] - 1s 116us/step - loss: 7.2706e-05 - val_loss: 7.4703e-05\n",
      "Epoch 184/200\n",
      "12800/12800 [==============================] - 1s 108us/step - loss: 7.2378e-05 - val_loss: 7.4406e-05\n",
      "Epoch 185/200\n",
      "12800/12800 [==============================] - 1s 108us/step - loss: 7.2052e-05 - val_loss: 7.4049e-05\n",
      "Epoch 186/200\n",
      "12800/12800 [==============================] - 1s 107us/step - loss: 7.1739e-05 - val_loss: 7.3778e-05\n",
      "Epoch 187/200\n",
      "12800/12800 [==============================] - 1s 108us/step - loss: 7.1424e-05 - val_loss: 7.3453e-05\n",
      "Epoch 188/200\n",
      "12800/12800 [==============================] - 1s 107us/step - loss: 7.1115e-05 - val_loss: 7.3122e-05\n",
      "Epoch 189/200\n",
      "12800/12800 [==============================] - 1s 109us/step - loss: 7.0799e-05 - val_loss: 7.2854e-05\n",
      "Epoch 190/200\n",
      "12800/12800 [==============================] - 2s 122us/step - loss: 7.0493e-05 - val_loss: 7.2475e-05\n",
      "Epoch 191/200\n",
      "12800/12800 [==============================] - 1s 114us/step - loss: 7.0188e-05 - val_loss: 7.2204e-05\n",
      "Epoch 192/200\n",
      "12800/12800 [==============================] - 1s 108us/step - loss: 6.9891e-05 - val_loss: 7.1909e-05\n",
      "Epoch 193/200\n",
      "12800/12800 [==============================] - 1s 111us/step - loss: 6.9592e-05 - val_loss: 7.1593e-05\n",
      "Epoch 194/200\n",
      "12800/12800 [==============================] - 1s 108us/step - loss: 6.9298e-05 - val_loss: 7.1322e-05\n",
      "Epoch 195/200\n",
      "12800/12800 [==============================] - 1s 111us/step - loss: 6.8998e-05 - val_loss: 7.1040e-05\n",
      "Epoch 196/200\n",
      "12800/12800 [==============================] - 1s 108us/step - loss: 6.8710e-05 - val_loss: 7.0755e-05\n",
      "Epoch 197/200\n",
      "12800/12800 [==============================] - 2s 124us/step - loss: 6.8433e-05 - val_loss: 7.0437e-05\n",
      "Epoch 198/200\n",
      "12800/12800 [==============================] - 1s 107us/step - loss: 6.8137e-05 - val_loss: 7.0152e-05\n",
      "Epoch 199/200\n",
      "12800/12800 [==============================] - 1s 110us/step - loss: 6.7855e-05 - val_loss: 6.9872e-05\n",
      "Epoch 200/200\n",
      "12800/12800 [==============================] - 1s 109us/step - loss: 6.7577e-05 - val_loss: 6.9583e-05\n",
      "Test MSE: 6.8162283e-05\n",
      "Total MSE: 6.7852651e-05\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 300)               30300     \n",
      "=================================================================\n",
      "Total params: 60,400\n",
      "Trainable params: 60,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/200\n",
      "12800/12800 [==============================] - 1s 63us/step - loss: 0.0018 - val_loss: 9.1362e-04\n",
      "Epoch 2/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 7.2395e-04 - val_loss: 5.8741e-04\n",
      "Epoch 3/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 5.0836e-04 - val_loss: 4.4253e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/200\n",
      "12800/12800 [==============================] - 1s 61us/step - loss: 3.9539e-04 - val_loss: 3.5465e-04\n",
      "Epoch 5/200\n",
      "12800/12800 [==============================] - 1s 54us/step - loss: 3.2488e-04 - val_loss: 2.9815e-04\n",
      "Epoch 6/200\n",
      "12800/12800 [==============================] - 1s 56us/step - loss: 2.7718e-04 - val_loss: 2.5911e-04\n",
      "Epoch 7/200\n",
      "12800/12800 [==============================] - 1s 54us/step - loss: 2.4294e-04 - val_loss: 2.2928e-04\n",
      "Epoch 8/200\n",
      "12800/12800 [==============================] - 1s 52us/step - loss: 2.1715e-04 - val_loss: 2.0687e-04\n",
      "Epoch 9/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 1.9705e-04 - val_loss: 1.8850e-04\n",
      "Epoch 10/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 1.8091e-04 - val_loss: 1.7463e-04\n",
      "Epoch 11/200\n",
      "12800/12800 [==============================] - 1s 48us/step - loss: 1.6766e-04 - val_loss: 1.6259e-04\n",
      "Epoch 12/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 1.5659e-04 - val_loss: 1.5216e-04\n",
      "Epoch 13/200\n",
      "12800/12800 [==============================] - 1s 52us/step - loss: 1.4718e-04 - val_loss: 1.4352e-04\n",
      "Epoch 14/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 1.3900e-04 - val_loss: 1.3624e-04\n",
      "Epoch 15/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 1.3194e-04 - val_loss: 1.2954e-04\n",
      "Epoch 16/200\n",
      "12800/12800 [==============================] - 1s 48us/step - loss: 1.2567e-04 - val_loss: 1.2348e-04\n",
      "Epoch 17/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 1.2008e-04 - val_loss: 1.1831e-04\n",
      "Epoch 18/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 1.1506e-04 - val_loss: 1.1364e-04\n",
      "Epoch 19/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 1.1050e-04 - val_loss: 1.0942e-04\n",
      "Epoch 20/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 1.0639e-04 - val_loss: 1.0546e-04\n",
      "Epoch 21/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 1.0260e-04 - val_loss: 1.0205e-04\n",
      "Epoch 22/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 9.9162e-05 - val_loss: 9.8626e-05\n",
      "Epoch 23/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 9.5975e-05 - val_loss: 9.5416e-05\n",
      "Epoch 24/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 9.3055e-05 - val_loss: 9.2625e-05\n",
      "Epoch 25/200\n",
      "12800/12800 [==============================] - 1s 53us/step - loss: 9.0312e-05 - val_loss: 9.0117e-05\n",
      "Epoch 26/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 8.7770e-05 - val_loss: 8.7680e-05\n",
      "Epoch 27/200\n",
      "12800/12800 [==============================] - 1s 52us/step - loss: 8.5432e-05 - val_loss: 8.5416e-05\n",
      "Epoch 28/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 8.3194e-05 - val_loss: 8.3180e-05\n",
      "Epoch 29/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 8.1154e-05 - val_loss: 8.1131e-05\n",
      "Epoch 30/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 7.9209e-05 - val_loss: 7.9409e-05\n",
      "Epoch 31/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 7.7405e-05 - val_loss: 7.7640e-05\n",
      "Epoch 32/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 7.5697e-05 - val_loss: 7.5958e-05\n",
      "Epoch 33/200\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 7.4084e-05 - val_loss: 7.4365e-05\n",
      "Epoch 34/200\n",
      "12800/12800 [==============================] - 1s 54us/step - loss: 7.2560e-05 - val_loss: 7.2826e-05\n",
      "Epoch 35/200\n",
      "12800/12800 [==============================] - 1s 52us/step - loss: 7.1120e-05 - val_loss: 7.1519e-05\n",
      "Epoch 36/200\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 6.9761e-05 - val_loss: 7.0159e-05\n",
      "Epoch 37/200\n",
      "12800/12800 [==============================] - 1s 54us/step - loss: 6.8458e-05 - val_loss: 6.8914e-05\n",
      "Epoch 38/200\n",
      "12800/12800 [==============================] - 1s 54us/step - loss: 6.7221e-05 - val_loss: 6.7739e-05\n",
      "Epoch 39/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 6.6039e-05 - val_loss: 6.6530e-05\n",
      "Epoch 40/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 6.4920e-05 - val_loss: 6.5363e-05\n",
      "Epoch 41/200\n",
      "12800/12800 [==============================] - 1s 54us/step - loss: 6.3839e-05 - val_loss: 6.4388e-05\n",
      "Epoch 42/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 6.2822e-05 - val_loss: 6.3414e-05\n",
      "Epoch 43/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 6.1843e-05 - val_loss: 6.2436e-05\n",
      "Epoch 44/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 6.0906e-05 - val_loss: 6.1473e-05\n",
      "Epoch 45/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 5.9993e-05 - val_loss: 6.0570e-05\n",
      "Epoch 46/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 5.9142e-05 - val_loss: 5.9794e-05\n",
      "Epoch 47/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 5.8300e-05 - val_loss: 5.8893e-05\n",
      "Epoch 48/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 5.7495e-05 - val_loss: 5.8190e-05\n",
      "Epoch 49/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 5.6727e-05 - val_loss: 5.7322e-05\n",
      "Epoch 50/200\n",
      "12800/12800 [==============================] - 1s 53us/step - loss: 5.5985e-05 - val_loss: 5.6596e-05\n",
      "Epoch 51/200\n",
      "12800/12800 [==============================] - 1s 54us/step - loss: 5.5257e-05 - val_loss: 5.5997e-05\n",
      "Epoch 52/200\n",
      "12800/12800 [==============================] - 1s 52us/step - loss: 5.4571e-05 - val_loss: 5.5309e-05\n",
      "Epoch 53/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 5.3877e-05 - val_loss: 5.4622e-05\n",
      "Epoch 54/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 5.3252e-05 - val_loss: 5.4005e-05\n",
      "Epoch 55/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 5.2612e-05 - val_loss: 5.3478e-05\n",
      "Epoch 56/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 5.2011e-05 - val_loss: 5.2862e-05\n",
      "Epoch 57/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 5.1421e-05 - val_loss: 5.2154e-05\n",
      "Epoch 58/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 5.0844e-05 - val_loss: 5.1564e-05\n",
      "Epoch 59/200\n",
      "12800/12800 [==============================] - 1s 52us/step - loss: 5.0295e-05 - val_loss: 5.1025e-05\n",
      "Epoch 60/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 4.9745e-05 - val_loss: 5.0494e-05\n",
      "Epoch 61/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 4.9237e-05 - val_loss: 4.9997e-05\n",
      "Epoch 62/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 4.8724e-05 - val_loss: 4.9543e-05\n",
      "Epoch 63/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 4.8241e-05 - val_loss: 4.9007e-05\n",
      "Epoch 64/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 4.7763e-05 - val_loss: 4.8562e-05\n",
      "Epoch 65/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 4.7295e-05 - val_loss: 4.8105e-05\n",
      "Epoch 66/200\n",
      "12800/12800 [==============================] - 1s 48us/step - loss: 4.6847e-05 - val_loss: 4.7650e-05\n",
      "Epoch 67/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 4.6401e-05 - val_loss: 4.7197e-05\n",
      "Epoch 68/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 4.5968e-05 - val_loss: 4.6771e-05\n",
      "Epoch 69/200\n",
      "12800/12800 [==============================] - 1s 52us/step - loss: 4.5548e-05 - val_loss: 4.6362e-05\n",
      "Epoch 70/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 4.5152e-05 - val_loss: 4.5966e-05\n",
      "Epoch 71/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 4.4753e-05 - val_loss: 4.5577e-05\n",
      "Epoch 72/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 4.4371e-05 - val_loss: 4.5189e-05\n",
      "Epoch 73/200\n",
      "12800/12800 [==============================] - 1s 52us/step - loss: 4.3987e-05 - val_loss: 4.4749e-05\n",
      "Epoch 74/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 4.3617e-05 - val_loss: 4.4493e-05\n",
      "Epoch 75/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 1s 53us/step - loss: 4.3269e-05 - val_loss: 4.4070e-05\n",
      "Epoch 76/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 4.2911e-05 - val_loss: 4.3712e-05\n",
      "Epoch 77/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 4.2571e-05 - val_loss: 4.3452e-05\n",
      "Epoch 78/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 4.2236e-05 - val_loss: 4.3031e-05\n",
      "Epoch 79/200\n",
      "12800/12800 [==============================] - 1s 52us/step - loss: 4.1905e-05 - val_loss: 4.2780e-05\n",
      "Epoch 80/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 4.1582e-05 - val_loss: 4.2421e-05\n",
      "Epoch 81/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 4.1271e-05 - val_loss: 4.2061e-05\n",
      "Epoch 82/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 4.0964e-05 - val_loss: 4.1772e-05\n",
      "Epoch 83/200\n",
      "12800/12800 [==============================] - 1s 53us/step - loss: 4.0669e-05 - val_loss: 4.1471e-05\n",
      "Epoch 84/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 4.0372e-05 - val_loss: 4.1212e-05\n",
      "Epoch 85/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 4.0088e-05 - val_loss: 4.0866e-05\n",
      "Epoch 86/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 3.9801e-05 - val_loss: 4.0641e-05\n",
      "Epoch 87/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 3.9532e-05 - val_loss: 4.0343e-05\n",
      "Epoch 88/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 3.9254e-05 - val_loss: 4.0024e-05\n",
      "Epoch 89/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 3.8992e-05 - val_loss: 3.9764e-05\n",
      "Epoch 90/200\n",
      "12800/12800 [==============================] - 1s 48us/step - loss: 3.8732e-05 - val_loss: 3.9543e-05\n",
      "Epoch 91/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 3.8478e-05 - val_loss: 3.9301e-05\n",
      "Epoch 92/200\n",
      "12800/12800 [==============================] - 1s 48us/step - loss: 3.8235e-05 - val_loss: 3.9014e-05\n",
      "Epoch 93/200\n",
      "12800/12800 [==============================] - 1s 54us/step - loss: 3.7981e-05 - val_loss: 3.8821e-05\n",
      "Epoch 94/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 3.7751e-05 - val_loss: 3.8577e-05\n",
      "Epoch 95/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 3.7510e-05 - val_loss: 3.8284e-05\n",
      "Epoch 96/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 3.7285e-05 - val_loss: 3.8103e-05\n",
      "Epoch 97/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 3.7061e-05 - val_loss: 3.7870e-05\n",
      "Epoch 98/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 3.6829e-05 - val_loss: 3.7642e-05\n",
      "Epoch 99/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 3.6606e-05 - val_loss: 3.7396e-05\n",
      "Epoch 100/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 3.6397e-05 - val_loss: 3.7201e-05\n",
      "Epoch 101/200\n",
      "12800/12800 [==============================] - 1s 52us/step - loss: 3.6183e-05 - val_loss: 3.6958e-05\n",
      "Epoch 102/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 3.5971e-05 - val_loss: 3.6810e-05\n",
      "Epoch 103/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 3.5780e-05 - val_loss: 3.6564e-05\n",
      "Epoch 104/200\n",
      "12800/12800 [==============================] - 1s 53us/step - loss: 3.5567e-05 - val_loss: 3.6344e-05\n",
      "Epoch 105/200\n",
      "12800/12800 [==============================] - 1s 54us/step - loss: 3.5374e-05 - val_loss: 3.6163e-05\n",
      "Epoch 106/200\n",
      "12800/12800 [==============================] - 1s 54us/step - loss: 3.5187e-05 - val_loss: 3.5998e-05\n",
      "Epoch 107/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 3.4998e-05 - val_loss: 3.5731e-05\n",
      "Epoch 108/200\n",
      "12800/12800 [==============================] - 1s 52us/step - loss: 3.4808e-05 - val_loss: 3.5570e-05\n",
      "Epoch 109/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 3.4622e-05 - val_loss: 3.5388e-05\n",
      "Epoch 110/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 3.4443e-05 - val_loss: 3.5278e-05\n",
      "Epoch 111/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 3.4260e-05 - val_loss: 3.5045e-05\n",
      "Epoch 112/200\n",
      "12800/12800 [==============================] - 1s 48us/step - loss: 3.4087e-05 - val_loss: 3.4913e-05\n",
      "Epoch 113/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 3.3914e-05 - val_loss: 3.4690e-05\n",
      "Epoch 114/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 3.3742e-05 - val_loss: 3.4555e-05\n",
      "Epoch 115/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 3.3583e-05 - val_loss: 3.4322e-05\n",
      "Epoch 116/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 3.3414e-05 - val_loss: 3.4217e-05\n",
      "Epoch 117/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 3.3248e-05 - val_loss: 3.4066e-05\n",
      "Epoch 118/200\n",
      "12800/12800 [==============================] - 1s 52us/step - loss: 3.3081e-05 - val_loss: 3.3922e-05\n",
      "Epoch 119/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 3.2939e-05 - val_loss: 3.3707e-05\n",
      "Epoch 120/200\n",
      "12800/12800 [==============================] - 1s 54us/step - loss: 3.2776e-05 - val_loss: 3.3497e-05\n",
      "Epoch 121/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 3.2624e-05 - val_loss: 3.3415e-05\n",
      "Epoch 122/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 3.2474e-05 - val_loss: 3.3280e-05\n",
      "Epoch 123/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 3.2339e-05 - val_loss: 3.3066e-05\n",
      "Epoch 124/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 3.2191e-05 - val_loss: 3.2896e-05\n",
      "Epoch 125/200\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 3.2040e-05 - val_loss: 3.2747e-05\n",
      "Epoch 126/200\n",
      "12800/12800 [==============================] - 1s 48us/step - loss: 3.1897e-05 - val_loss: 3.2646e-05\n",
      "Epoch 127/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 3.1757e-05 - val_loss: 3.2521e-05\n",
      "Epoch 128/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 3.1624e-05 - val_loss: 3.2384e-05\n",
      "Epoch 129/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 3.1478e-05 - val_loss: 3.2271e-05\n",
      "Epoch 130/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 3.1347e-05 - val_loss: 3.2138e-05\n",
      "Epoch 131/200\n",
      "12800/12800 [==============================] - 1s 48us/step - loss: 3.1218e-05 - val_loss: 3.1954e-05\n",
      "Epoch 132/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 3.1091e-05 - val_loss: 3.1812e-05\n",
      "Epoch 133/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 3.0961e-05 - val_loss: 3.1734e-05\n",
      "Epoch 134/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 3.0837e-05 - val_loss: 3.1593e-05\n",
      "Epoch 135/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 3.0709e-05 - val_loss: 3.1487e-05\n",
      "Epoch 136/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 3.0593e-05 - val_loss: 3.1369e-05\n",
      "Epoch 137/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 3.0472e-05 - val_loss: 3.1168e-05\n",
      "Epoch 138/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 3.0346e-05 - val_loss: 3.1101e-05\n",
      "Epoch 139/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 3.0236e-05 - val_loss: 3.0917e-05\n",
      "Epoch 140/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 3.0113e-05 - val_loss: 3.0844e-05\n",
      "Epoch 141/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 2.9998e-05 - val_loss: 3.0684e-05\n",
      "Epoch 142/200\n",
      "12800/12800 [==============================] - 1s 52us/step - loss: 2.9881e-05 - val_loss: 3.0595e-05\n",
      "Epoch 143/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 2.9770e-05 - val_loss: 3.0470e-05\n",
      "Epoch 144/200\n",
      "12800/12800 [==============================] - 1s 52us/step - loss: 2.9659e-05 - val_loss: 3.0376e-05\n",
      "Epoch 145/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 2.9551e-05 - val_loss: 3.0240e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 146/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 2.9438e-05 - val_loss: 3.0141e-05\n",
      "Epoch 147/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 2.9332e-05 - val_loss: 3.0040e-05\n",
      "Epoch 148/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 2.9226e-05 - val_loss: 2.9935e-05\n",
      "Epoch 149/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 2.9127e-05 - val_loss: 2.9815e-05\n",
      "Epoch 150/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 2.9020e-05 - val_loss: 2.9738e-05\n",
      "Epoch 151/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 2.8922e-05 - val_loss: 2.9598e-05\n",
      "Epoch 152/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 2.8821e-05 - val_loss: 2.9541e-05\n",
      "Epoch 153/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 2.8725e-05 - val_loss: 2.9394e-05\n",
      "Epoch 154/200\n",
      "12800/12800 [==============================] - 1s 54us/step - loss: 2.8629e-05 - val_loss: 2.9315e-05\n",
      "Epoch 155/200\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 2.8529e-05 - val_loss: 2.9219e-05\n",
      "Epoch 156/200\n",
      "12800/12800 [==============================] - 1s 54us/step - loss: 2.8435e-05 - val_loss: 2.9148e-05\n",
      "Epoch 157/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 2.8338e-05 - val_loss: 2.9064e-05\n",
      "Epoch 158/200\n",
      "12800/12800 [==============================] - 1s 53us/step - loss: 2.8249e-05 - val_loss: 2.8960e-05\n",
      "Epoch 159/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 2.8153e-05 - val_loss: 2.8817e-05\n",
      "Epoch 160/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 2.8057e-05 - val_loss: 2.8738e-05\n",
      "Epoch 161/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 2.7974e-05 - val_loss: 2.8671e-05\n",
      "Epoch 162/200\n",
      "12800/12800 [==============================] - 1s 52us/step - loss: 2.7882e-05 - val_loss: 2.8582e-05\n",
      "Epoch 163/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 2.7793e-05 - val_loss: 2.8538e-05\n",
      "Epoch 164/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 2.7710e-05 - val_loss: 2.8391e-05\n",
      "Epoch 165/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 2.7624e-05 - val_loss: 2.8299e-05\n",
      "Epoch 166/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 2.7537e-05 - val_loss: 2.8274e-05\n",
      "Epoch 167/200\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 2.7451e-05 - val_loss: 2.8187e-05\n",
      "Epoch 168/200\n",
      "12800/12800 [==============================] - 1s 53us/step - loss: 2.7367e-05 - val_loss: 2.8061e-05\n",
      "Epoch 169/200\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 2.7284e-05 - val_loss: 2.7958e-05\n",
      "Epoch 170/200\n",
      "12800/12800 [==============================] - 1s 53us/step - loss: 2.7205e-05 - val_loss: 2.7850e-05\n",
      "Epoch 171/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 2.7125e-05 - val_loss: 2.7815e-05\n",
      "Epoch 172/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 2.7048e-05 - val_loss: 2.7730e-05\n",
      "Epoch 173/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 2.6968e-05 - val_loss: 2.7634e-05\n",
      "Epoch 174/200\n",
      "12800/12800 [==============================] - 1s 48us/step - loss: 2.6890e-05 - val_loss: 2.7554e-05\n",
      "Epoch 175/200\n",
      "12800/12800 [==============================] - 1s 52us/step - loss: 2.6812e-05 - val_loss: 2.7482e-05\n",
      "Epoch 176/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 2.6736e-05 - val_loss: 2.7404e-05\n",
      "Epoch 177/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 2.6662e-05 - val_loss: 2.7345e-05\n",
      "Epoch 178/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 2.6583e-05 - val_loss: 2.7285e-05\n",
      "Epoch 179/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 2.6508e-05 - val_loss: 2.7164e-05\n",
      "Epoch 180/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 2.6435e-05 - val_loss: 2.7133e-05\n",
      "Epoch 181/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 2.6364e-05 - val_loss: 2.7065e-05\n",
      "Epoch 182/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 2.6297e-05 - val_loss: 2.6970e-05\n",
      "Epoch 183/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 2.6227e-05 - val_loss: 2.6882e-05\n",
      "Epoch 184/200\n",
      "12800/12800 [==============================] - 1s 52us/step - loss: 2.6151e-05 - val_loss: 2.6867e-05\n",
      "Epoch 185/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 2.6082e-05 - val_loss: 2.6769e-05\n",
      "Epoch 186/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 2.6020e-05 - val_loss: 2.6669e-05\n",
      "Epoch 187/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 2.5948e-05 - val_loss: 2.6639e-05\n",
      "Epoch 188/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 2.5880e-05 - val_loss: 2.6548e-05\n",
      "Epoch 189/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 2.5813e-05 - val_loss: 2.6474e-05\n",
      "Epoch 190/200\n",
      "12800/12800 [==============================] - 1s 56us/step - loss: 2.5745e-05 - val_loss: 2.6440e-05\n",
      "Epoch 191/200\n",
      "12800/12800 [==============================] - 1s 53us/step - loss: 2.5679e-05 - val_loss: 2.6396e-05\n",
      "Epoch 192/200\n",
      "12800/12800 [==============================] - 1s 53us/step - loss: 2.5618e-05 - val_loss: 2.6282e-05\n",
      "Epoch 193/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 2.5551e-05 - val_loss: 2.6196e-05\n",
      "Epoch 194/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 2.5488e-05 - val_loss: 2.6134e-05\n",
      "Epoch 195/200\n",
      "12800/12800 [==============================] - 1s 48us/step - loss: 2.5424e-05 - val_loss: 2.6074e-05\n",
      "Epoch 196/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 2.5366e-05 - val_loss: 2.5992e-05\n",
      "Epoch 197/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 2.5304e-05 - val_loss: 2.5959e-05\n",
      "Epoch 198/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 2.5240e-05 - val_loss: 2.5934e-05\n",
      "Epoch 199/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 2.5182e-05 - val_loss: 2.5812e-05\n",
      "Epoch 200/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 2.5123e-05 - val_loss: 2.5770e-05\n",
      "Test MSE: 2.7370835e-05\n",
      "Total MSE: 2.5589075e-05\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input-layer (InputLayer)     (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "hidden-layer-1 (Dense)       (None, 200)               60200     \n",
      "_________________________________________________________________\n",
      "hidden-layer-2 (Dense)       (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "hidden-layer-3 (Dense)       (None, 200)               20200     \n",
      "_________________________________________________________________\n",
      "output-layer (Dense)         (None, 300)               60300     \n",
      "=================================================================\n",
      "Total params: 160,800\n",
      "Trainable params: 160,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/200\n",
      "12800/12800 [==============================] - 1s 101us/step - loss: 0.0034 - val_loss: 0.0014\n",
      "Epoch 2/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 3/200\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 9.4305e-04 - val_loss: 8.5070e-04\n",
      "Epoch 4/200\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 7.7768e-04 - val_loss: 7.1409e-04\n",
      "Epoch 5/200\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 6.6183e-04 - val_loss: 6.1468e-04\n",
      "Epoch 6/200\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 5.7569e-04 - val_loss: 5.4068e-04\n",
      "Epoch 7/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 1s 85us/step - loss: 5.0946e-04 - val_loss: 4.8092e-04\n",
      "Epoch 8/200\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 4.5690e-04 - val_loss: 4.3418e-04\n",
      "Epoch 9/200\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 4.1433e-04 - val_loss: 3.9523e-04\n",
      "Epoch 10/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 3.7893e-04 - val_loss: 3.6324e-04\n",
      "Epoch 11/200\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 3.4938e-04 - val_loss: 3.3590e-04\n",
      "Epoch 12/200\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 3.2433e-04 - val_loss: 3.1209e-04\n",
      "Epoch 13/200\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 3.0282e-04 - val_loss: 2.9264e-04\n",
      "Epoch 14/200\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 2.8438e-04 - val_loss: 2.7532e-04\n",
      "Epoch 15/200\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 2.6827e-04 - val_loss: 2.6035e-04\n",
      "Epoch 16/200\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 2.5416e-04 - val_loss: 2.4700e-04\n",
      "Epoch 17/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 2.4171e-04 - val_loss: 2.3518e-04\n",
      "Epoch 18/200\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 2.3066e-04 - val_loss: 2.2487e-04\n",
      "Epoch 19/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 2.2062e-04 - val_loss: 2.1531e-04\n",
      "Epoch 20/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 2.1173e-04 - val_loss: 2.0660e-04\n",
      "Epoch 21/200\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 2.0360e-04 - val_loss: 1.9923e-04\n",
      "Epoch 22/200\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 1.9622e-04 - val_loss: 1.9180e-04\n",
      "Epoch 23/200\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 1.8944e-04 - val_loss: 1.8530e-04\n",
      "Epoch 24/200\n",
      "12800/12800 [==============================] - 1s 92us/step - loss: 1.8319e-04 - val_loss: 1.7967e-04\n",
      "Epoch 25/200\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 1.7749e-04 - val_loss: 1.7413e-04\n",
      "Epoch 26/200\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 1.7217e-04 - val_loss: 1.6898e-04\n",
      "Epoch 27/200\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 1.6726e-04 - val_loss: 1.6423e-04\n",
      "Epoch 28/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 1.6268e-04 - val_loss: 1.6012e-04\n",
      "Epoch 29/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 1.5842e-04 - val_loss: 1.5563e-04\n",
      "Epoch 30/200\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 1.5440e-04 - val_loss: 1.5176e-04\n",
      "Epoch 31/200\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 1.5064e-04 - val_loss: 1.4810e-04\n",
      "Epoch 32/200\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 1.4712e-04 - val_loss: 1.4490e-04\n",
      "Epoch 33/200\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 1.4381e-04 - val_loss: 1.4172e-04\n",
      "Epoch 34/200\n",
      "12800/12800 [==============================] - 1s 92us/step - loss: 1.4066e-04 - val_loss: 1.3859e-04\n",
      "Epoch 35/200\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 1.3771e-04 - val_loss: 1.3543e-04\n",
      "Epoch 36/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 1.3489e-04 - val_loss: 1.3276e-04\n",
      "Epoch 37/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 1.3223e-04 - val_loss: 1.3006e-04\n",
      "Epoch 38/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 1.2968e-04 - val_loss: 1.2778e-04\n",
      "Epoch 39/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 1.2725e-04 - val_loss: 1.2545e-04\n",
      "Epoch 40/200\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 1.2494e-04 - val_loss: 1.2311e-04\n",
      "Epoch 41/200\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 1.2274e-04 - val_loss: 1.2097e-04\n",
      "Epoch 42/200\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 1.2062e-04 - val_loss: 1.1905e-04\n",
      "Epoch 43/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 1.1863e-04 - val_loss: 1.1686e-04\n",
      "Epoch 44/200\n",
      "12800/12800 [==============================] - 1s 93us/step - loss: 1.1669e-04 - val_loss: 1.1502e-04\n",
      "Epoch 45/200\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 1.1483e-04 - val_loss: 1.1320e-04\n",
      "Epoch 46/200\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 1.1304e-04 - val_loss: 1.1140e-04\n",
      "Epoch 47/200\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 1.1132e-04 - val_loss: 1.0968e-04\n",
      "Epoch 48/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 1.0969e-04 - val_loss: 1.0805e-04\n",
      "Epoch 49/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 1.0810e-04 - val_loss: 1.0654e-04\n",
      "Epoch 50/200\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 1.0655e-04 - val_loss: 1.0504e-04\n",
      "Epoch 51/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 1.0507e-04 - val_loss: 1.0338e-04\n",
      "Epoch 52/200\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 1.0366e-04 - val_loss: 1.0203e-04\n",
      "Epoch 53/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 1.0228e-04 - val_loss: 1.0073e-04\n",
      "Epoch 54/200\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 1.0094e-04 - val_loss: 9.9323e-05\n",
      "Epoch 55/200\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 9.9622e-05 - val_loss: 9.8226e-05\n",
      "Epoch 56/200\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 9.8383e-05 - val_loss: 9.6943e-05\n",
      "Epoch 57/200\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 9.7175e-05 - val_loss: 9.5648e-05\n",
      "Epoch 58/200\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 9.5975e-05 - val_loss: 9.4562e-05\n",
      "Epoch 59/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 9.4835e-05 - val_loss: 9.3472e-05\n",
      "Epoch 60/200\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 9.3731e-05 - val_loss: 9.2308e-05\n",
      "Epoch 61/200\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 9.2651e-05 - val_loss: 9.1293e-05\n",
      "Epoch 62/200\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 9.1596e-05 - val_loss: 9.0162e-05\n",
      "Epoch 63/200\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 9.0591e-05 - val_loss: 8.9124e-05\n",
      "Epoch 64/200\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 8.9589e-05 - val_loss: 8.8192e-05\n",
      "Epoch 65/200\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 8.8620e-05 - val_loss: 8.7287e-05\n",
      "Epoch 66/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 8.7696e-05 - val_loss: 8.6343e-05\n",
      "Epoch 67/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 8.6779e-05 - val_loss: 8.5436e-05\n",
      "Epoch 68/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 8.5872e-05 - val_loss: 8.4651e-05\n",
      "Epoch 69/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 8.5025e-05 - val_loss: 8.3649e-05\n",
      "Epoch 70/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 8.4158e-05 - val_loss: 8.2944e-05\n",
      "Epoch 71/200\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 8.3335e-05 - val_loss: 8.1989e-05\n",
      "Epoch 72/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 8.2535e-05 - val_loss: 8.1334e-05\n",
      "Epoch 73/200\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 8.1752e-05 - val_loss: 8.0444e-05\n",
      "Epoch 74/200\n",
      "12800/12800 [==============================] - 1s 111us/step - loss: 8.0983e-05 - val_loss: 7.9720e-05\n",
      "Epoch 75/200\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 8.0241e-05 - val_loss: 7.8906e-05\n",
      "Epoch 76/200\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 7.9501e-05 - val_loss: 7.8394e-05\n",
      "Epoch 77/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 7.8784e-05 - val_loss: 7.7635e-05\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 1s 82us/step - loss: 7.8087e-05 - val_loss: 7.6798e-05\n",
      "Epoch 79/200\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 7.7403e-05 - val_loss: 7.6200e-05\n",
      "Epoch 80/200\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 7.6728e-05 - val_loss: 7.5506e-05\n",
      "Epoch 81/200\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 7.6088e-05 - val_loss: 7.4828e-05\n",
      "Epoch 82/200\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 7.5441e-05 - val_loss: 7.4286e-05\n",
      "Epoch 83/200\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 7.4818e-05 - val_loss: 7.3581e-05\n",
      "Epoch 84/200\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 7.4202e-05 - val_loss: 7.2971e-05\n",
      "Epoch 85/200\n",
      "12800/12800 [==============================] - 1s 95us/step - loss: 7.3608e-05 - val_loss: 7.2416e-05\n",
      "Epoch 86/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 7.3021e-05 - val_loss: 7.1834e-05\n",
      "Epoch 87/200\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 7.2435e-05 - val_loss: 7.1260e-05\n",
      "Epoch 88/200\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 7.1877e-05 - val_loss: 7.0670e-05\n",
      "Epoch 89/200\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 7.1311e-05 - val_loss: 7.0171e-05\n",
      "Epoch 90/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 7.0778e-05 - val_loss: 6.9653e-05\n",
      "Epoch 91/200\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 7.0245e-05 - val_loss: 6.9138e-05\n",
      "Epoch 92/200\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 6.9721e-05 - val_loss: 6.8558e-05\n",
      "Epoch 93/200\n",
      "12800/12800 [==============================] - 1s 88us/step - loss: 6.9203e-05 - val_loss: 6.8111e-05\n",
      "Epoch 94/200\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 6.8700e-05 - val_loss: 6.7531e-05\n",
      "Epoch 95/200\n",
      "12800/12800 [==============================] - 1s 97us/step - loss: 6.8202e-05 - val_loss: 6.7042e-05\n",
      "Epoch 96/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 6.7723e-05 - val_loss: 6.6545e-05\n",
      "Epoch 97/200\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 6.7238e-05 - val_loss: 6.6167e-05\n",
      "Epoch 98/200\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 6.6778e-05 - val_loss: 6.5707e-05\n",
      "Epoch 99/200\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 6.6310e-05 - val_loss: 6.5236e-05\n",
      "Epoch 100/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 6.5855e-05 - val_loss: 6.4793e-05\n",
      "Epoch 101/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 6.5418e-05 - val_loss: 6.4246e-05\n",
      "Epoch 102/200\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 6.4975e-05 - val_loss: 6.3906e-05\n",
      "Epoch 103/200\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 6.4545e-05 - val_loss: 6.3481e-05\n",
      "Epoch 104/200\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 6.4116e-05 - val_loss: 6.3020e-05\n",
      "Epoch 105/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 6.3703e-05 - val_loss: 6.2652e-05\n",
      "Epoch 106/200\n",
      "12800/12800 [==============================] - 1s 92us/step - loss: 6.3285e-05 - val_loss: 6.2239e-05\n",
      "Epoch 107/200\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 6.2884e-05 - val_loss: 6.1807e-05\n",
      "Epoch 108/200\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 6.2490e-05 - val_loss: 6.1424e-05\n",
      "Epoch 109/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 6.2088e-05 - val_loss: 6.1112e-05\n",
      "Epoch 110/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 6.1707e-05 - val_loss: 6.0712e-05\n",
      "Epoch 111/200\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 6.1327e-05 - val_loss: 6.0289e-05\n",
      "Epoch 112/200\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 6.0955e-05 - val_loss: 5.9927e-05\n",
      "Epoch 113/200\n",
      "12800/12800 [==============================] - 1s 100us/step - loss: 6.0586e-05 - val_loss: 5.9605e-05\n",
      "Epoch 114/200\n",
      "12800/12800 [==============================] - 1s 106us/step - loss: 6.0221e-05 - val_loss: 5.9194e-05\n",
      "Epoch 115/200\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 5.9868e-05 - val_loss: 5.8828e-05\n",
      "Epoch 116/200\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 5.9503e-05 - val_loss: 5.8471e-05\n",
      "Epoch 117/200\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 5.9159e-05 - val_loss: 5.8165e-05\n",
      "Epoch 118/200\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 5.8817e-05 - val_loss: 5.7809e-05\n",
      "Epoch 119/200\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 5.8468e-05 - val_loss: 5.7537e-05\n",
      "Epoch 120/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 5.8139e-05 - val_loss: 5.7179e-05\n",
      "Epoch 121/200\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 5.7819e-05 - val_loss: 5.6831e-05\n",
      "Epoch 122/200\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 5.7478e-05 - val_loss: 5.6515e-05\n",
      "Epoch 123/200\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 5.7154e-05 - val_loss: 5.6221e-05\n",
      "Epoch 124/200\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 5.6844e-05 - val_loss: 5.5826e-05\n",
      "Epoch 125/200\n",
      "12800/12800 [==============================] - 1s 98us/step - loss: 5.6524e-05 - val_loss: 5.5596e-05\n",
      "Epoch 126/200\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 5.6235e-05 - val_loss: 5.5248e-05\n",
      "Epoch 127/200\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 5.5921e-05 - val_loss: 5.4995e-05\n",
      "Epoch 128/200\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 5.5608e-05 - val_loss: 5.4636e-05\n",
      "Epoch 129/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 5.5318e-05 - val_loss: 5.4418e-05\n",
      "Epoch 130/200\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 5.5027e-05 - val_loss: 5.4048e-05\n",
      "Epoch 131/200\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 5.4733e-05 - val_loss: 5.3876e-05\n",
      "Epoch 132/200\n",
      "12800/12800 [==============================] - 2s 168us/step - loss: 5.4451e-05 - val_loss: 5.3491e-05\n",
      "Epoch 133/200\n",
      "12800/12800 [==============================] - 2s 191us/step - loss: 5.4169e-05 - val_loss: 5.3229e-05\n",
      "Epoch 134/200\n",
      "12800/12800 [==============================] - 2s 176us/step - loss: 5.3894e-05 - val_loss: 5.2927e-05\n",
      "Epoch 135/200\n",
      "12800/12800 [==============================] - 2s 145us/step - loss: 5.3618e-05 - val_loss: 5.2668e-05\n",
      "Epoch 136/200\n",
      "12800/12800 [==============================] - 2s 144us/step - loss: 5.3345e-05 - val_loss: 5.2439e-05\n",
      "Epoch 137/200\n",
      "12800/12800 [==============================] - 2s 136us/step - loss: 5.3091e-05 - val_loss: 5.2134e-05\n",
      "Epoch 138/200\n",
      "12800/12800 [==============================] - 2s 160us/step - loss: 5.2819e-05 - val_loss: 5.1877e-05\n",
      "Epoch 139/200\n",
      "12800/12800 [==============================] - 2s 136us/step - loss: 5.2553e-05 - val_loss: 5.1627e-05\n",
      "Epoch 140/200\n",
      "12800/12800 [==============================] - 2s 146us/step - loss: 5.2291e-05 - val_loss: 5.1433e-05\n",
      "Epoch 141/200\n",
      "12800/12800 [==============================] - 2s 139us/step - loss: 5.2036e-05 - val_loss: 5.1136e-05\n",
      "Epoch 142/200\n",
      "12800/12800 [==============================] - 2s 147us/step - loss: 5.1783e-05 - val_loss: 5.0864e-05\n",
      "Epoch 143/200\n",
      "12800/12800 [==============================] - 2s 148us/step - loss: 5.1527e-05 - val_loss: 5.0625e-05\n",
      "Epoch 144/200\n",
      "12800/12800 [==============================] - 2s 161us/step - loss: 5.1283e-05 - val_loss: 5.0401e-05\n",
      "Epoch 145/200\n",
      "12800/12800 [==============================] - 2s 133us/step - loss: 5.1052e-05 - val_loss: 5.0136e-05\n",
      "Epoch 146/200\n",
      "12800/12800 [==============================] - 2s 132us/step - loss: 5.0803e-05 - val_loss: 4.9943e-05\n",
      "Epoch 147/200\n",
      "12800/12800 [==============================] - 2s 133us/step - loss: 5.0555e-05 - val_loss: 4.9683e-05\n",
      "Epoch 148/200\n",
      "12800/12800 [==============================] - 2s 156us/step - loss: 5.0330e-05 - val_loss: 4.9392e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 149/200\n",
      "12800/12800 [==============================] - 1s 113us/step - loss: 5.0088e-05 - val_loss: 4.9156e-05\n",
      "Epoch 150/200\n",
      "12800/12800 [==============================] - 2s 123us/step - loss: 4.9858e-05 - val_loss: 4.8970e-05\n",
      "Epoch 151/200\n",
      "12800/12800 [==============================] - 2s 127us/step - loss: 4.9644e-05 - val_loss: 4.8776e-05\n",
      "Epoch 152/200\n",
      "12800/12800 [==============================] - 2s 148us/step - loss: 4.9413e-05 - val_loss: 4.8534e-05\n",
      "Epoch 153/200\n",
      "12800/12800 [==============================] - 2s 157us/step - loss: 4.9193e-05 - val_loss: 4.8295e-05\n",
      "Epoch 154/200\n",
      "12800/12800 [==============================] - 2s 151us/step - loss: 4.8973e-05 - val_loss: 4.8043e-05\n",
      "Epoch 155/200\n",
      "12800/12800 [==============================] - 2s 148us/step - loss: 4.8753e-05 - val_loss: 4.7883e-05\n",
      "Epoch 156/200\n",
      "12800/12800 [==============================] - 2s 140us/step - loss: 4.8538e-05 - val_loss: 4.7658e-05\n",
      "Epoch 157/200\n",
      "12800/12800 [==============================] - 2s 136us/step - loss: 4.8322e-05 - val_loss: 4.7470e-05\n",
      "Epoch 158/200\n",
      "12800/12800 [==============================] - 2s 135us/step - loss: 4.8113e-05 - val_loss: 4.7237e-05\n",
      "Epoch 159/200\n",
      "12800/12800 [==============================] - 1s 107us/step - loss: 4.7902e-05 - val_loss: 4.7059e-05\n",
      "Epoch 160/200\n",
      "12800/12800 [==============================] - 2s 136us/step - loss: 4.7704e-05 - val_loss: 4.6829e-05\n",
      "Epoch 161/200\n",
      "12800/12800 [==============================] - 2s 121us/step - loss: 4.7503e-05 - val_loss: 4.6667e-05\n",
      "Epoch 162/200\n",
      "12800/12800 [==============================] - 2s 132us/step - loss: 4.7296e-05 - val_loss: 4.6424e-05\n",
      "Epoch 163/200\n",
      "12800/12800 [==============================] - 2s 127us/step - loss: 4.7094e-05 - val_loss: 4.6311e-05\n",
      "Epoch 164/200\n",
      "12800/12800 [==============================] - 1s 107us/step - loss: 4.6907e-05 - val_loss: 4.6086e-05\n",
      "Epoch 165/200\n",
      "12800/12800 [==============================] - 2s 121us/step - loss: 4.6710e-05 - val_loss: 4.5854e-05\n",
      "Epoch 166/200\n",
      "12800/12800 [==============================] - 2s 122us/step - loss: 4.6501e-05 - val_loss: 4.5747e-05\n",
      "Epoch 167/200\n",
      "12800/12800 [==============================] - 2s 121us/step - loss: 4.6330e-05 - val_loss: 4.5483e-05\n",
      "Epoch 168/200\n",
      "12800/12800 [==============================] - 2s 122us/step - loss: 4.6140e-05 - val_loss: 4.5282e-05\n",
      "Epoch 169/200\n",
      "12800/12800 [==============================] - 2s 140us/step - loss: 4.5958e-05 - val_loss: 4.5056e-05\n",
      "Epoch 170/200\n",
      "12800/12800 [==============================] - 2s 138us/step - loss: 4.5764e-05 - val_loss: 4.4957e-05\n",
      "Epoch 171/200\n",
      "12800/12800 [==============================] - 2s 132us/step - loss: 4.5588e-05 - val_loss: 4.4731e-05\n",
      "Epoch 172/200\n",
      "12800/12800 [==============================] - 2s 124us/step - loss: 4.5411e-05 - val_loss: 4.4643e-05\n",
      "Epoch 173/200\n",
      "12800/12800 [==============================] - 1s 109us/step - loss: 4.5228e-05 - val_loss: 4.4419e-05\n",
      "Epoch 174/200\n",
      "12800/12800 [==============================] - 1s 114us/step - loss: 4.5053e-05 - val_loss: 4.4239e-05\n",
      "Epoch 175/200\n",
      "12800/12800 [==============================] - 1s 107us/step - loss: 4.4880e-05 - val_loss: 4.4076e-05\n",
      "Epoch 176/200\n",
      "12800/12800 [==============================] - 2s 150us/step - loss: 4.4708e-05 - val_loss: 4.3883e-05\n",
      "Epoch 177/200\n",
      "12800/12800 [==============================] - 1s 115us/step - loss: 4.4541e-05 - val_loss: 4.3709e-05\n",
      "Epoch 178/200\n",
      "12800/12800 [==============================] - 2s 118us/step - loss: 4.4365e-05 - val_loss: 4.3582e-05\n",
      "Epoch 179/200\n",
      "12800/12800 [==============================] - 2s 159us/step - loss: 4.4199e-05 - val_loss: 4.3363e-05\n",
      "Epoch 180/200\n",
      "12800/12800 [==============================] - 1s 100us/step - loss: 4.4035e-05 - val_loss: 4.3217e-05\n",
      "Epoch 181/200\n",
      "12800/12800 [==============================] - 1s 102us/step - loss: 4.3875e-05 - val_loss: 4.3049e-05\n",
      "Epoch 182/200\n",
      "12800/12800 [==============================] - 1s 93us/step - loss: 4.3712e-05 - val_loss: 4.2901e-05\n",
      "Epoch 183/200\n",
      "12800/12800 [==============================] - 1s 115us/step - loss: 4.3549e-05 - val_loss: 4.2796e-05\n",
      "Epoch 184/200\n",
      "12800/12800 [==============================] - 1s 92us/step - loss: 4.3392e-05 - val_loss: 4.2553e-05\n",
      "Epoch 185/200\n",
      "12800/12800 [==============================] - 1s 111us/step - loss: 4.3236e-05 - val_loss: 4.2437e-05\n",
      "Epoch 186/200\n",
      "12800/12800 [==============================] - 1s 103us/step - loss: 4.3076e-05 - val_loss: 4.2315e-05\n",
      "Epoch 187/200\n",
      "12800/12800 [==============================] - 1s 113us/step - loss: 4.2924e-05 - val_loss: 4.2131e-05\n",
      "Epoch 188/200\n",
      "12800/12800 [==============================] - 1s 102us/step - loss: 4.2782e-05 - val_loss: 4.1971e-05\n",
      "Epoch 189/200\n",
      "12800/12800 [==============================] - 2s 120us/step - loss: 4.2623e-05 - val_loss: 4.1811e-05\n",
      "Epoch 190/200\n",
      "12800/12800 [==============================] - 2s 131us/step - loss: 4.2473e-05 - val_loss: 4.1708e-05\n",
      "Epoch 191/200\n",
      "12800/12800 [==============================] - 2s 122us/step - loss: 4.2324e-05 - val_loss: 4.1504e-05\n",
      "Epoch 192/200\n",
      "12800/12800 [==============================] - 1s 112us/step - loss: 4.2180e-05 - val_loss: 4.1399e-05\n",
      "Epoch 193/200\n",
      "12800/12800 [==============================] - 1s 111us/step - loss: 4.2037e-05 - val_loss: 4.1246e-05\n",
      "Epoch 194/200\n",
      "12800/12800 [==============================] - 1s 96us/step - loss: 4.1895e-05 - val_loss: 4.1050e-05\n",
      "Epoch 195/200\n",
      "12800/12800 [==============================] - 2s 146us/step - loss: 4.1749e-05 - val_loss: 4.0953e-05\n",
      "Epoch 196/200\n",
      "12800/12800 [==============================] - 2s 127us/step - loss: 4.1610e-05 - val_loss: 4.0803e-05\n",
      "Epoch 197/200\n",
      "12800/12800 [==============================] - 1s 100us/step - loss: 4.1464e-05 - val_loss: 4.0739e-05\n",
      "Epoch 198/200\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 4.1336e-05 - val_loss: 4.0569e-05\n",
      "Epoch 199/200\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 4.1195e-05 - val_loss: 4.0423e-05\n",
      "Epoch 200/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 4.1064e-05 - val_loss: 4.0261e-05\n",
      "Test MSE: 4.1551155e-05\n",
      "Total MSE: 4.0898936e-05\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 75)                22575     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 300)               22800     \n",
      "=================================================================\n",
      "Total params: 45,375\n",
      "Trainable params: 45,375\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/200\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 0.0017 - val_loss: 9.3085e-04\n",
      "Epoch 2/200\n",
      "12800/12800 [==============================] - 1s 48us/step - loss: 8.0140e-04 - val_loss: 6.2928e-04\n",
      "Epoch 3/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 5.7834e-04 - val_loss: 4.8237e-04\n",
      "Epoch 4/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 4.5736e-04 - val_loss: 3.9446e-04\n",
      "Epoch 5/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 3.8077e-04 - val_loss: 3.3528e-04\n",
      "Epoch 6/200\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 3.2783e-04 - val_loss: 2.9274e-04\n",
      "Epoch 7/200\n",
      "12800/12800 [==============================] - 1s 54us/step - loss: 2.8940e-04 - val_loss: 2.6159e-04\n",
      "Epoch 8/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 2.6039e-04 - val_loss: 2.3750e-04\n",
      "Epoch 9/200\n",
      "12800/12800 [==============================] - 1s 48us/step - loss: 2.3788e-04 - val_loss: 2.1916e-04\n",
      "Epoch 10/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 2.1983e-04 - val_loss: 2.0315e-04\n",
      "Epoch 11/200\n",
      "12800/12800 [==============================] - 1s 69us/step - loss: 2.0530e-04 - val_loss: 1.9079e-04\n",
      "Epoch 12/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 1s 68us/step - loss: 1.9320e-04 - val_loss: 1.8017e-04\n",
      "Epoch 13/200\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 1.8285e-04 - val_loss: 1.7153e-04\n",
      "Epoch 14/200\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 1.7408e-04 - val_loss: 1.6373e-04\n",
      "Epoch 15/200\n",
      "12800/12800 [==============================] - 1s 57us/step - loss: 1.6640e-04 - val_loss: 1.5661e-04\n",
      "Epoch 16/200\n",
      "12800/12800 [==============================] - 1s 52us/step - loss: 1.5972e-04 - val_loss: 1.5096e-04\n",
      "Epoch 17/200\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 1.5381e-04 - val_loss: 1.4565e-04\n",
      "Epoch 18/200\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 1.4856e-04 - val_loss: 1.4110e-04\n",
      "Epoch 19/200\n",
      "12800/12800 [==============================] - 1s 65us/step - loss: 1.4386e-04 - val_loss: 1.3677e-04\n",
      "Epoch 20/200\n",
      "12800/12800 [==============================] - 1s 65us/step - loss: 1.3962e-04 - val_loss: 1.3299e-04\n",
      "Epoch 21/200\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 1.3582e-04 - val_loss: 1.2937e-04\n",
      "Epoch 22/200\n",
      "12800/12800 [==============================] - 1s 52us/step - loss: 1.3235e-04 - val_loss: 1.2630e-04\n",
      "Epoch 23/200\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 1.2918e-04 - val_loss: 1.2352e-04\n",
      "Epoch 24/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 1.2631e-04 - val_loss: 1.2077e-04\n",
      "Epoch 25/200\n",
      "12800/12800 [==============================] - 1s 63us/step - loss: 1.2364e-04 - val_loss: 1.1840e-04\n",
      "Epoch 26/200\n",
      "12800/12800 [==============================] - 1s 93us/step - loss: 1.2119e-04 - val_loss: 1.1616e-04\n",
      "Epoch 27/200\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 1.1891e-04 - val_loss: 1.1412e-04\n",
      "Epoch 28/200\n",
      "12800/12800 [==============================] - 1s 107us/step - loss: 1.1680e-04 - val_loss: 1.1211e-04\n",
      "Epoch 29/200\n",
      "12800/12800 [==============================] - 1s 96us/step - loss: 1.1482e-04 - val_loss: 1.1016e-04\n",
      "Epoch 30/200\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 1.1298e-04 - val_loss: 1.0867e-04\n",
      "Epoch 31/200\n",
      "12800/12800 [==============================] - 1s 54us/step - loss: 1.1124e-04 - val_loss: 1.0699e-04\n",
      "Epoch 32/200\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 1.0961e-04 - val_loss: 1.0555e-04\n",
      "Epoch 33/200\n",
      "12800/12800 [==============================] - 1s 53us/step - loss: 1.0808e-04 - val_loss: 1.0405e-04\n",
      "Epoch 34/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 1.0664e-04 - val_loss: 1.0271e-04\n",
      "Epoch 35/200\n",
      "12800/12800 [==============================] - 1s 54us/step - loss: 1.0526e-04 - val_loss: 1.0155e-04\n",
      "Epoch 36/200\n",
      "12800/12800 [==============================] - 1s 48us/step - loss: 1.0394e-04 - val_loss: 1.0021e-04\n",
      "Epoch 37/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 1.0274e-04 - val_loss: 9.9100e-05\n",
      "Epoch 38/200\n",
      "12800/12800 [==============================] - 1s 53us/step - loss: 1.0154e-04 - val_loss: 9.8024e-05\n",
      "Epoch 39/200\n",
      "12800/12800 [==============================] - 1s 48us/step - loss: 1.0041e-04 - val_loss: 9.7154e-05\n",
      "Epoch 40/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 9.9355e-05 - val_loss: 9.5954e-05\n",
      "Epoch 41/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 9.8307e-05 - val_loss: 9.5008e-05\n",
      "Epoch 42/200\n",
      "12800/12800 [==============================] - 1s 62us/step - loss: 9.7358e-05 - val_loss: 9.4147e-05\n",
      "Epoch 43/200\n",
      "12800/12800 [==============================] - 1s 56us/step - loss: 9.6389e-05 - val_loss: 9.3220e-05\n",
      "Epoch 44/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 9.5482e-05 - val_loss: 9.2384e-05\n",
      "Epoch 45/200\n",
      "12800/12800 [==============================] - 1s 53us/step - loss: 9.4625e-05 - val_loss: 9.1583e-05\n",
      "Epoch 46/200\n",
      "12800/12800 [==============================] - 1s 69us/step - loss: 9.3791e-05 - val_loss: 9.0810e-05\n",
      "Epoch 47/200\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 9.2986e-05 - val_loss: 9.0004e-05\n",
      "Epoch 48/200\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 9.2188e-05 - val_loss: 8.9288e-05\n",
      "Epoch 49/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 9.1442e-05 - val_loss: 8.8573e-05\n",
      "Epoch 50/200\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 9.0709e-05 - val_loss: 8.7850e-05\n",
      "Epoch 51/200\n",
      "12800/12800 [==============================] - 2s 128us/step - loss: 9.0009e-05 - val_loss: 8.7326e-05\n",
      "Epoch 52/200\n",
      "12800/12800 [==============================] - 2s 118us/step - loss: 8.9308e-05 - val_loss: 8.6674e-05\n",
      "Epoch 53/200\n",
      "12800/12800 [==============================] - 1s 112us/step - loss: 8.8661e-05 - val_loss: 8.5999e-05\n",
      "Epoch 54/200\n",
      "12800/12800 [==============================] - 2s 153us/step - loss: 8.8016e-05 - val_loss: 8.5334e-05\n",
      "Epoch 55/200\n",
      "12800/12800 [==============================] - 2s 140us/step - loss: 8.7398e-05 - val_loss: 8.4883e-05\n",
      "Epoch 56/200\n",
      "12800/12800 [==============================] - 2s 143us/step - loss: 8.6778e-05 - val_loss: 8.4225e-05\n",
      "Epoch 57/200\n",
      "12800/12800 [==============================] - 2s 120us/step - loss: 8.6186e-05 - val_loss: 8.3637e-05\n",
      "Epoch 58/200\n",
      "12800/12800 [==============================] - 2s 128us/step - loss: 8.5609e-05 - val_loss: 8.3075e-05\n",
      "Epoch 59/200\n",
      "12800/12800 [==============================] - 2s 144us/step - loss: 8.5052e-05 - val_loss: 8.2593e-05\n",
      "Epoch 60/200\n",
      "12800/12800 [==============================] - 2s 157us/step - loss: 8.4513e-05 - val_loss: 8.2192e-05\n",
      "Epoch 61/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 8.3980e-05 - val_loss: 8.1584e-05\n",
      "Epoch 62/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 8.3472e-05 - val_loss: 8.1110e-05\n",
      "Epoch 63/200\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 8.2982e-05 - val_loss: 8.0643e-05\n",
      "Epoch 64/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 8.2487e-05 - val_loss: 8.0190e-05\n",
      "Epoch 65/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 8.2030e-05 - val_loss: 7.9794e-05\n",
      "Epoch 66/200\n",
      "12800/12800 [==============================] - 1s 69us/step - loss: 8.1581e-05 - val_loss: 7.9361e-05\n",
      "Epoch 67/200\n",
      "12800/12800 [==============================] - 1s 72us/step - loss: 8.1129e-05 - val_loss: 7.9022e-05\n",
      "Epoch 68/200\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 8.0714e-05 - val_loss: 7.8512e-05\n",
      "Epoch 69/200\n",
      "12800/12800 [==============================] - 1s 72us/step - loss: 8.0294e-05 - val_loss: 7.8156e-05\n",
      "Epoch 70/200\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 7.9889e-05 - val_loss: 7.7756e-05\n",
      "Epoch 71/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 7.9499e-05 - val_loss: 7.7412e-05\n",
      "Epoch 72/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 7.9124e-05 - val_loss: 7.6996e-05\n",
      "Epoch 73/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 7.8753e-05 - val_loss: 7.6705e-05\n",
      "Epoch 74/200\n",
      "12800/12800 [==============================] - 1s 72us/step - loss: 7.8401e-05 - val_loss: 7.6313e-05\n",
      "Epoch 75/200\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 7.8067e-05 - val_loss: 7.6003e-05\n",
      "Epoch 76/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 7.7727e-05 - val_loss: 7.5693e-05\n",
      "Epoch 77/200\n",
      "12800/12800 [==============================] - 1s 68us/step - loss: 7.7399e-05 - val_loss: 7.5435e-05\n",
      "Epoch 78/200\n",
      "12800/12800 [==============================] - 1s 68us/step - loss: 7.7097e-05 - val_loss: 7.5122e-05\n",
      "Epoch 79/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 7.6781e-05 - val_loss: 7.4846e-05\n",
      "Epoch 80/200\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 7.6503e-05 - val_loss: 7.4530e-05\n",
      "Epoch 81/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 7.6214e-05 - val_loss: 7.4384e-05\n",
      "Epoch 82/200\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 7.5946e-05 - val_loss: 7.4152e-05\n",
      "Epoch 83/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 1s 67us/step - loss: 7.5676e-05 - val_loss: 7.3808e-05\n",
      "Epoch 84/200\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 7.5417e-05 - val_loss: 7.3528e-05\n",
      "Epoch 85/200\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 7.5164e-05 - val_loss: 7.3340e-05\n",
      "Epoch 86/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 7.4936e-05 - val_loss: 7.3069e-05\n",
      "Epoch 87/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 7.4695e-05 - val_loss: 7.2941e-05\n",
      "Epoch 88/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 7.4474e-05 - val_loss: 7.2691e-05\n",
      "Epoch 89/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 7.4247e-05 - val_loss: 7.2535e-05\n",
      "Epoch 90/200\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 7.4045e-05 - val_loss: 7.2294e-05\n",
      "Epoch 91/200\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 7.3845e-05 - val_loss: 7.2109e-05\n",
      "Epoch 92/200\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 7.3652e-05 - val_loss: 7.1908e-05\n",
      "Epoch 93/200\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 7.3454e-05 - val_loss: 7.1685e-05\n",
      "Epoch 94/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 7.3262e-05 - val_loss: 7.1510e-05\n",
      "Epoch 95/200\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 7.3085e-05 - val_loss: 7.1338e-05\n",
      "Epoch 96/200\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 7.2905e-05 - val_loss: 7.1157e-05\n",
      "Epoch 97/200\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 7.2745e-05 - val_loss: 7.0953e-05\n",
      "Epoch 98/200\n",
      "12800/12800 [==============================] - 1s 64us/step - loss: 7.2587e-05 - val_loss: 7.0896e-05\n",
      "Epoch 99/200\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 7.2410e-05 - val_loss: 7.0761e-05\n",
      "Epoch 100/200\n",
      "12800/12800 [==============================] - 1s 112us/step - loss: 7.2262e-05 - val_loss: 7.0546e-05\n",
      "Epoch 101/200\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 7.2104e-05 - val_loss: 7.0439e-05\n",
      "Epoch 102/200\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 7.1971e-05 - val_loss: 7.0239e-05\n",
      "Epoch 103/200\n",
      "12800/12800 [==============================] - 2s 127us/step - loss: 7.1829e-05 - val_loss: 7.0157e-05\n",
      "Epoch 104/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 7.1693e-05 - val_loss: 7.0032e-05\n",
      "Epoch 105/200\n",
      "12800/12800 [==============================] - 1s 62us/step - loss: 7.1561e-05 - val_loss: 6.9876e-05\n",
      "Epoch 106/200\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 7.1421e-05 - val_loss: 6.9755e-05\n",
      "Epoch 107/200\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 7.1305e-05 - val_loss: 6.9576e-05\n",
      "Epoch 108/200\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 7.1188e-05 - val_loss: 6.9451e-05\n",
      "Epoch 109/200\n",
      "12800/12800 [==============================] - 1s 97us/step - loss: 7.1060e-05 - val_loss: 6.9304e-05\n",
      "Epoch 110/200\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 7.0935e-05 - val_loss: 6.9206e-05\n",
      "Epoch 111/200\n",
      "12800/12800 [==============================] - 1s 56us/step - loss: 7.0834e-05 - val_loss: 6.9167e-05\n",
      "Epoch 112/200\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 7.0726e-05 - val_loss: 6.9024e-05\n",
      "Epoch 113/200\n",
      "12800/12800 [==============================] - 1s 53us/step - loss: 7.0626e-05 - val_loss: 6.8958e-05\n",
      "Epoch 114/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 7.0514e-05 - val_loss: 6.8880e-05\n",
      "Epoch 115/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 7.0411e-05 - val_loss: 6.8750e-05\n",
      "Epoch 116/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 7.0314e-05 - val_loss: 6.8618e-05\n",
      "Epoch 117/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 7.0226e-05 - val_loss: 6.8575e-05\n",
      "Epoch 118/200\n",
      "12800/12800 [==============================] - 1s 63us/step - loss: 7.0130e-05 - val_loss: 6.8451e-05\n",
      "Epoch 119/200\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 7.0033e-05 - val_loss: 6.8356e-05\n",
      "Epoch 120/200\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 6.9953e-05 - val_loss: 6.8293e-05\n",
      "Epoch 121/200\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 6.9867e-05 - val_loss: 6.8172e-05\n",
      "Epoch 122/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 6.9775e-05 - val_loss: 6.8138e-05\n",
      "Epoch 123/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 6.9697e-05 - val_loss: 6.8032e-05\n",
      "Epoch 124/200\n",
      "12800/12800 [==============================] - 1s 52us/step - loss: 6.9624e-05 - val_loss: 6.7958e-05\n",
      "Epoch 125/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 6.9542e-05 - val_loss: 6.7886e-05\n",
      "Epoch 126/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 6.9470e-05 - val_loss: 6.7807e-05\n",
      "Epoch 127/200\n",
      "12800/12800 [==============================] - 1s 53us/step - loss: 6.9398e-05 - val_loss: 6.7690e-05\n",
      "Epoch 128/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 6.9321e-05 - val_loss: 6.7686e-05\n",
      "Epoch 129/200\n",
      "12800/12800 [==============================] - 1s 63us/step - loss: 6.9250e-05 - val_loss: 6.7667e-05\n",
      "Epoch 130/200\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 6.9182e-05 - val_loss: 6.7512e-05\n",
      "Epoch 131/200\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 6.9119e-05 - val_loss: 6.7442e-05\n",
      "Epoch 132/200\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 6.9054e-05 - val_loss: 6.7468e-05\n",
      "Epoch 133/200\n",
      "12800/12800 [==============================] - 1s 69us/step - loss: 6.8987e-05 - val_loss: 6.7315e-05\n",
      "Epoch 134/200\n",
      "12800/12800 [==============================] - 1s 54us/step - loss: 6.8925e-05 - val_loss: 6.7295e-05\n",
      "Epoch 135/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 6.8864e-05 - val_loss: 6.7113e-05\n",
      "Epoch 136/200\n",
      "12800/12800 [==============================] - 1s 54us/step - loss: 6.8798e-05 - val_loss: 6.7156e-05\n",
      "Epoch 137/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 6.8758e-05 - val_loss: 6.7051e-05\n",
      "Epoch 138/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 6.8691e-05 - val_loss: 6.7052e-05\n",
      "Epoch 139/200\n",
      "12800/12800 [==============================] - 1s 54us/step - loss: 6.8638e-05 - val_loss: 6.6974e-05\n",
      "Epoch 140/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 6.8578e-05 - val_loss: 6.6896e-05\n",
      "Epoch 141/200\n",
      "12800/12800 [==============================] - 1s 52us/step - loss: 6.8522e-05 - val_loss: 6.6826e-05\n",
      "Epoch 142/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 6.8465e-05 - val_loss: 6.6784e-05\n",
      "Epoch 143/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 6.8426e-05 - val_loss: 6.6742e-05\n",
      "Epoch 144/200\n",
      "12800/12800 [==============================] - 1s 53us/step - loss: 6.8370e-05 - val_loss: 6.6722e-05\n",
      "Epoch 145/200\n",
      "12800/12800 [==============================] - 1s 97us/step - loss: 6.8314e-05 - val_loss: 6.6697e-05\n",
      "Epoch 146/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 6.8277e-05 - val_loss: 6.6621e-05\n",
      "Epoch 147/200\n",
      "12800/12800 [==============================] - 1s 96us/step - loss: 6.8229e-05 - val_loss: 6.6531e-05\n",
      "Epoch 148/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 6.8179e-05 - val_loss: 6.6500e-05\n",
      "Epoch 149/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 6.8139e-05 - val_loss: 6.6484e-05\n",
      "Epoch 150/200\n",
      "12800/12800 [==============================] - 1s 53us/step - loss: 6.8094e-05 - val_loss: 6.6432e-05\n",
      "Epoch 151/200\n",
      "12800/12800 [==============================] - 1s 53us/step - loss: 6.8052e-05 - val_loss: 6.6410e-05\n",
      "Epoch 152/200\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 6.7997e-05 - val_loss: 6.6380e-05\n",
      "Epoch 153/200\n",
      "12800/12800 [==============================] - 1s 88us/step - loss: 6.7962e-05 - val_loss: 6.6321e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 154/200\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 6.7923e-05 - val_loss: 6.6272e-05\n",
      "Epoch 155/200\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 6.7885e-05 - val_loss: 6.6259e-05\n",
      "Epoch 156/200\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 6.7854e-05 - val_loss: 6.6171e-05\n",
      "Epoch 157/200\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 6.7811e-05 - val_loss: 6.6134e-05\n",
      "Epoch 158/200\n",
      "12800/12800 [==============================] - 1s 88us/step - loss: 6.7765e-05 - val_loss: 6.6196e-05\n",
      "Epoch 159/200\n",
      "12800/12800 [==============================] - 1s 97us/step - loss: 6.7735e-05 - val_loss: 6.6055e-05\n",
      "Epoch 160/200\n",
      "12800/12800 [==============================] - 1s 68us/step - loss: 6.7695e-05 - val_loss: 6.6046e-05\n",
      "Epoch 161/200\n",
      "12800/12800 [==============================] - 1s 54us/step - loss: 6.7657e-05 - val_loss: 6.6084e-05\n",
      "Epoch 162/200\n",
      "12800/12800 [==============================] - 1s 52us/step - loss: 6.7634e-05 - val_loss: 6.5979e-05\n",
      "Epoch 163/200\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 6.7595e-05 - val_loss: 6.5883e-05\n",
      "Epoch 164/200\n",
      "12800/12800 [==============================] - 2s 120us/step - loss: 6.7555e-05 - val_loss: 6.5871e-05\n",
      "Epoch 165/200\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 6.7527e-05 - val_loss: 6.5926e-05\n",
      "Epoch 166/200\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 6.7494e-05 - val_loss: 6.5828e-05\n",
      "Epoch 167/200\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 6.7469e-05 - val_loss: 6.5745e-05\n",
      "Epoch 168/200\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 6.7430e-05 - val_loss: 6.5765e-05\n",
      "Epoch 169/200\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 6.7403e-05 - val_loss: 6.5733e-05\n",
      "Epoch 170/200\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 6.7367e-05 - val_loss: 6.5711e-05\n",
      "Epoch 171/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 6.7332e-05 - val_loss: 6.5716e-05\n",
      "Epoch 172/200\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 6.7306e-05 - val_loss: 6.5679e-05\n",
      "Epoch 173/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 6.7285e-05 - val_loss: 6.5603e-05\n",
      "Epoch 174/200\n",
      "12800/12800 [==============================] - 1s 100us/step - loss: 6.7243e-05 - val_loss: 6.5563e-05\n",
      "Epoch 175/200\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 6.7223e-05 - val_loss: 6.5576e-05\n",
      "Epoch 176/200\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 6.7190e-05 - val_loss: 6.5567e-05\n",
      "Epoch 177/200\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 6.7184e-05 - val_loss: 6.5491e-05\n",
      "Epoch 178/200\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 6.7138e-05 - val_loss: 6.5459e-05\n",
      "Epoch 179/200\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 6.7124e-05 - val_loss: 6.5426e-05\n",
      "Epoch 180/200\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 6.7095e-05 - val_loss: 6.5380e-05\n",
      "Epoch 181/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 6.7073e-05 - val_loss: 6.5430e-05\n",
      "Epoch 182/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 6.7038e-05 - val_loss: 6.5397e-05\n",
      "Epoch 183/200\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 6.7019e-05 - val_loss: 6.5312e-05\n",
      "Epoch 184/200\n",
      "12800/12800 [==============================] - 1s 112us/step - loss: 6.6999e-05 - val_loss: 6.5327e-05\n",
      "Epoch 185/200\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 6.6977e-05 - val_loss: 6.5229e-05\n",
      "Epoch 186/200\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 6.6942e-05 - val_loss: 6.5335e-05\n",
      "Epoch 187/200\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 6.6922e-05 - val_loss: 6.5225e-05\n",
      "Epoch 188/200\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 6.6892e-05 - val_loss: 6.5254e-05\n",
      "Epoch 189/200\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 6.6881e-05 - val_loss: 6.5191e-05\n",
      "Epoch 190/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 6.6850e-05 - val_loss: 6.5243e-05\n",
      "Epoch 191/200\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 6.6834e-05 - val_loss: 6.5163e-05\n",
      "Epoch 192/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 6.6813e-05 - val_loss: 6.5142e-05\n",
      "Epoch 193/200\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 6.6791e-05 - val_loss: 6.5109e-05\n",
      "Epoch 194/200\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 6.6776e-05 - val_loss: 6.5032e-05\n",
      "Epoch 195/200\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 6.6750e-05 - val_loss: 6.5055e-05\n",
      "Epoch 196/200\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 6.6730e-05 - val_loss: 6.5095e-05\n",
      "Epoch 197/200\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 6.6715e-05 - val_loss: 6.5088e-05\n",
      "Epoch 198/200\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 6.6695e-05 - val_loss: 6.4949e-05\n",
      "Epoch 199/200\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 6.6662e-05 - val_loss: 6.4970e-05\n",
      "Epoch 200/200\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 6.6656e-05 - val_loss: 6.4966e-05\n",
      "Test MSE: 6.6101947e-05\n",
      "Total MSE: 6.6116405e-05\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input-layer (InputLayer)     (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "hidden-layer-1 (Dense)       (None, 150)               45150     \n",
      "_________________________________________________________________\n",
      "hidden-layer-2 (Dense)       (None, 75)                11325     \n",
      "_________________________________________________________________\n",
      "hidden-layer-3 (Dense)       (None, 150)               11400     \n",
      "_________________________________________________________________\n",
      "output-layer (Dense)         (None, 300)               45300     \n",
      "=================================================================\n",
      "Total params: 113,175\n",
      "Trainable params: 113,175\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/200\n",
      "12800/12800 [==============================] - 2s 189us/step - loss: 0.0025 - val_loss: 0.0012\n",
      "Epoch 2/200\n",
      "12800/12800 [==============================] - 2s 128us/step - loss: 0.0011 - val_loss: 8.9933e-04\n",
      "Epoch 3/200\n",
      "12800/12800 [==============================] - 1s 116us/step - loss: 8.3648e-04 - val_loss: 7.2106e-04\n",
      "Epoch 4/200\n",
      "12800/12800 [==============================] - 2s 127us/step - loss: 6.8969e-04 - val_loss: 6.1002e-04\n",
      "Epoch 5/200\n",
      "12800/12800 [==============================] - 1s 109us/step - loss: 5.8998e-04 - val_loss: 5.2805e-04\n",
      "Epoch 6/200\n",
      "12800/12800 [==============================] - 2s 136us/step - loss: 5.1679e-04 - val_loss: 4.6739e-04\n",
      "Epoch 7/200\n",
      "12800/12800 [==============================] - 2s 122us/step - loss: 4.6050e-04 - val_loss: 4.1998e-04\n",
      "Epoch 8/200\n",
      "12800/12800 [==============================] - 1s 117us/step - loss: 4.1550e-04 - val_loss: 3.8057e-04\n",
      "Epoch 9/200\n",
      "12800/12800 [==============================] - 2s 125us/step - loss: 3.7918e-04 - val_loss: 3.4877e-04\n",
      "Epoch 10/200\n",
      "12800/12800 [==============================] - 2s 157us/step - loss: 3.4938e-04 - val_loss: 3.2217e-04\n",
      "Epoch 11/200\n",
      "12800/12800 [==============================] - 1s 107us/step - loss: 3.2453e-04 - val_loss: 3.0004e-04\n",
      "Epoch 12/200\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 3.0356e-04 - val_loss: 2.8237e-04\n",
      "Epoch 13/200\n",
      "12800/12800 [==============================] - 2s 127us/step - loss: 2.8557e-04 - val_loss: 2.6566e-04\n",
      "Epoch 14/200\n",
      "12800/12800 [==============================] - 2s 121us/step - loss: 2.7013e-04 - val_loss: 2.5181e-04\n",
      "Epoch 15/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 1s 101us/step - loss: 2.5666e-04 - val_loss: 2.3975e-04\n",
      "Epoch 16/200\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 2.4487e-04 - val_loss: 2.2919e-04\n",
      "Epoch 17/200\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 2.3441e-04 - val_loss: 2.1964e-04\n",
      "Epoch 18/200\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 2.2518e-04 - val_loss: 2.1101e-04\n",
      "Epoch 19/200\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 2.1683e-04 - val_loss: 2.0366e-04\n",
      "Epoch 20/200\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 2.0932e-04 - val_loss: 1.9677e-04\n",
      "Epoch 21/200\n",
      "12800/12800 [==============================] - 1s 94us/step - loss: 2.0256e-04 - val_loss: 1.9045e-04\n",
      "Epoch 22/200\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 1.9633e-04 - val_loss: 1.8482e-04\n",
      "Epoch 23/200\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 1.9063e-04 - val_loss: 1.7948e-04\n",
      "Epoch 24/200\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 1.8536e-04 - val_loss: 1.7474e-04\n",
      "Epoch 25/200\n",
      "12800/12800 [==============================] - 2s 126us/step - loss: 1.8053e-04 - val_loss: 1.7036e-04\n",
      "Epoch 26/200\n",
      "12800/12800 [==============================] - 1s 116us/step - loss: 1.7602e-04 - val_loss: 1.6608e-04\n",
      "Epoch 27/200\n",
      "12800/12800 [==============================] - 1s 96us/step - loss: 1.7183e-04 - val_loss: 1.6214e-04\n",
      "Epoch 28/200\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 1.6788e-04 - val_loss: 1.5881e-04\n",
      "Epoch 29/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 1.6423e-04 - val_loss: 1.5528e-04\n",
      "Epoch 30/200\n",
      "12800/12800 [==============================] - 1s 95us/step - loss: 1.6071e-04 - val_loss: 1.5204e-04\n",
      "Epoch 31/200\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 1.5749e-04 - val_loss: 1.4897e-04\n",
      "Epoch 32/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 1.5437e-04 - val_loss: 1.4616e-04\n",
      "Epoch 33/200\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 1.5146e-04 - val_loss: 1.4342e-04\n",
      "Epoch 34/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 1.4871e-04 - val_loss: 1.4099e-04\n",
      "Epoch 35/200\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 1.4608e-04 - val_loss: 1.3850e-04\n",
      "Epoch 36/200\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 1.4358e-04 - val_loss: 1.3634e-04\n",
      "Epoch 37/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 1.4118e-04 - val_loss: 1.3395e-04\n",
      "Epoch 38/200\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 1.3895e-04 - val_loss: 1.3198e-04\n",
      "Epoch 39/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 1.3676e-04 - val_loss: 1.2985e-04\n",
      "Epoch 40/200\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 1.3472e-04 - val_loss: 1.2810e-04\n",
      "Epoch 41/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 1.3275e-04 - val_loss: 1.2618e-04\n",
      "Epoch 42/200\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 1.3086e-04 - val_loss: 1.2461e-04\n",
      "Epoch 43/200\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 1.2908e-04 - val_loss: 1.2282e-04\n",
      "Epoch 44/200\n",
      "12800/12800 [==============================] - 1s 69us/step - loss: 1.2732e-04 - val_loss: 1.2127e-04\n",
      "Epoch 45/200\n",
      "12800/12800 [==============================] - 1s 69us/step - loss: 1.2570e-04 - val_loss: 1.1959e-04\n",
      "Epoch 46/200\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 1.2409e-04 - val_loss: 1.1827e-04\n",
      "Epoch 47/200\n",
      "12800/12800 [==============================] - 1s 69us/step - loss: 1.2257e-04 - val_loss: 1.1674e-04\n",
      "Epoch 48/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 1.2111e-04 - val_loss: 1.1548e-04\n",
      "Epoch 49/200\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 1.1970e-04 - val_loss: 1.1430e-04\n",
      "Epoch 50/200\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 1.1835e-04 - val_loss: 1.1305e-04\n",
      "Epoch 51/200\n",
      "12800/12800 [==============================] - 2s 136us/step - loss: 1.1706e-04 - val_loss: 1.1174e-04\n",
      "Epoch 52/200\n",
      "12800/12800 [==============================] - 2s 118us/step - loss: 1.1579e-04 - val_loss: 1.1052e-04\n",
      "Epoch 53/200\n",
      "12800/12800 [==============================] - 1s 110us/step - loss: 1.1458e-04 - val_loss: 1.0944e-04\n",
      "Epoch 54/200\n",
      "12800/12800 [==============================] - 1s 105us/step - loss: 1.1342e-04 - val_loss: 1.0836e-04\n",
      "Epoch 55/200\n",
      "12800/12800 [==============================] - 1s 101us/step - loss: 1.1231e-04 - val_loss: 1.0732e-04\n",
      "Epoch 56/200\n",
      "12800/12800 [==============================] - 1s 106us/step - loss: 1.1121e-04 - val_loss: 1.0652e-04\n",
      "Epoch 57/200\n",
      "12800/12800 [==============================] - 1s 112us/step - loss: 1.1018e-04 - val_loss: 1.0567e-04\n",
      "Epoch 58/200\n",
      "12800/12800 [==============================] - 2s 136us/step - loss: 1.0917e-04 - val_loss: 1.0442e-04\n",
      "Epoch 59/200\n",
      "12800/12800 [==============================] - 2s 123us/step - loss: 1.0818e-04 - val_loss: 1.0355e-04\n",
      "Epoch 60/200\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 1.0725e-04 - val_loss: 1.0260e-04\n",
      "Epoch 61/200\n",
      "12800/12800 [==============================] - 1s 107us/step - loss: 1.0636e-04 - val_loss: 1.0178e-04\n",
      "Epoch 62/200\n",
      "12800/12800 [==============================] - 1s 105us/step - loss: 1.0545e-04 - val_loss: 1.0094e-04\n",
      "Epoch 63/200\n",
      "12800/12800 [==============================] - 1s 100us/step - loss: 1.0460e-04 - val_loss: 1.0018e-04\n",
      "Epoch 64/200\n",
      "12800/12800 [==============================] - 1s 102us/step - loss: 1.0377e-04 - val_loss: 9.9435e-05\n",
      "Epoch 65/200\n",
      "12800/12800 [==============================] - 1s 108us/step - loss: 1.0299e-04 - val_loss: 9.8617e-05\n",
      "Epoch 66/200\n",
      "12800/12800 [==============================] - 2s 125us/step - loss: 1.0219e-04 - val_loss: 9.7978e-05\n",
      "Epoch 67/200\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 1.0146e-04 - val_loss: 9.7293e-05\n",
      "Epoch 68/200\n",
      "12800/12800 [==============================] - 1s 111us/step - loss: 1.0072e-04 - val_loss: 9.6631e-05\n",
      "Epoch 69/200\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 1.0000e-04 - val_loss: 9.6174e-05\n",
      "Epoch 70/200\n",
      "12800/12800 [==============================] - 2s 123us/step - loss: 9.9316e-05 - val_loss: 9.5379e-05\n",
      "Epoch 71/200\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 9.8642e-05 - val_loss: 9.4685e-05\n",
      "Epoch 72/200\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 9.7995e-05 - val_loss: 9.4065e-05\n",
      "Epoch 73/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 9.7377e-05 - val_loss: 9.3541e-05\n",
      "Epoch 74/200\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 9.6769e-05 - val_loss: 9.2936e-05\n",
      "Epoch 75/200\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 9.6162e-05 - val_loss: 9.2347e-05\n",
      "Epoch 76/200\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 9.5593e-05 - val_loss: 9.1879e-05\n",
      "Epoch 77/200\n",
      "12800/12800 [==============================] - 1s 115us/step - loss: 9.5044e-05 - val_loss: 9.1315e-05\n",
      "Epoch 78/200\n",
      "12800/12800 [==============================] - 2s 125us/step - loss: 9.4484e-05 - val_loss: 9.0859e-05\n",
      "Epoch 79/200\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 9.3964e-05 - val_loss: 9.0260e-05\n",
      "Epoch 80/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 9.3459e-05 - val_loss: 8.9814e-05\n",
      "Epoch 81/200\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 9.2942e-05 - val_loss: 8.9444e-05\n",
      "Epoch 82/200\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 9.2477e-05 - val_loss: 8.8863e-05\n",
      "Epoch 83/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 9.1995e-05 - val_loss: 8.8536e-05\n",
      "Epoch 84/200\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 9.1520e-05 - val_loss: 8.7968e-05\n",
      "Epoch 85/200\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 9.1094e-05 - val_loss: 8.7533e-05\n",
      "Epoch 86/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 1s 79us/step - loss: 9.0651e-05 - val_loss: 8.7262e-05\n",
      "Epoch 87/200\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 9.0226e-05 - val_loss: 8.6793e-05\n",
      "Epoch 88/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 8.9808e-05 - val_loss: 8.6367e-05\n",
      "Epoch 89/200\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 8.9401e-05 - val_loss: 8.6083e-05\n",
      "Epoch 90/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 8.9009e-05 - val_loss: 8.5616e-05\n",
      "Epoch 91/200\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 8.8639e-05 - val_loss: 8.5289e-05\n",
      "Epoch 92/200\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 8.8277e-05 - val_loss: 8.4945e-05\n",
      "Epoch 93/200\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 8.7906e-05 - val_loss: 8.4582e-05\n",
      "Epoch 94/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 8.7552e-05 - val_loss: 8.4249e-05\n",
      "Epoch 95/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 8.7208e-05 - val_loss: 8.3919e-05\n",
      "Epoch 96/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 8.6859e-05 - val_loss: 8.3580e-05\n",
      "Epoch 97/200\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 8.6548e-05 - val_loss: 8.3284e-05\n",
      "Epoch 98/200\n",
      "12800/12800 [==============================] - 2s 134us/step - loss: 8.6229e-05 - val_loss: 8.2943e-05\n",
      "Epoch 99/200\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 8.5905e-05 - val_loss: 8.2701e-05\n",
      "Epoch 100/200\n",
      "12800/12800 [==============================] - 1s 99us/step - loss: 8.5610e-05 - val_loss: 8.2395e-05\n",
      "Epoch 101/200\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 8.5294e-05 - val_loss: 8.2088e-05\n",
      "Epoch 102/200\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 8.5005e-05 - val_loss: 8.1800e-05\n",
      "Epoch 103/200\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 8.4736e-05 - val_loss: 8.1509e-05\n",
      "Epoch 104/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 8.4457e-05 - val_loss: 8.1221e-05\n",
      "Epoch 105/200\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 8.4157e-05 - val_loss: 8.1099e-05\n",
      "Epoch 106/200\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 8.3909e-05 - val_loss: 8.0767e-05\n",
      "Epoch 107/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 8.3654e-05 - val_loss: 8.0523e-05\n",
      "Epoch 108/200\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 8.3410e-05 - val_loss: 8.0240e-05\n",
      "Epoch 109/200\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 8.3134e-05 - val_loss: 8.0110e-05\n",
      "Epoch 110/200\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 8.2888e-05 - val_loss: 7.9746e-05\n",
      "Epoch 111/200\n",
      "12800/12800 [==============================] - 1s 72us/step - loss: 8.2652e-05 - val_loss: 7.9605e-05\n",
      "Epoch 112/200\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 8.2429e-05 - val_loss: 7.9388e-05\n",
      "Epoch 113/200\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 8.2194e-05 - val_loss: 7.9192e-05\n",
      "Epoch 114/200\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 8.1965e-05 - val_loss: 7.8957e-05\n",
      "Epoch 115/200\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 8.1739e-05 - val_loss: 7.8735e-05\n",
      "Epoch 116/200\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 8.1532e-05 - val_loss: 7.8538e-05\n",
      "Epoch 117/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 8.1325e-05 - val_loss: 7.8352e-05\n",
      "Epoch 118/200\n",
      "12800/12800 [==============================] - 1s 109us/step - loss: 8.1110e-05 - val_loss: 7.8159e-05\n",
      "Epoch 119/200\n",
      "12800/12800 [==============================] - 2s 137us/step - loss: 8.0912e-05 - val_loss: 7.8048e-05\n",
      "Epoch 120/200\n",
      "12800/12800 [==============================] - 2s 132us/step - loss: 8.0710e-05 - val_loss: 7.7739e-05\n",
      "Epoch 121/200\n",
      "12800/12800 [==============================] - 1s 114us/step - loss: 8.0494e-05 - val_loss: 7.7585e-05\n",
      "Epoch 122/200\n",
      "12800/12800 [==============================] - 2s 132us/step - loss: 8.0320e-05 - val_loss: 7.7317e-05\n",
      "Epoch 123/200\n",
      "12800/12800 [==============================] - 1s 116us/step - loss: 8.0131e-05 - val_loss: 7.7096e-05\n",
      "Epoch 124/200\n",
      "12800/12800 [==============================] - 1s 113us/step - loss: 7.9934e-05 - val_loss: 7.6946e-05\n",
      "Epoch 125/200\n",
      "12800/12800 [==============================] - 1s 113us/step - loss: 7.9754e-05 - val_loss: 7.6784e-05\n",
      "Epoch 126/200\n",
      "12800/12800 [==============================] - 1s 106us/step - loss: 7.9572e-05 - val_loss: 7.6574e-05\n",
      "Epoch 127/200\n",
      "12800/12800 [==============================] - 1s 107us/step - loss: 7.9373e-05 - val_loss: 7.6461e-05\n",
      "Epoch 128/200\n",
      "12800/12800 [==============================] - 1s 113us/step - loss: 7.9211e-05 - val_loss: 7.6325e-05\n",
      "Epoch 129/200\n",
      "12800/12800 [==============================] - 1s 109us/step - loss: 7.9036e-05 - val_loss: 7.6107e-05\n",
      "Epoch 130/200\n",
      "12800/12800 [==============================] - 1s 106us/step - loss: 7.8875e-05 - val_loss: 7.5914e-05\n",
      "Epoch 131/200\n",
      "12800/12800 [==============================] - 2s 136us/step - loss: 7.8690e-05 - val_loss: 7.5873e-05\n",
      "Epoch 132/200\n",
      "12800/12800 [==============================] - 2s 136us/step - loss: 7.8533e-05 - val_loss: 7.5638e-05\n",
      "Epoch 133/200\n",
      "12800/12800 [==============================] - 2s 131us/step - loss: 7.8369e-05 - val_loss: 7.5438e-05\n",
      "Epoch 134/200\n",
      "12800/12800 [==============================] - 2s 119us/step - loss: 7.8214e-05 - val_loss: 7.5269e-05\n",
      "Epoch 135/200\n",
      "12800/12800 [==============================] - 2s 121us/step - loss: 7.8050e-05 - val_loss: 7.5108e-05\n",
      "Epoch 136/200\n",
      "12800/12800 [==============================] - 2s 147us/step - loss: 7.7903e-05 - val_loss: 7.5013e-05\n",
      "Epoch 137/200\n",
      "12800/12800 [==============================] - 2s 128us/step - loss: 7.7746e-05 - val_loss: 7.4859e-05\n",
      "Epoch 138/200\n",
      "12800/12800 [==============================] - 2s 129us/step - loss: 7.7583e-05 - val_loss: 7.4705e-05\n",
      "Epoch 139/200\n",
      "12800/12800 [==============================] - 2s 121us/step - loss: 7.7441e-05 - val_loss: 7.4590e-05\n",
      "Epoch 140/200\n",
      "12800/12800 [==============================] - 1s 115us/step - loss: 7.7295e-05 - val_loss: 7.4447e-05\n",
      "Epoch 141/200\n",
      "12800/12800 [==============================] - 2s 133us/step - loss: 7.7161e-05 - val_loss: 7.4402e-05\n",
      "Epoch 142/200\n",
      "12800/12800 [==============================] - 1s 106us/step - loss: 7.7009e-05 - val_loss: 7.4181e-05\n",
      "Epoch 143/200\n",
      "12800/12800 [==============================] - 1s 103us/step - loss: 7.6859e-05 - val_loss: 7.4024e-05\n",
      "Epoch 144/200\n",
      "12800/12800 [==============================] - 1s 111us/step - loss: 7.6722e-05 - val_loss: 7.3846e-05\n",
      "Epoch 145/200\n",
      "12800/12800 [==============================] - 1s 114us/step - loss: 7.6586e-05 - val_loss: 7.3772e-05\n",
      "Epoch 146/200\n",
      "12800/12800 [==============================] - 1s 112us/step - loss: 7.6452e-05 - val_loss: 7.3643e-05\n",
      "Epoch 147/200\n",
      "12800/12800 [==============================] - 1s 98us/step - loss: 7.6323e-05 - val_loss: 7.3456e-05\n",
      "Epoch 148/200\n",
      "12800/12800 [==============================] - 1s 99us/step - loss: 7.6182e-05 - val_loss: 7.3480e-05\n",
      "Epoch 149/200\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 7.6062e-05 - val_loss: 7.3273e-05\n",
      "Epoch 150/200\n",
      "12800/12800 [==============================] - 1s 94us/step - loss: 7.5948e-05 - val_loss: 7.3095e-05\n",
      "Epoch 151/200\n",
      "12800/12800 [==============================] - 1s 98us/step - loss: 7.5805e-05 - val_loss: 7.3057e-05\n",
      "Epoch 152/200\n",
      "12800/12800 [==============================] - 1s 96us/step - loss: 7.5686e-05 - val_loss: 7.2822e-05\n",
      "Epoch 153/200\n",
      "12800/12800 [==============================] - 1s 98us/step - loss: 7.5555e-05 - val_loss: 7.2835e-05\n",
      "Epoch 154/200\n",
      "12800/12800 [==============================] - 1s 112us/step - loss: 7.5422e-05 - val_loss: 7.2720e-05\n",
      "Epoch 155/200\n",
      "12800/12800 [==============================] - 1s 97us/step - loss: 7.5320e-05 - val_loss: 7.2565e-05\n",
      "Epoch 156/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 1s 94us/step - loss: 7.5200e-05 - val_loss: 7.2372e-05\n",
      "Epoch 157/200\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 7.5075e-05 - val_loss: 7.2377e-05\n",
      "Epoch 158/200\n",
      "12800/12800 [==============================] - 1s 88us/step - loss: 7.4972e-05 - val_loss: 7.2136e-05\n",
      "Epoch 159/200\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 7.4853e-05 - val_loss: 7.1990e-05\n",
      "Epoch 160/200\n",
      "12800/12800 [==============================] - 1s 102us/step - loss: 7.4744e-05 - val_loss: 7.2005e-05\n",
      "Epoch 161/200\n",
      "12800/12800 [==============================] - 1s 109us/step - loss: 7.4628e-05 - val_loss: 7.1845e-05\n",
      "Epoch 162/200\n",
      "12800/12800 [==============================] - 1s 103us/step - loss: 7.4524e-05 - val_loss: 7.1798e-05\n",
      "Epoch 163/200\n",
      "12800/12800 [==============================] - 1s 108us/step - loss: 7.4418e-05 - val_loss: 7.1613e-05\n",
      "Epoch 164/200\n",
      "12800/12800 [==============================] - 2s 155us/step - loss: 7.4313e-05 - val_loss: 7.1475e-05\n",
      "Epoch 165/200\n",
      "12800/12800 [==============================] - 1s 107us/step - loss: 7.4206e-05 - val_loss: 7.1432e-05\n",
      "Epoch 166/200\n",
      "12800/12800 [==============================] - 1s 115us/step - loss: 7.4091e-05 - val_loss: 7.1308e-05\n",
      "Epoch 167/200\n",
      "12800/12800 [==============================] - 1s 92us/step - loss: 7.3998e-05 - val_loss: 7.1180e-05\n",
      "Epoch 168/200\n",
      "12800/12800 [==============================] - 1s 108us/step - loss: 7.3890e-05 - val_loss: 7.1166e-05\n",
      "Epoch 169/200\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 7.3801e-05 - val_loss: 7.1024e-05\n",
      "Epoch 170/200\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 7.3703e-05 - val_loss: 7.0981e-05\n",
      "Epoch 171/200\n",
      "12800/12800 [==============================] - 2s 137us/step - loss: 7.3584e-05 - val_loss: 7.0854e-05\n",
      "Epoch 172/200\n",
      "12800/12800 [==============================] - 1s 98us/step - loss: 7.3497e-05 - val_loss: 7.0855e-05\n",
      "Epoch 173/200\n",
      "12800/12800 [==============================] - 2s 126us/step - loss: 7.3408e-05 - val_loss: 7.0695e-05\n",
      "Epoch 174/200\n",
      "12800/12800 [==============================] - 1s 112us/step - loss: 7.3323e-05 - val_loss: 7.0537e-05\n",
      "Epoch 175/200\n",
      "12800/12800 [==============================] - 1s 111us/step - loss: 7.3220e-05 - val_loss: 7.0488e-05\n",
      "Epoch 176/200\n",
      "12800/12800 [==============================] - 1s 116us/step - loss: 7.3137e-05 - val_loss: 7.0427e-05\n",
      "Epoch 177/200\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 7.3047e-05 - val_loss: 7.0318e-05\n",
      "Epoch 178/200\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 7.2970e-05 - val_loss: 7.0219e-05\n",
      "Epoch 179/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 7.2852e-05 - val_loss: 7.0153e-05\n",
      "Epoch 180/200\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 7.2781e-05 - val_loss: 7.0121e-05\n",
      "Epoch 181/200\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 7.2696e-05 - val_loss: 6.9967e-05\n",
      "Epoch 182/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 7.2606e-05 - val_loss: 6.9848e-05\n",
      "Epoch 183/200\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 7.2519e-05 - val_loss: 6.9847e-05\n",
      "Epoch 184/200\n",
      "12800/12800 [==============================] - 1s 104us/step - loss: 7.2440e-05 - val_loss: 6.9770e-05\n",
      "Epoch 185/200\n",
      "12800/12800 [==============================] - 2s 145us/step - loss: 7.2362e-05 - val_loss: 6.9649e-05\n",
      "Epoch 186/200\n",
      "12800/12800 [==============================] - 2s 128us/step - loss: 7.2264e-05 - val_loss: 6.9561e-05\n",
      "Epoch 187/200\n",
      "12800/12800 [==============================] - 2s 165us/step - loss: 7.2205e-05 - val_loss: 6.9488e-05\n",
      "Epoch 188/200\n",
      "12800/12800 [==============================] - 2s 132us/step - loss: 7.2125e-05 - val_loss: 6.9415e-05\n",
      "Epoch 189/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 7.2046e-05 - val_loss: 6.9354e-05\n",
      "Epoch 190/200\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 7.1967e-05 - val_loss: 6.9275e-05\n",
      "Epoch 191/200\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 7.1884e-05 - val_loss: 6.9217e-05\n",
      "Epoch 192/200\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 7.1824e-05 - val_loss: 6.9114e-05\n",
      "Epoch 193/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 7.1750e-05 - val_loss: 6.9041e-05\n",
      "Epoch 194/200\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 7.1670e-05 - val_loss: 6.8979e-05\n",
      "Epoch 195/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 7.1600e-05 - val_loss: 6.8929e-05\n",
      "Epoch 196/200\n",
      "12800/12800 [==============================] - 1s 69us/step - loss: 7.1526e-05 - val_loss: 6.8891e-05\n",
      "Epoch 197/200\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 7.1459e-05 - val_loss: 6.8817e-05\n",
      "Epoch 198/200\n",
      "12800/12800 [==============================] - 1s 69us/step - loss: 7.1386e-05 - val_loss: 6.8724e-05\n",
      "Epoch 199/200\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 7.1326e-05 - val_loss: 6.8587e-05\n",
      "Epoch 200/200\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 7.1244e-05 - val_loss: 6.8634e-05\n",
      "Test MSE: 7.1986488e-05\n",
      "Total MSE: 7.0802955e-05\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 60)                18060     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 300)               18300     \n",
      "=================================================================\n",
      "Total params: 36,360\n",
      "Trainable params: 36,360\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/200\n",
      "12800/12800 [==============================] - 1s 116us/step - loss: 0.0018 - val_loss: 0.0011\n",
      "Epoch 2/200\n",
      "12800/12800 [==============================] - 1s 68us/step - loss: 9.0670e-04 - val_loss: 7.8231e-04\n",
      "Epoch 3/200\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 6.8775e-04 - val_loss: 6.2626e-04\n",
      "Epoch 4/200\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 5.6576e-04 - val_loss: 5.2967e-04\n",
      "Epoch 5/200\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 4.8648e-04 - val_loss: 4.6307e-04\n",
      "Epoch 6/200\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 4.3046e-04 - val_loss: 4.1463e-04\n",
      "Epoch 7/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 3.8808e-04 - val_loss: 3.7647e-04\n",
      "Epoch 8/200\n",
      "12800/12800 [==============================] - 1s 72us/step - loss: 3.5477e-04 - val_loss: 3.4655e-04\n",
      "Epoch 9/200\n",
      "12800/12800 [==============================] - 1s 68us/step - loss: 3.2793e-04 - val_loss: 3.2165e-04\n",
      "Epoch 10/200\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 3.0581e-04 - val_loss: 3.0167e-04\n",
      "Epoch 11/200\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 2.8736e-04 - val_loss: 2.8496e-04\n",
      "Epoch 12/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 2.7198e-04 - val_loss: 2.7034e-04\n",
      "Epoch 13/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 2.5883e-04 - val_loss: 2.5879e-04\n",
      "Epoch 14/200\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 2.4781e-04 - val_loss: 2.4839e-04\n",
      "Epoch 15/200\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 2.3825e-04 - val_loss: 2.3937e-04\n",
      "Epoch 16/200\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 2.2998e-04 - val_loss: 2.3202e-04\n",
      "Epoch 17/200\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 2.2277e-04 - val_loss: 2.2473e-04\n",
      "Epoch 18/200\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 2.1637e-04 - val_loss: 2.1899e-04\n",
      "Epoch 19/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 1s 71us/step - loss: 2.1065e-04 - val_loss: 2.1345e-04\n",
      "Epoch 20/200\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 2.0550e-04 - val_loss: 2.0858e-04\n",
      "Epoch 21/200\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 2.0090e-04 - val_loss: 2.0409e-04\n",
      "Epoch 22/200\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 1.9666e-04 - val_loss: 2.0003e-04\n",
      "Epoch 23/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 1.9286e-04 - val_loss: 1.9612e-04\n",
      "Epoch 24/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 1.8928e-04 - val_loss: 1.9314e-04\n",
      "Epoch 25/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 1.8596e-04 - val_loss: 1.9008e-04\n",
      "Epoch 26/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 1.8293e-04 - val_loss: 1.8672e-04\n",
      "Epoch 27/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 1.8011e-04 - val_loss: 1.8390e-04- loss: 1.81\n",
      "Epoch 28/200\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 1.7745e-04 - val_loss: 1.8107e-04\n",
      "Epoch 29/200\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 1.7493e-04 - val_loss: 1.7888e-04\n",
      "Epoch 30/200\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 1.7260e-04 - val_loss: 1.7668e-04\n",
      "Epoch 31/200\n",
      "12800/12800 [==============================] - 1s 98us/step - loss: 1.7040e-04 - val_loss: 1.7432e-04\n",
      "Epoch 32/200\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 1.6836e-04 - val_loss: 1.7231e-04\n",
      "Epoch 33/200\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 1.6643e-04 - val_loss: 1.7030e-04\n",
      "Epoch 34/200\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 1.6455e-04 - val_loss: 1.6879e-04\n",
      "Epoch 35/200\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 1.6285e-04 - val_loss: 1.6689e-04\n",
      "Epoch 36/200\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 1.6119e-04 - val_loss: 1.6530e-04\n",
      "Epoch 37/200\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 1.5967e-04 - val_loss: 1.6362e-04\n",
      "Epoch 38/200\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 1.5822e-04 - val_loss: 1.6219e-04\n",
      "Epoch 39/200\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 1.5684e-04 - val_loss: 1.6081e-04\n",
      "Epoch 40/200\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 1.5557e-04 - val_loss: 1.5949e-04\n",
      "Epoch 41/200\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 1.5432e-04 - val_loss: 1.5826e-04\n",
      "Epoch 42/200\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 1.5317e-04 - val_loss: 1.5712e-04\n",
      "Epoch 43/200\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 1.5209e-04 - val_loss: 1.5605e-04\n",
      "Epoch 44/200\n",
      "12800/12800 [==============================] - 1s 103us/step - loss: 1.5105e-04 - val_loss: 1.5489e-04\n",
      "Epoch 45/200\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 1.5004e-04 - val_loss: 1.5399e-04\n",
      "Epoch 46/200\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 1.4913e-04 - val_loss: 1.5304e-04\n",
      "Epoch 47/200\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 1.4823e-04 - val_loss: 1.5214e-04\n",
      "Epoch 48/200\n",
      "12800/12800 [==============================] - 1s 52us/step - loss: 1.4742e-04 - val_loss: 1.5130e-04\n",
      "Epoch 49/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 1.4661e-04 - val_loss: 1.5045e-04\n",
      "Epoch 50/200\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 1.4588e-04 - val_loss: 1.4970e-04\n",
      "Epoch 51/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 1.4517e-04 - val_loss: 1.4907e-04\n",
      "Epoch 52/200\n",
      "12800/12800 [==============================] - 1s 54us/step - loss: 1.4452e-04 - val_loss: 1.4825e-04\n",
      "Epoch 53/200\n",
      "12800/12800 [==============================] - 1s 63us/step - loss: 1.4383e-04 - val_loss: 1.4767e-04\n",
      "Epoch 54/200\n",
      "12800/12800 [==============================] - 1s 53us/step - loss: 1.4322e-04 - val_loss: 1.4701e-04\n",
      "Epoch 55/200\n",
      "12800/12800 [==============================] - 1s 62us/step - loss: 1.4265e-04 - val_loss: 1.4636e-04\n",
      "Epoch 56/200\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 1.4208e-04 - val_loss: 1.4573e-04\n",
      "Epoch 57/200\n",
      "12800/12800 [==============================] - 1s 61us/step - loss: 1.4157e-04 - val_loss: 1.4522e-04\n",
      "Epoch 58/200\n",
      "12800/12800 [==============================] - 1s 56us/step - loss: 1.4104e-04 - val_loss: 1.4484e-04\n",
      "Epoch 59/200\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 1.4057e-04 - val_loss: 1.4424e-04\n",
      "Epoch 60/200\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 1.4009e-04 - val_loss: 1.4386e-04\n",
      "Epoch 61/200\n",
      "12800/12800 [==============================] - 1s 54us/step - loss: 1.3968e-04 - val_loss: 1.4329e-04\n",
      "Epoch 62/200\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 1.3924e-04 - val_loss: 1.4292e-04\n",
      "Epoch 63/200\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 1.3883e-04 - val_loss: 1.4249e-04\n",
      "Epoch 64/200\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 1.3844e-04 - val_loss: 1.4204e-04\n",
      "Epoch 65/200\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 1.3809e-04 - val_loss: 1.4170e-04\n",
      "Epoch 66/200\n",
      "12800/12800 [==============================] - 1s 54us/step - loss: 1.3770e-04 - val_loss: 1.4144e-04\n",
      "Epoch 67/200\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 1.3736e-04 - val_loss: 1.4104e-04\n",
      "Epoch 68/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 1.3706e-04 - val_loss: 1.4061e-04\n",
      "Epoch 69/200\n",
      "12800/12800 [==============================] - 1s 53us/step - loss: 1.3673e-04 - val_loss: 1.4022e-04\n",
      "Epoch 70/200\n",
      "12800/12800 [==============================] - 1s 69us/step - loss: 1.3642e-04 - val_loss: 1.4007e-04\n",
      "Epoch 71/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 1.3613e-04 - val_loss: 1.3964e-04\n",
      "Epoch 72/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 1.3584e-04 - val_loss: 1.3947e-04\n",
      "Epoch 73/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 1.3559e-04 - val_loss: 1.3913e-04\n",
      "Epoch 74/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 1.3531e-04 - val_loss: 1.3881e-04\n",
      "Epoch 75/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 1.3508e-04 - val_loss: 1.3854e-04\n",
      "Epoch 76/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 1.3485e-04 - val_loss: 1.3828e-04\n",
      "Epoch 77/200\n",
      "12800/12800 [==============================] - 1s 52us/step - loss: 1.3458e-04 - val_loss: 1.3810e-04\n",
      "Epoch 78/200\n",
      "12800/12800 [==============================] - 1s 57us/step - loss: 1.3438e-04 - val_loss: 1.3795e-04\n",
      "Epoch 79/200\n",
      "12800/12800 [==============================] - 1s 56us/step - loss: 1.3416e-04 - val_loss: 1.3762e-04\n",
      "Epoch 80/200\n",
      "12800/12800 [==============================] - 1s 52us/step - loss: 1.3394e-04 - val_loss: 1.3741e-04\n",
      "Epoch 81/200\n",
      "12800/12800 [==============================] - 1s 68us/step - loss: 1.3376e-04 - val_loss: 1.3721e-04\n",
      "Epoch 82/200\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 1.3357e-04 - val_loss: 1.3706e-04\n",
      "Epoch 83/200\n",
      "12800/12800 [==============================] - 1s 54us/step - loss: 1.3338e-04 - val_loss: 1.3684e-04\n",
      "Epoch 84/200\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 1.3317e-04 - val_loss: 1.3674e-04\n",
      "Epoch 85/200\n",
      "12800/12800 [==============================] - 1s 57us/step - loss: 1.3302e-04 - val_loss: 1.3648e-04\n",
      "Epoch 86/200\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 1.3287e-04 - val_loss: 1.3636e-04\n",
      "Epoch 87/200\n",
      "12800/12800 [==============================] - 1s 65us/step - loss: 1.3268e-04 - val_loss: 1.3617e-04\n",
      "Epoch 88/200\n",
      "12800/12800 [==============================] - 1s 62us/step - loss: 1.3252e-04 - val_loss: 1.3601e-04\n",
      "Epoch 89/200\n",
      "12800/12800 [==============================] - 1s 56us/step - loss: 1.3238e-04 - val_loss: 1.3588e-04\n",
      "Epoch 90/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 1s 57us/step - loss: 1.3223e-04 - val_loss: 1.3574e-04\n",
      "Epoch 91/200\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 1.3209e-04 - val_loss: 1.3551e-04\n",
      "Epoch 92/200\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 1.3197e-04 - val_loss: 1.3531e-04\n",
      "Epoch 93/200\n",
      "12800/12800 [==============================] - 1s 57us/step - loss: 1.3181e-04 - val_loss: 1.3523e-04\n",
      "Epoch 94/200\n",
      "12800/12800 [==============================] - 1s 115us/step - loss: 1.3170e-04 - val_loss: 1.3523e-04\n",
      "Epoch 95/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 1.3158e-04 - val_loss: 1.3515e-04\n",
      "Epoch 96/200\n",
      "12800/12800 [==============================] - 1s 69us/step - loss: 1.3146e-04 - val_loss: 1.3485e-04\n",
      "Epoch 97/200\n",
      "12800/12800 [==============================] - 1s 54us/step - loss: 1.3133e-04 - val_loss: 1.3477e-04\n",
      "Epoch 98/200\n",
      "12800/12800 [==============================] - 1s 57us/step - loss: 1.3121e-04 - val_loss: 1.3470e-04\n",
      "Epoch 99/200\n",
      "12800/12800 [==============================] - 1s 54us/step - loss: 1.3113e-04 - val_loss: 1.3448e-04\n",
      "Epoch 100/200\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 1.3101e-04 - val_loss: 1.3442e-04\n",
      "Epoch 101/200\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 1.3091e-04 - val_loss: 1.3432e-04\n",
      "Epoch 102/200\n",
      "12800/12800 [==============================] - 1s 63us/step - loss: 1.3079e-04 - val_loss: 1.3422e-04\n",
      "Epoch 103/200\n",
      "12800/12800 [==============================] - 1s 100us/step - loss: 1.3071e-04 - val_loss: 1.3412e-04\n",
      "Epoch 104/200\n",
      "12800/12800 [==============================] - 1s 109us/step - loss: 1.3061e-04 - val_loss: 1.3395e-04\n",
      "Epoch 105/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 1.3053e-04 - val_loss: 1.3396e-04\n",
      "Epoch 106/200\n",
      "12800/12800 [==============================] - 1s 61us/step - loss: 1.3044e-04 - val_loss: 1.3384e-04\n",
      "Epoch 107/200\n",
      "12800/12800 [==============================] - 1s 53us/step - loss: 1.3034e-04 - val_loss: 1.3373e-04\n",
      "Epoch 108/200\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 1.3025e-04 - val_loss: 1.3369e-04\n",
      "Epoch 109/200\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 1.3017e-04 - val_loss: 1.3361e-04\n",
      "Epoch 110/200\n",
      "12800/12800 [==============================] - 1s 72us/step - loss: 1.3010e-04 - val_loss: 1.3351e-04\n",
      "Epoch 111/200\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 1.3001e-04 - val_loss: 1.3343e-04\n",
      "Epoch 112/200\n",
      "12800/12800 [==============================] - 1s 63us/step - loss: 1.2995e-04 - val_loss: 1.3332e-04\n",
      "Epoch 113/200\n",
      "12800/12800 [==============================] - 1s 63us/step - loss: 1.2987e-04 - val_loss: 1.3319e-04\n",
      "Epoch 114/200\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 1.2979e-04 - val_loss: 1.3320e-04\n",
      "Epoch 115/200\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 1.2974e-04 - val_loss: 1.3307e-04\n",
      "Epoch 116/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 1.2966e-04 - val_loss: 1.3298e-04\n",
      "Epoch 117/200\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 1.2960e-04 - val_loss: 1.3298e-04\n",
      "Epoch 118/200\n",
      "12800/12800 [==============================] - 1s 68us/step - loss: 1.2953e-04 - val_loss: 1.3284e-04\n",
      "Epoch 119/200\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 1.2945e-04 - val_loss: 1.3296e-04\n",
      "Epoch 120/200\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 1.2941e-04 - val_loss: 1.3276e-04\n",
      "Epoch 121/200\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 1.2934e-04 - val_loss: 1.3270e-04\n",
      "Epoch 122/200\n",
      "12800/12800 [==============================] - 1s 72us/step - loss: 1.2928e-04 - val_loss: 1.3266e-04\n",
      "Epoch 123/200\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 1.2922e-04 - val_loss: 1.3262e-04\n",
      "Epoch 124/200\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 1.2917e-04 - val_loss: 1.3257e-04\n",
      "Epoch 125/200\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 1.2909e-04 - val_loss: 1.3248e-04\n",
      "Epoch 126/200\n",
      "12800/12800 [==============================] - 1s 54us/step - loss: 1.2907e-04 - val_loss: 1.3247e-04\n",
      "Epoch 127/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 1.2900e-04 - val_loss: 1.3236e-04\n",
      "Epoch 128/200\n",
      "12800/12800 [==============================] - 1s 52us/step - loss: 1.2894e-04 - val_loss: 1.3244e-04\n",
      "Epoch 129/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 1.2893e-04 - val_loss: 1.3236e-04\n",
      "Epoch 130/200\n",
      "12800/12800 [==============================] - 1s 52us/step - loss: 1.2885e-04 - val_loss: 1.3232e-04\n",
      "Epoch 131/200\n",
      "12800/12800 [==============================] - 1s 48us/step - loss: 1.2883e-04 - val_loss: 1.3223e-04\n",
      "Epoch 132/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 1.2877e-04 - val_loss: 1.3232e-04\n",
      "Epoch 133/200\n",
      "12800/12800 [==============================] - 1s 52us/step - loss: 1.2872e-04 - val_loss: 1.3210e-04\n",
      "Epoch 134/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 1.2869e-04 - val_loss: 1.3215e-04\n",
      "Epoch 135/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 1.2863e-04 - val_loss: 1.3211e-04\n",
      "Epoch 136/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 1.2861e-04 - val_loss: 1.3193e-04\n",
      "Epoch 137/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 1.2855e-04 - val_loss: 1.3190e-04\n",
      "Epoch 138/200\n",
      "12800/12800 [==============================] - 1s 48us/step - loss: 1.2850e-04 - val_loss: 1.3197e-04\n",
      "Epoch 139/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 1.2846e-04 - val_loss: 1.3198e-04\n",
      "Epoch 140/200\n",
      "12800/12800 [==============================] - 1s 48us/step - loss: 1.2845e-04 - val_loss: 1.3180e-04\n",
      "Epoch 141/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 1.2839e-04 - val_loss: 1.3191e-04\n",
      "Epoch 142/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 1.2836e-04 - val_loss: 1.3174e-04\n",
      "Epoch 143/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 1.2832e-04 - val_loss: 1.3180e-04\n",
      "Epoch 144/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 1.2829e-04 - val_loss: 1.3168e-04\n",
      "Epoch 145/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 1.2826e-04 - val_loss: 1.3162e-04\n",
      "Epoch 146/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 1.2823e-04 - val_loss: 1.3166e-04\n",
      "Epoch 147/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 1.2819e-04 - val_loss: 1.3161e-04\n",
      "Epoch 148/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 1.2814e-04 - val_loss: 1.3153e-04\n",
      "Epoch 149/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 1.2811e-04 - val_loss: 1.3154e-04\n",
      "Epoch 150/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 1.2809e-04 - val_loss: 1.3149e-04\n",
      "Epoch 151/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 1.2805e-04 - val_loss: 1.3156e-04\n",
      "Epoch 152/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 1.2802e-04 - val_loss: 1.3147e-04\n",
      "Epoch 153/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 1.2798e-04 - val_loss: 1.3156e-04\n",
      "Epoch 154/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 1.2796e-04 - val_loss: 1.3144e-04\n",
      "Epoch 155/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 1.2793e-04 - val_loss: 1.3142e-04\n",
      "Epoch 156/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 1.2791e-04 - val_loss: 1.3137e-04\n",
      "Epoch 157/200\n",
      "12800/12800 [==============================] - 1s 48us/step - loss: 1.2788e-04 - val_loss: 1.3130e-04\n",
      "Epoch 158/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 1.2784e-04 - val_loss: 1.3135e-04\n",
      "Epoch 159/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 1.2783e-04 - val_loss: 1.3126e-04\n",
      "Epoch 160/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 1.2779e-04 - val_loss: 1.3128e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 1.2777e-04 - val_loss: 1.3123e-04\n",
      "Epoch 162/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 1.2774e-04 - val_loss: 1.3120e-04\n",
      "Epoch 163/200\n",
      "12800/12800 [==============================] - 1s 48us/step - loss: 1.2770e-04 - val_loss: 1.3124e-04\n",
      "Epoch 164/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 1.2768e-04 - val_loss: 1.3116e-04\n",
      "Epoch 165/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 1.2766e-04 - val_loss: 1.3120e-04\n",
      "Epoch 166/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 1.2764e-04 - val_loss: 1.3111e-04\n",
      "Epoch 167/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 1.2762e-04 - val_loss: 1.3108e-04\n",
      "Epoch 168/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 1.2761e-04 - val_loss: 1.3101e-04\n",
      "Epoch 169/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 1.2758e-04 - val_loss: 1.3106e-04\n",
      "Epoch 170/200\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 1.2755e-04 - val_loss: 1.3103e-04\n",
      "Epoch 171/200\n",
      "12800/12800 [==============================] - 1s 56us/step - loss: 1.2753e-04 - val_loss: 1.3097e-04\n",
      "Epoch 172/200\n",
      "12800/12800 [==============================] - 1s 108us/step - loss: 1.2751e-04 - val_loss: 1.3102e-04\n",
      "Epoch 173/200\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 1.2748e-04 - val_loss: 1.3108e-04\n",
      "Epoch 174/200\n",
      "12800/12800 [==============================] - 1s 64us/step - loss: 1.2745e-04 - val_loss: 1.3096e-04\n",
      "Epoch 175/200\n",
      "12800/12800 [==============================] - 1s 96us/step - loss: 1.2745e-04 - val_loss: 1.3088e-04\n",
      "Epoch 176/200\n",
      "12800/12800 [==============================] - 1s 54us/step - loss: 1.2742e-04 - val_loss: 1.3094e-04\n",
      "Epoch 177/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 1.2739e-04 - val_loss: 1.3089e-04\n",
      "Epoch 178/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 1.2737e-04 - val_loss: 1.3090e-04\n",
      "Epoch 179/200\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 1.2736e-04 - val_loss: 1.3089e-04\n",
      "Epoch 180/200\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 1.2735e-04 - val_loss: 1.3085e-04\n",
      "Epoch 181/200\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 1.2733e-04 - val_loss: 1.3080e-04\n",
      "Epoch 182/200\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 1.2730e-04 - val_loss: 1.3079e-04\n",
      "Epoch 183/200\n",
      "12800/12800 [==============================] - 1s 52us/step - loss: 1.2730e-04 - val_loss: 1.3075e-04\n",
      "Epoch 184/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 1.2727e-04 - val_loss: 1.3078e-04\n",
      "Epoch 185/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 1.2725e-04 - val_loss: 1.3074e-04\n",
      "Epoch 186/200\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 1.2723e-04 - val_loss: 1.3081e-04\n",
      "Epoch 187/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 1.2723e-04 - val_loss: 1.3078e-04\n",
      "Epoch 188/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 1.2720e-04 - val_loss: 1.3080e-04\n",
      "Epoch 189/200\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 1.2717e-04 - val_loss: 1.3072e-04\n",
      "Epoch 190/200\n",
      "12800/12800 [==============================] - 1s 52us/step - loss: 1.2716e-04 - val_loss: 1.3061e-04\n",
      "Epoch 191/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 1.2716e-04 - val_loss: 1.3064e-04\n",
      "Epoch 192/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 1.2714e-04 - val_loss: 1.3065e-04\n",
      "Epoch 193/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 1.2712e-04 - val_loss: 1.3070e-04\n",
      "Epoch 194/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 1.2710e-04 - val_loss: 1.3064e-04\n",
      "Epoch 195/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 1.2707e-04 - val_loss: 1.3068e-04\n",
      "Epoch 196/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 1.2708e-04 - val_loss: 1.3059e-04\n",
      "Epoch 197/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 1.2705e-04 - val_loss: 1.3062e-04\n",
      "Epoch 198/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 1.2704e-04 - val_loss: 1.3061e-04\n",
      "Epoch 199/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 1.2702e-04 - val_loss: 1.3061e-04\n",
      "Epoch 200/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 1.2701e-04 - val_loss: 1.3055e-04\n",
      "Test MSE: 0.00013096254\n",
      "Total MSE: 0.00012816954\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input-layer (InputLayer)     (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "hidden-layer-1 (Dense)       (None, 120)               36120     \n",
      "_________________________________________________________________\n",
      "hidden-layer-2 (Dense)       (None, 60)                7260      \n",
      "_________________________________________________________________\n",
      "hidden-layer-3 (Dense)       (None, 120)               7320      \n",
      "_________________________________________________________________\n",
      "output-layer (Dense)         (None, 300)               36300     \n",
      "=================================================================\n",
      "Total params: 87,000\n",
      "Trainable params: 87,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/200\n",
      "12800/12800 [==============================] - 1s 106us/step - loss: 0.0025 - val_loss: 0.0014\n",
      "Epoch 2/200\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 0.0012 - val_loss: 0.0010\n",
      "Epoch 3/200\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 9.3208e-04 - val_loss: 8.5855e-04\n",
      "Epoch 4/200\n",
      "12800/12800 [==============================] - 1s 65us/step - loss: 7.9388e-04 - val_loss: 7.4148e-04\n",
      "Epoch 5/200\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 6.9511e-04 - val_loss: 6.5763e-04\n",
      "Epoch 6/200\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 6.1991e-04 - val_loss: 5.9186e-04\n",
      "Epoch 7/200\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 5.6092e-04 - val_loss: 5.3991e-04\n",
      "Epoch 8/200\n",
      "12800/12800 [==============================] - 1s 68us/step - loss: 5.1322e-04 - val_loss: 4.9676e-04\n",
      "Epoch 9/200\n",
      "12800/12800 [==============================] - 1s 65us/step - loss: 4.7387e-04 - val_loss: 4.6060e-04\n",
      "Epoch 10/200\n",
      "12800/12800 [==============================] - 1s 61us/step - loss: 4.4111e-04 - val_loss: 4.3063e-04\n",
      "Epoch 11/200\n",
      "12800/12800 [==============================] - 1s 65us/step - loss: 4.1367e-04 - val_loss: 4.0657e-04\n",
      "Epoch 12/200\n",
      "12800/12800 [==============================] - 1s 62us/step - loss: 3.9037e-04 - val_loss: 3.8358e-04\n",
      "Epoch 13/200\n",
      "12800/12800 [==============================] - 1s 62us/step - loss: 3.7031e-04 - val_loss: 3.6543e-04\n",
      "Epoch 14/200\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 3.5308e-04 - val_loss: 3.4914e-04\n",
      "Epoch 15/200\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 3.3790e-04 - val_loss: 3.3447e-04\n",
      "Epoch 16/200\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 3.2437e-04 - val_loss: 3.2164e-04\n",
      "Epoch 17/200\n",
      "12800/12800 [==============================] - 2s 138us/step - loss: 3.1237e-04 - val_loss: 3.1023e-04\n",
      "Epoch 18/200\n",
      "12800/12800 [==============================] - 1s 99us/step - loss: 3.0153e-04 - val_loss: 3.0032e-04\n",
      "Epoch 19/200\n",
      "12800/12800 [==============================] - 1s 95us/step - loss: 2.9180e-04 - val_loss: 2.9103e-04\n",
      "Epoch 20/200\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 2.8270e-04 - val_loss: 2.8196e-04\n",
      "Epoch 21/200\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 2.7451e-04 - val_loss: 2.7383e-04\n",
      "Epoch 22/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 1s 71us/step - loss: 2.6685e-04 - val_loss: 2.6708e-04\n",
      "Epoch 23/200\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 2.5977e-04 - val_loss: 2.5988e-04\n",
      "Epoch 24/200\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 2.5323e-04 - val_loss: 2.5315e-04\n",
      "Epoch 25/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 2.4709e-04 - val_loss: 2.4755e-04\n",
      "Epoch 26/200\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 2.4142e-04 - val_loss: 2.4181e-04\n",
      "Epoch 27/200\n",
      "12800/12800 [==============================] - 1s 68us/step - loss: 2.3615e-04 - val_loss: 2.3692e-04\n",
      "Epoch 28/200\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 2.3126e-04 - val_loss: 2.3197e-04\n",
      "Epoch 29/200\n",
      "12800/12800 [==============================] - 1s 61us/step - loss: 2.2664e-04 - val_loss: 2.2740e-04\n",
      "Epoch 30/200\n",
      "12800/12800 [==============================] - 1s 63us/step - loss: 2.2233e-04 - val_loss: 2.2327e-04\n",
      "Epoch 31/200\n",
      "12800/12800 [==============================] - 1s 65us/step - loss: 2.1835e-04 - val_loss: 2.1937e-04\n",
      "Epoch 32/200\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 2.1461e-04 - val_loss: 2.1571e-04\n",
      "Epoch 33/200\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 2.1111e-04 - val_loss: 2.1225e-04\n",
      "Epoch 34/200\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 2.0784e-04 - val_loss: 2.0916e-04\n",
      "Epoch 35/200\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 2.0477e-04 - val_loss: 2.0611e-04\n",
      "Epoch 36/200\n",
      "12800/12800 [==============================] - 1s 62us/step - loss: 2.0188e-04 - val_loss: 2.0322e-04\n",
      "Epoch 37/200\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 1.9920e-04 - val_loss: 2.0053e-04\n",
      "Epoch 38/200\n",
      "12800/12800 [==============================] - 2s 123us/step - loss: 1.9665e-04 - val_loss: 1.9812e-04\n",
      "Epoch 39/200\n",
      "12800/12800 [==============================] - 2s 131us/step - loss: 1.9429e-04 - val_loss: 1.9577e-04\n",
      "Epoch 40/200\n",
      "12800/12800 [==============================] - 2s 118us/step - loss: 1.9202e-04 - val_loss: 1.9374e-04\n",
      "Epoch 41/200\n",
      "12800/12800 [==============================] - 1s 101us/step - loss: 1.8991e-04 - val_loss: 1.9137e-04\n",
      "Epoch 42/200\n",
      "12800/12800 [==============================] - 1s 108us/step - loss: 1.8792e-04 - val_loss: 1.8961e-04\n",
      "Epoch 43/200\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 1.8604e-04 - val_loss: 1.8747e-04\n",
      "Epoch 44/200\n",
      "12800/12800 [==============================] - 1s 97us/step - loss: 1.8423e-04 - val_loss: 1.8575e-04\n",
      "Epoch 45/200\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 1.8253e-04 - val_loss: 1.8434e-04\n",
      "Epoch 46/200\n",
      "12800/12800 [==============================] - 2s 119us/step - loss: 1.8091e-04 - val_loss: 1.8257e-04\n",
      "Epoch 47/200\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 1.7941e-04 - val_loss: 1.8098e-04\n",
      "Epoch 48/200\n",
      "12800/12800 [==============================] - 1s 63us/step - loss: 1.7796e-04 - val_loss: 1.7951e-04\n",
      "Epoch 49/200\n",
      "12800/12800 [==============================] - 1s 61us/step - loss: 1.7656e-04 - val_loss: 1.7812e-04\n",
      "Epoch 50/200\n",
      "12800/12800 [==============================] - 1s 61us/step - loss: 1.7524e-04 - val_loss: 1.7672e-04\n",
      "Epoch 51/200\n",
      "12800/12800 [==============================] - 1s 65us/step - loss: 1.7398e-04 - val_loss: 1.7557e-04\n",
      "Epoch 52/200\n",
      "12800/12800 [==============================] - 1s 63us/step - loss: 1.7278e-04 - val_loss: 1.7432e-04\n",
      "Epoch 53/200\n",
      "12800/12800 [==============================] - 1s 61us/step - loss: 1.7162e-04 - val_loss: 1.7307e-04\n",
      "Epoch 54/200\n",
      "12800/12800 [==============================] - 1s 61us/step - loss: 1.7052e-04 - val_loss: 1.7203e-04\n",
      "Epoch 55/200\n",
      "12800/12800 [==============================] - 1s 61us/step - loss: 1.6938e-04 - val_loss: 1.7086e-04\n",
      "Epoch 56/200\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 1.6836e-04 - val_loss: 1.6993e-04\n",
      "Epoch 57/200\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 1.6736e-04 - val_loss: 1.6889e-04\n",
      "Epoch 58/200\n",
      "12800/12800 [==============================] - 1s 63us/step - loss: 1.6639e-04 - val_loss: 1.6777e-04\n",
      "Epoch 59/200\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 1.6545e-04 - val_loss: 1.6679e-04\n",
      "Epoch 60/200\n",
      "12800/12800 [==============================] - 1s 62us/step - loss: 1.6451e-04 - val_loss: 1.6600e-04\n",
      "Epoch 61/200\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 1.6362e-04 - val_loss: 1.6524e-04\n",
      "Epoch 62/200\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 1.6275e-04 - val_loss: 1.6427e-04\n",
      "Epoch 63/200\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 1.6192e-04 - val_loss: 1.6343e-04\n",
      "Epoch 64/200\n",
      "12800/12800 [==============================] - 1s 92us/step - loss: 1.6108e-04 - val_loss: 1.6250e-04\n",
      "Epoch 65/200\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 1.6030e-04 - val_loss: 1.6181e-04\n",
      "Epoch 66/200\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 1.5951e-04 - val_loss: 1.6092e-04\n",
      "Epoch 67/200\n",
      "12800/12800 [==============================] - 1s 94us/step - loss: 1.5878e-04 - val_loss: 1.6034e-04\n",
      "Epoch 68/200\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 1.5801e-04 - val_loss: 1.5938e-04\n",
      "Epoch 69/200\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 1.5729e-04 - val_loss: 1.5878e-04\n",
      "Epoch 70/200\n",
      "12800/12800 [==============================] - 1s 96us/step - loss: 1.5658e-04 - val_loss: 1.5785e-04\n",
      "Epoch 71/200\n",
      "12800/12800 [==============================] - 1s 93us/step - loss: 1.5588e-04 - val_loss: 1.5728e-04\n",
      "Epoch 72/200\n",
      "12800/12800 [==============================] - 1s 106us/step - loss: 1.5522e-04 - val_loss: 1.5656e-04\n",
      "Epoch 73/200\n",
      "12800/12800 [==============================] - 1s 103us/step - loss: 1.5452e-04 - val_loss: 1.5572e-04\n",
      "Epoch 74/200\n",
      "12800/12800 [==============================] - 1s 99us/step - loss: 1.5388e-04 - val_loss: 1.5515e-04\n",
      "Epoch 75/200\n",
      "12800/12800 [==============================] - 1s 104us/step - loss: 1.5326e-04 - val_loss: 1.5447e-04\n",
      "Epoch 76/200\n",
      "12800/12800 [==============================] - 1s 98us/step - loss: 1.5263e-04 - val_loss: 1.5397e-04\n",
      "Epoch 77/200\n",
      "12800/12800 [==============================] - 1s 97us/step - loss: 1.5200e-04 - val_loss: 1.5331e-04\n",
      "Epoch 78/200\n",
      "12800/12800 [==============================] - 1s 65us/step - loss: 1.5141e-04 - val_loss: 1.5262e-04\n",
      "Epoch 79/200\n",
      "12800/12800 [==============================] - 1s 64us/step - loss: 1.5082e-04 - val_loss: 1.5214e-04\n",
      "Epoch 80/200\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 1.5025e-04 - val_loss: 1.5153e-04\n",
      "Epoch 81/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 1.4967e-04 - val_loss: 1.5107e-04\n",
      "Epoch 82/200\n",
      "12800/12800 [==============================] - 1s 72us/step - loss: 1.4914e-04 - val_loss: 1.5061e-04\n",
      "Epoch 83/200\n",
      "12800/12800 [==============================] - 1s 64us/step - loss: 1.4861e-04 - val_loss: 1.4993e-04\n",
      "Epoch 84/200\n",
      "12800/12800 [==============================] - 1s 61us/step - loss: 1.4810e-04 - val_loss: 1.4949e-04\n",
      "Epoch 85/200\n",
      "12800/12800 [==============================] - 1s 61us/step - loss: 1.4758e-04 - val_loss: 1.4914e-04\n",
      "Epoch 86/200\n",
      "12800/12800 [==============================] - 1s 65us/step - loss: 1.4709e-04 - val_loss: 1.4833e-04\n",
      "Epoch 87/200\n",
      "12800/12800 [==============================] - 1s 63us/step - loss: 1.4660e-04 - val_loss: 1.4807e-04\n",
      "Epoch 88/200\n",
      "12800/12800 [==============================] - 1s 64us/step - loss: 1.4611e-04 - val_loss: 1.4746e-04\n",
      "Epoch 89/200\n",
      "12800/12800 [==============================] - 1s 64us/step - loss: 1.4564e-04 - val_loss: 1.4703e-04\n",
      "Epoch 90/200\n",
      "12800/12800 [==============================] - 1s 65us/step - loss: 1.4520e-04 - val_loss: 1.4667e-04\n",
      "Epoch 91/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 1.4475e-04 - val_loss: 1.4618e-04\n",
      "Epoch 92/200\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 1.4432e-04 - val_loss: 1.4584e-04\n",
      "Epoch 93/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 1s 66us/step - loss: 1.4391e-04 - val_loss: 1.4534e-04\n",
      "Epoch 94/200\n",
      "12800/12800 [==============================] - 1s 61us/step - loss: 1.4351e-04 - val_loss: 1.4490e-04\n",
      "Epoch 95/200\n",
      "12800/12800 [==============================] - 1s 61us/step - loss: 1.4312e-04 - val_loss: 1.4456e-04\n",
      "Epoch 96/200\n",
      "12800/12800 [==============================] - 1s 61us/step - loss: 1.4274e-04 - val_loss: 1.4422e-04\n",
      "Epoch 97/200\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 1.4236e-04 - val_loss: 1.4389e-04\n",
      "Epoch 98/200\n",
      "12800/12800 [==============================] - 1s 63us/step - loss: 1.4198e-04 - val_loss: 1.4362e-04\n",
      "Epoch 99/200\n",
      "12800/12800 [==============================] - 1s 65us/step - loss: 1.4166e-04 - val_loss: 1.4304e-04\n",
      "Epoch 100/200\n",
      "12800/12800 [==============================] - 1s 62us/step - loss: 1.4130e-04 - val_loss: 1.4284e-04\n",
      "Epoch 101/200\n",
      "12800/12800 [==============================] - 1s 63us/step - loss: 1.4098e-04 - val_loss: 1.4258e-04\n",
      "Epoch 102/200\n",
      "12800/12800 [==============================] - 1s 61us/step - loss: 1.4067e-04 - val_loss: 1.4217e-04\n",
      "Epoch 103/200\n",
      "12800/12800 [==============================] - 1s 61us/step - loss: 1.4035e-04 - val_loss: 1.4191e-04\n",
      "Epoch 104/200\n",
      "12800/12800 [==============================] - 1s 61us/step - loss: 1.4006e-04 - val_loss: 1.4157e-04\n",
      "Epoch 105/200\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 1.3975e-04 - val_loss: 1.4136e-04\n",
      "Epoch 106/200\n",
      "12800/12800 [==============================] - 1s 62us/step - loss: 1.3948e-04 - val_loss: 1.4120e-04\n",
      "Epoch 107/200\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 1.3920e-04 - val_loss: 1.4082e-04\n",
      "Epoch 108/200\n",
      "12800/12800 [==============================] - 1s 107us/step - loss: 1.3893e-04 - val_loss: 1.4059e-04\n",
      "Epoch 109/200\n",
      "12800/12800 [==============================] - 2s 122us/step - loss: 1.3868e-04 - val_loss: 1.4027e-04\n",
      "Epoch 110/200\n",
      "12800/12800 [==============================] - 1s 96us/step - loss: 1.3843e-04 - val_loss: 1.4014e-04\n",
      "Epoch 111/200\n",
      "12800/12800 [==============================] - 1s 94us/step - loss: 1.3817e-04 - val_loss: 1.3987e-04\n",
      "Epoch 112/200\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 1.3797e-04 - val_loss: 1.3988e-04\n",
      "Epoch 113/200\n",
      "12800/12800 [==============================] - 1s 94us/step - loss: 1.3772e-04 - val_loss: 1.3946e-04\n",
      "Epoch 114/200\n",
      "12800/12800 [==============================] - 1s 103us/step - loss: 1.3751e-04 - val_loss: 1.3924e-04\n",
      "Epoch 115/200\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 1.3729e-04 - val_loss: 1.3912e-04\n",
      "Epoch 116/200\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 1.3709e-04 - val_loss: 1.3890e-04\n",
      "Epoch 117/200\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 1.3690e-04 - val_loss: 1.3862e-04\n",
      "Epoch 118/200\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 1.3670e-04 - val_loss: 1.3841e-04\n",
      "Epoch 119/200\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 1.3650e-04 - val_loss: 1.3825e-04\n",
      "Epoch 120/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 1.3634e-04 - val_loss: 1.3817e-04\n",
      "Epoch 121/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 1.3616e-04 - val_loss: 1.3804e-04\n",
      "Epoch 122/200\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 1.3600e-04 - val_loss: 1.3776e-04\n",
      "Epoch 123/200\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 1.3582e-04 - val_loss: 1.3772e-04\n",
      "Epoch 124/200\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 1.3565e-04 - val_loss: 1.3764e-04\n",
      "Epoch 125/200\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 1.3552e-04 - val_loss: 1.3742e-04\n",
      "Epoch 126/200\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 1.3537e-04 - val_loss: 1.3721e-04\n",
      "Epoch 127/200\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 1.3520e-04 - val_loss: 1.3714e-04\n",
      "Epoch 128/200\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 1.3507e-04 - val_loss: 1.3694e-04\n",
      "Epoch 129/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 1.3492e-04 - val_loss: 1.3694e-04\n",
      "Epoch 130/200\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 1.3480e-04 - val_loss: 1.3671e-04\n",
      "Epoch 131/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 1.3465e-04 - val_loss: 1.3659e-04\n",
      "Epoch 132/200\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 1.3453e-04 - val_loss: 1.3650e-04\n",
      "Epoch 133/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 1.3441e-04 - val_loss: 1.3641e-04\n",
      "Epoch 134/200\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 1.3427e-04 - val_loss: 1.3626e-04\n",
      "Epoch 135/200\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 1.3415e-04 - val_loss: 1.3611e-04\n",
      "Epoch 136/200\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 1.3407e-04 - val_loss: 1.3598e-04\n",
      "Epoch 137/200\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 1.3395e-04 - val_loss: 1.3590e-04\n",
      "Epoch 138/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 1.3382e-04 - val_loss: 1.3584e-04\n",
      "Epoch 139/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 1.3372e-04 - val_loss: 1.3586e-04\n",
      "Epoch 140/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 1.3362e-04 - val_loss: 1.3562e-04\n",
      "Epoch 141/200\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 1.3351e-04 - val_loss: 1.3562e-04\n",
      "Epoch 142/200\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 1.3343e-04 - val_loss: 1.3545e-04\n",
      "Epoch 143/200\n",
      "12800/12800 [==============================] - 1s 94us/step - loss: 1.3332e-04 - val_loss: 1.3551e-04\n",
      "Epoch 144/200\n",
      "12800/12800 [==============================] - 1s 105us/step - loss: 1.3323e-04 - val_loss: 1.3545e-04\n",
      "Epoch 145/200\n",
      "12800/12800 [==============================] - 1s 102us/step - loss: 1.3313e-04 - val_loss: 1.3522e-04\n",
      "Epoch 146/200\n",
      "12800/12800 [==============================] - 1s 102us/step - loss: 1.3304e-04 - val_loss: 1.3519e-04\n",
      "Epoch 147/200\n",
      "12800/12800 [==============================] - 1s 106us/step - loss: 1.3294e-04 - val_loss: 1.3507e-04\n",
      "Epoch 148/200\n",
      "12800/12800 [==============================] - 1s 92us/step - loss: 1.3286e-04 - val_loss: 1.3503e-04\n",
      "Epoch 149/200\n",
      "12800/12800 [==============================] - 1s 93us/step - loss: 1.3277e-04 - val_loss: 1.3489e-04\n",
      "Epoch 150/200\n",
      "12800/12800 [==============================] - 2s 130us/step - loss: 1.3267e-04 - val_loss: 1.3483e-04\n",
      "Epoch 151/200\n",
      "12800/12800 [==============================] - 1s 88us/step - loss: 1.3262e-04 - val_loss: 1.3477e-04\n",
      "Epoch 152/200\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 1.3253e-04 - val_loss: 1.3477e-04\n",
      "Epoch 153/200\n",
      "12800/12800 [==============================] - 1s 107us/step - loss: 1.3246e-04 - val_loss: 1.3453e-04\n",
      "Epoch 154/200\n",
      "12800/12800 [==============================] - 1s 96us/step - loss: 1.3239e-04 - val_loss: 1.3451e-04\n",
      "Epoch 155/200\n",
      "12800/12800 [==============================] - 2s 120us/step - loss: 1.3232e-04 - val_loss: 1.3449e-04\n",
      "Epoch 156/200\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 1.3224e-04 - val_loss: 1.3450e-04\n",
      "Epoch 157/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 1.3216e-04 - val_loss: 1.3429e-04\n",
      "Epoch 158/200\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 1.3209e-04 - val_loss: 1.3419e-04\n",
      "Epoch 159/200\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 1.3203e-04 - val_loss: 1.3414e-04\n",
      "Epoch 160/200\n",
      "12800/12800 [==============================] - 2s 144us/step - loss: 1.3196e-04 - val_loss: 1.3414e-04\n",
      "Epoch 161/200\n",
      "12800/12800 [==============================] - 2s 124us/step - loss: 1.3189e-04 - val_loss: 1.3396e-04\n",
      "Epoch 162/200\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 1.3181e-04 - val_loss: 1.3404e-04\n",
      "Epoch 163/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 1s 78us/step - loss: 1.3176e-04 - val_loss: 1.3392e-04\n",
      "Epoch 164/200\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 1.3170e-04 - val_loss: 1.3388e-04\n",
      "Epoch 165/200\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 1.3163e-04 - val_loss: 1.3378e-04\n",
      "Epoch 166/200\n",
      "12800/12800 [==============================] - 1s 65us/step - loss: 1.3157e-04 - val_loss: 1.3375e-04\n",
      "Epoch 167/200\n",
      "12800/12800 [==============================] - 1s 62us/step - loss: 1.3153e-04 - val_loss: 1.3375e-04\n",
      "Epoch 168/200\n",
      "12800/12800 [==============================] - 1s 61us/step - loss: 1.3147e-04 - val_loss: 1.3363e-04\n",
      "Epoch 169/200\n",
      "12800/12800 [==============================] - 1s 61us/step - loss: 1.3141e-04 - val_loss: 1.3366e-04\n",
      "Epoch 170/200\n",
      "12800/12800 [==============================] - 1s 72us/step - loss: 1.3134e-04 - val_loss: 1.3365e-04\n",
      "Epoch 171/200\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 1.3127e-04 - val_loss: 1.3347e-04\n",
      "Epoch 172/200\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 1.3122e-04 - val_loss: 1.3342e-04\n",
      "Epoch 173/200\n",
      "12800/12800 [==============================] - 1s 64us/step - loss: 1.3118e-04 - val_loss: 1.3336e-04\n",
      "Epoch 174/200\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 1.3113e-04 - val_loss: 1.3334e-04\n",
      "Epoch 175/200\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 1.3106e-04 - val_loss: 1.3334e-04\n",
      "Epoch 176/200\n",
      "12800/12800 [==============================] - 1s 64us/step - loss: 1.3103e-04 - val_loss: 1.3331e-04\n",
      "Epoch 177/200\n",
      "12800/12800 [==============================] - 1s 65us/step - loss: 1.3097e-04 - val_loss: 1.3315e-04\n",
      "Epoch 178/200\n",
      "12800/12800 [==============================] - 1s 64us/step - loss: 1.3091e-04 - val_loss: 1.3320e-04\n",
      "Epoch 179/200\n",
      "12800/12800 [==============================] - 1s 61us/step - loss: 1.3087e-04 - val_loss: 1.3318e-04\n",
      "Epoch 180/200\n",
      "12800/12800 [==============================] - 1s 62us/step - loss: 1.3084e-04 - val_loss: 1.3306e-04\n",
      "Epoch 181/200\n",
      "12800/12800 [==============================] - 1s 62us/step - loss: 1.3077e-04 - val_loss: 1.3303e-04\n",
      "Epoch 182/200\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 1.3073e-04 - val_loss: 1.3298e-04\n",
      "Epoch 183/200\n",
      "12800/12800 [==============================] - 1s 117us/step - loss: 1.3067e-04 - val_loss: 1.3287e-04\n",
      "Epoch 184/200\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 1.3064e-04 - val_loss: 1.3291e-04\n",
      "Epoch 185/200\n",
      "12800/12800 [==============================] - 1s 95us/step - loss: 1.3060e-04 - val_loss: 1.3281e-04\n",
      "Epoch 186/200\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 1.3056e-04 - val_loss: 1.3270e-04\n",
      "Epoch 187/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 1.3050e-04 - val_loss: 1.3288e-04\n",
      "Epoch 188/200\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 1.3049e-04 - val_loss: 1.3255e-04\n",
      "Epoch 189/200\n",
      "12800/12800 [==============================] - 1s 107us/step - loss: 1.3042e-04 - val_loss: 1.3267e-04\n",
      "Epoch 190/200\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 1.3039e-04 - val_loss: 1.3256e-04\n",
      "Epoch 191/200\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 1.3035e-04 - val_loss: 1.3262e-04\n",
      "Epoch 192/200\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 1.3030e-04 - val_loss: 1.3259e-04\n",
      "Epoch 193/200\n",
      "12800/12800 [==============================] - 1s 69us/step - loss: 1.3026e-04 - val_loss: 1.3257e-04\n",
      "Epoch 194/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 1.3023e-04 - val_loss: 1.3254e-04\n",
      "Epoch 195/200\n",
      "12800/12800 [==============================] - 1s 72us/step - loss: 1.3018e-04 - val_loss: 1.3243e-04\n",
      "Epoch 196/200\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 1.3015e-04 - val_loss: 1.3254e-04\n",
      "Epoch 197/200\n",
      "12800/12800 [==============================] - 1s 88us/step - loss: 1.3011e-04 - val_loss: 1.3244e-04\n",
      "Epoch 198/200\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 1.3007e-04 - val_loss: 1.3247e-04\n",
      "Epoch 199/200\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 1.3003e-04 - val_loss: 1.3238e-04\n",
      "Epoch 200/200\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 1.2999e-04 - val_loss: 1.3227e-04\n",
      "Test MSE: 0.0001277886\n",
      "Total MSE: 0.000129652\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 50)                15050     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 300)               15300     \n",
      "=================================================================\n",
      "Total params: 30,350\n",
      "Trainable params: 30,350\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/200\n",
      "12800/12800 [==============================] - 1s 69us/step - loss: 0.0018 - val_loss: 0.0011\n",
      "Epoch 2/200\n",
      "12800/12800 [==============================] - ETA: 0s - loss: 9.4920e-0 - 1s 60us/step - loss: 9.4719e-04 - val_loss: 8.1875e-04\n",
      "Epoch 3/200\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 7.3856e-04 - val_loss: 6.6815e-04\n",
      "Epoch 4/200\n",
      "12800/12800 [==============================] - 1s 72us/step - loss: 6.1902e-04 - val_loss: 5.7423e-04\n",
      "Epoch 5/200\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 5.3969e-04 - val_loss: 5.1008e-04\n",
      "Epoch 6/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 4.8409e-04 - val_loss: 4.6311e-04\n",
      "Epoch 7/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 4.4301e-04 - val_loss: 4.2784e-04\n",
      "Epoch 8/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 4.1141e-04 - val_loss: 3.9938e-04\n",
      "Epoch 9/200\n",
      "12800/12800 [==============================] - 1s 44us/step - loss: 3.8624e-04 - val_loss: 3.7663e-04\n",
      "Epoch 10/200\n",
      "12800/12800 [==============================] - 1s 43us/step - loss: 3.6542e-04 - val_loss: 3.5795e-04\n",
      "Epoch 11/200\n",
      "12800/12800 [==============================] - 1s 44us/step - loss: 3.4785e-04 - val_loss: 3.4191e-04\n",
      "Epoch 12/200\n",
      "12800/12800 [==============================] - 1s 44us/step - loss: 3.3273e-04 - val_loss: 3.2762e-04\n",
      "Epoch 13/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 3.1958e-04 - val_loss: 3.1535e-04\n",
      "Epoch 14/200\n",
      "12800/12800 [==============================] - 1s 54us/step - loss: 3.0815e-04 - val_loss: 3.0463e-04\n",
      "Epoch 15/200\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 2.9816e-04 - val_loss: 2.9519e-04\n",
      "Epoch 16/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 2.8946e-04 - val_loss: 2.8711e-04\n",
      "Epoch 17/200\n",
      "12800/12800 [==============================] - 1s 72us/step - loss: 2.8177e-04 - val_loss: 2.7940e-04\n",
      "Epoch 18/200\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 2.7497e-04 - val_loss: 2.7319e-04\n",
      "Epoch 19/200\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 2.6907e-04 - val_loss: 2.6751e-04\n",
      "Epoch 20/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 2.6370e-04 - val_loss: 2.6231e-04\n",
      "Epoch 21/200\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 2.5889e-04 - val_loss: 2.5821e-04\n",
      "Epoch 22/200\n",
      "12800/12800 [==============================] - 1s 65us/step - loss: 2.5459e-04 - val_loss: 2.5391e-04\n",
      "Epoch 23/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 2.5070e-04 - val_loss: 2.5024e-04\n",
      "Epoch 24/200\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 2.4725e-04 - val_loss: 2.4716e-04\n",
      "Epoch 25/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 2.4402e-04 - val_loss: 2.4389e-04\n",
      "Epoch 26/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 1s 46us/step - loss: 2.4107e-04 - val_loss: 2.4107e-04\n",
      "Epoch 27/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 2.3839e-04 - val_loss: 2.3863e-04\n",
      "Epoch 28/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 2.3596e-04 - val_loss: 2.3641e-04\n",
      "Epoch 29/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 2.3367e-04 - val_loss: 2.3407e-04\n",
      "Epoch 30/200\n",
      "12800/12800 [==============================] - 1s 44us/step - loss: 2.3162e-04 - val_loss: 2.3236e-04\n",
      "Epoch 31/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 2.2972e-04 - val_loss: 2.3039e-04\n",
      "Epoch 32/200\n",
      "12800/12800 [==============================] - 1s 44us/step - loss: 2.2802e-04 - val_loss: 2.2878e-04\n",
      "Epoch 33/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 2.2638e-04 - val_loss: 2.2728e-04\n",
      "Epoch 34/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 2.2489e-04 - val_loss: 2.2583e-04\n",
      "Epoch 35/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 2.2351e-04 - val_loss: 2.2466e-04\n",
      "Epoch 36/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 2.2220e-04 - val_loss: 2.2327e-04\n",
      "Epoch 37/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 2.2105e-04 - val_loss: 2.2225e-04\n",
      "Epoch 38/200\n",
      "12800/12800 [==============================] - 1s 44us/step - loss: 2.1998e-04 - val_loss: 2.2122e-04\n",
      "Epoch 39/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 2.1891e-04 - val_loss: 2.2039e-04\n",
      "Epoch 40/200\n",
      "12800/12800 [==============================] - 1s 44us/step - loss: 2.1798e-04 - val_loss: 2.1934e-04\n",
      "Epoch 41/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 2.1710e-04 - val_loss: 2.1875e-04\n",
      "Epoch 42/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 2.1627e-04 - val_loss: 2.1775e-04\n",
      "Epoch 43/200\n",
      "12800/12800 [==============================] - 1s 44us/step - loss: 2.1552e-04 - val_loss: 2.1703e-04\n",
      "Epoch 44/200\n",
      "12800/12800 [==============================] - 1s 43us/step - loss: 2.1478e-04 - val_loss: 2.1634e-04\n",
      "Epoch 45/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 2.1414e-04 - val_loss: 2.1592e-04\n",
      "Epoch 46/200\n",
      "12800/12800 [==============================] - 1s 44us/step - loss: 2.1352e-04 - val_loss: 2.1529e-04\n",
      "Epoch 47/200\n",
      "12800/12800 [==============================] - 1s 48us/step - loss: 2.1295e-04 - val_loss: 2.1470e-04\n",
      "Epoch 48/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 2.1239e-04 - val_loss: 2.1419e-04\n",
      "Epoch 49/200\n",
      "12800/12800 [==============================] - 1s 44us/step - loss: 2.1191e-04 - val_loss: 2.1391e-04\n",
      "Epoch 50/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 2.1143e-04 - val_loss: 2.1334e-04\n",
      "Epoch 51/200\n",
      "12800/12800 [==============================] - 1s 64us/step - loss: 2.1099e-04 - val_loss: 2.1299e-04\n",
      "Epoch 52/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 2.1057e-04 - val_loss: 2.1273e-04\n",
      "Epoch 53/200\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 2.1020e-04 - val_loss: 2.1224e-04\n",
      "Epoch 54/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 2.0982e-04 - val_loss: 2.1217e-04\n",
      "Epoch 55/200\n",
      "12800/12800 [==============================] - 1s 53us/step - loss: 2.0947e-04 - val_loss: 2.1180e-04\n",
      "Epoch 56/200\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 2.0916e-04 - val_loss: 2.1146e-04\n",
      "Epoch 57/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 2.0885e-04 - val_loss: 2.1122e-04\n",
      "Epoch 58/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 2.0857e-04 - val_loss: 2.1086e-04\n",
      "Epoch 59/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 2.0826e-04 - val_loss: 2.1065e-04\n",
      "Epoch 60/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 2.0803e-04 - val_loss: 2.1057e-04\n",
      "Epoch 61/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 2.0778e-04 - val_loss: 2.1029e-04\n",
      "Epoch 62/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 2.0755e-04 - val_loss: 2.0998e-04\n",
      "Epoch 63/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 2.0736e-04 - val_loss: 2.0982e-04\n",
      "Epoch 64/200\n",
      "12800/12800 [==============================] - 1s 44us/step - loss: 2.0715e-04 - val_loss: 2.0974e-04\n",
      "Epoch 65/200\n",
      "12800/12800 [==============================] - 1s 115us/step - loss: 2.0693e-04 - val_loss: 2.0970e-04\n",
      "Epoch 66/200\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 2.0676e-04 - val_loss: 2.0946e-04\n",
      "Epoch 67/200\n",
      "12800/12800 [==============================] - 1s 95us/step - loss: 2.0656e-04 - val_loss: 2.0939e-04\n",
      "Epoch 68/200\n",
      "12800/12800 [==============================] - 1s 88us/step - loss: 2.0640e-04 - val_loss: 2.0929e-04\n",
      "Epoch 69/200\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 2.0625e-04 - val_loss: 2.0906e-04\n",
      "Epoch 70/200\n",
      "12800/12800 [==============================] - 1s 68us/step - loss: 2.0607e-04 - val_loss: 2.0892e-04\n",
      "Epoch 71/200\n",
      "12800/12800 [==============================] - 1s 57us/step - loss: 2.0596e-04 - val_loss: 2.0876e-04\n",
      "Epoch 72/200\n",
      "12800/12800 [==============================] - 1s 109us/step - loss: 2.0582e-04 - val_loss: 2.0862e-04\n",
      "Epoch 73/200\n",
      "12800/12800 [==============================] - 1s 109us/step - loss: 2.0568e-04 - val_loss: 2.0854e-04\n",
      "Epoch 74/200\n",
      "12800/12800 [==============================] - 1s 110us/step - loss: 2.0557e-04 - val_loss: 2.0832e-04\n",
      "Epoch 75/200\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 2.0544e-04 - val_loss: 2.0825e-04\n",
      "Epoch 76/200\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 2.0535e-04 - val_loss: 2.0822e-04\n",
      "Epoch 77/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 2.0523e-04 - val_loss: 2.0810e-04\n",
      "Epoch 78/200\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 2.0513e-04 - val_loss: 2.0800e-04\n",
      "Epoch 79/200\n",
      "12800/12800 [==============================] - 1s 72us/step - loss: 2.0500e-04 - val_loss: 2.0811e-04\n",
      "Epoch 80/200\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 2.0491e-04 - val_loss: 2.0793e-04\n",
      "Epoch 81/200\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 2.0481e-04 - val_loss: 2.0767e-04\n",
      "Epoch 82/200\n",
      "12800/12800 [==============================] - 1s 54us/step - loss: 2.0474e-04 - val_loss: 2.0776e-04\n",
      "Epoch 83/200\n",
      "12800/12800 [==============================] - 1s 63us/step - loss: 2.0462e-04 - val_loss: 2.0774e-04\n",
      "Epoch 84/200\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 2.0456e-04 - val_loss: 2.0762e-04\n",
      "Epoch 85/200\n",
      "12800/12800 [==============================] - 1s 56us/step - loss: 2.0448e-04 - val_loss: 2.0756e-04\n",
      "Epoch 86/200\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 2.0440e-04 - val_loss: 2.0746e-04\n",
      "Epoch 87/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 2.0434e-04 - val_loss: 2.0747e-04\n",
      "Epoch 88/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 2.0424e-04 - val_loss: 2.0735e-04\n",
      "Epoch 89/200\n",
      "12800/12800 [==============================] - 1s 53us/step - loss: 2.0419e-04 - val_loss: 2.0725e-04\n",
      "Epoch 90/200\n",
      "12800/12800 [==============================] - 1s 62us/step - loss: 2.0414e-04 - val_loss: 2.0721e-04\n",
      "Epoch 91/200\n",
      "12800/12800 [==============================] - 1s 56us/step - loss: 2.0405e-04 - val_loss: 2.0715e-04\n",
      "Epoch 92/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 2.0401e-04 - val_loss: 2.0714e-04\n",
      "Epoch 93/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 2.0396e-04 - val_loss: 2.0704e-04\n",
      "Epoch 94/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 2.0389e-04 - val_loss: 2.0710e-04\n",
      "Epoch 95/200\n",
      "12800/12800 [==============================] - 1s 52us/step - loss: 2.0384e-04 - val_loss: 2.0695e-04\n",
      "Epoch 96/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 2.0379e-04 - val_loss: 2.0689e-04\n",
      "Epoch 97/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 1s 51us/step - loss: 2.0373e-04 - val_loss: 2.0690e-04\n",
      "Epoch 98/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 2.0366e-04 - val_loss: 2.0687e-04\n",
      "Epoch 99/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 2.0361e-04 - val_loss: 2.0696e-04\n",
      "Epoch 100/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 2.0356e-04 - val_loss: 2.0671e-04\n",
      "Epoch 101/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 2.0352e-04 - val_loss: 2.0672e-04\n",
      "Epoch 102/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 2.0345e-04 - val_loss: 2.0675e-04\n",
      "Epoch 103/200\n",
      "12800/12800 [==============================] - 1s 48us/step - loss: 2.0345e-04 - val_loss: 2.0650e-04\n",
      "Epoch 104/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 2.0338e-04 - val_loss: 2.0665e-04\n",
      "Epoch 105/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 2.0333e-04 - val_loss: 2.0667e-04\n",
      "Epoch 106/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 2.0333e-04 - val_loss: 2.0649e-04\n",
      "Epoch 107/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 2.0325e-04 - val_loss: 2.0674e-04\n",
      "Epoch 108/200\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 2.0321e-04 - val_loss: 2.0655e-04\n",
      "Epoch 109/200\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 2.0319e-04 - val_loss: 2.0657e-04\n",
      "Epoch 110/200\n",
      "12800/12800 [==============================] - 1s 65us/step - loss: 2.0316e-04 - val_loss: 2.0644e-04\n",
      "Epoch 111/200\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 2.0313e-04 - val_loss: 2.0636e-04\n",
      "Epoch 112/200\n",
      "12800/12800 [==============================] - 1s 57us/step - loss: 2.0308e-04 - val_loss: 2.0632e-04\n",
      "Epoch 113/200\n",
      "12800/12800 [==============================] - 1s 61us/step - loss: 2.0302e-04 - val_loss: 2.0636e-04\n",
      "Epoch 114/200\n",
      "12800/12800 [==============================] - 1s 54us/step - loss: 2.0300e-04 - val_loss: 2.0630e-04\n",
      "Epoch 115/200\n",
      "12800/12800 [==============================] - 1s 65us/step - loss: 2.0298e-04 - val_loss: 2.0628e-04\n",
      "Epoch 116/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 2.0294e-04 - val_loss: 2.0622e-04\n",
      "Epoch 117/200\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 2.0293e-04 - val_loss: 2.0615e-04\n",
      "Epoch 118/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 2.0290e-04 - val_loss: 2.0632e-04\n",
      "Epoch 119/200\n",
      "12800/12800 [==============================] - 1s 57us/step - loss: 2.0286e-04 - val_loss: 2.0610e-04\n",
      "Epoch 120/200\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 2.0287e-04 - val_loss: 2.0613e-04\n",
      "Epoch 121/200\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 2.0281e-04 - val_loss: 2.0615e-04\n",
      "Epoch 122/200\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 2.0278e-04 - val_loss: 2.0618e-04\n",
      "Epoch 123/200\n",
      "12800/12800 [==============================] - 1s 56us/step - loss: 2.0275e-04 - val_loss: 2.0604e-04\n",
      "Epoch 124/200\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 2.0273e-04 - val_loss: 2.0619e-04\n",
      "Epoch 125/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 2.0270e-04 - val_loss: 2.0612e-04\n",
      "Epoch 126/200\n",
      "12800/12800 [==============================] - 1s 56us/step - loss: 2.0267e-04 - val_loss: 2.0592e-04\n",
      "Epoch 127/200\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 2.0265e-04 - val_loss: 2.0603e-04\n",
      "Epoch 128/200\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 2.0263e-04 - val_loss: 2.0600e-04\n",
      "Epoch 129/200\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 2.0262e-04 - val_loss: 2.0607e-04\n",
      "Epoch 130/200\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 2.0258e-04 - val_loss: 2.0614e-04\n",
      "Epoch 131/200\n",
      "12800/12800 [==============================] - 1s 69us/step - loss: 2.0253e-04 - val_loss: 2.0586e-04\n",
      "Epoch 132/200\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 2.0255e-04 - val_loss: 2.0588e-04\n",
      "Epoch 133/200\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 2.0252e-04 - val_loss: 2.0596e-04\n",
      "Epoch 134/200\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 2.0250e-04 - val_loss: 2.0583e-04\n",
      "Epoch 135/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 2.0248e-04 - val_loss: 2.0586e-04\n",
      "Epoch 136/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 2.0245e-04 - val_loss: 2.0587e-04\n",
      "Epoch 137/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 2.0243e-04 - val_loss: 2.0587e-04\n",
      "Epoch 138/200\n",
      "12800/12800 [==============================] - 1s 57us/step - loss: 2.0242e-04 - val_loss: 2.0584e-04\n",
      "Epoch 139/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 2.0241e-04 - val_loss: 2.0572e-04\n",
      "Epoch 140/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 2.0237e-04 - val_loss: 2.0583e-04\n",
      "Epoch 141/200\n",
      "12800/12800 [==============================] - 1s 52us/step - loss: 2.0236e-04 - val_loss: 2.0577e-04\n",
      "Epoch 142/200\n",
      "12800/12800 [==============================] - 1s 72us/step - loss: 2.0234e-04 - val_loss: 2.0576e-04\n",
      "Epoch 143/200\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 2.0232e-04 - val_loss: 2.0574e-04\n",
      "Epoch 144/200\n",
      "12800/12800 [==============================] - 1s 93us/step - loss: 2.0230e-04 - val_loss: 2.0579e-04\n",
      "Epoch 145/200\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 2.0228e-04 - val_loss: 2.0567e-04\n",
      "Epoch 146/200\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 2.0229e-04 - val_loss: 2.0565e-04\n",
      "Epoch 147/200\n",
      "12800/12800 [==============================] - 1s 57us/step - loss: 2.0226e-04 - val_loss: 2.0561e-04\n",
      "Epoch 148/200\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 2.0224e-04 - val_loss: 2.0569e-04\n",
      "Epoch 149/200\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 2.0222e-04 - val_loss: 2.0556e-04\n",
      "Epoch 150/200\n",
      "12800/12800 [==============================] - 1s 63us/step - loss: 2.0221e-04 - val_loss: 2.0569e-04\n",
      "Epoch 151/200\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 2.0219e-04 - val_loss: 2.0559e-04\n",
      "Epoch 152/200\n",
      "12800/12800 [==============================] - 1s 52us/step - loss: 2.0216e-04 - val_loss: 2.0557e-04\n",
      "Epoch 153/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 2.0216e-04 - val_loss: 2.0551e-04\n",
      "Epoch 154/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 2.0211e-04 - val_loss: 2.0565e-04\n",
      "Epoch 155/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 2.0214e-04 - val_loss: 2.0557e-04\n",
      "Epoch 156/200\n",
      "12800/12800 [==============================] - 1s 48us/step - loss: 2.0211e-04 - val_loss: 2.0553e-04\n",
      "Epoch 157/200\n",
      "12800/12800 [==============================] - 1s 54us/step - loss: 2.0209e-04 - val_loss: 2.0563e-04\n",
      "Epoch 158/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 2.0210e-04 - val_loss: 2.0554e-04\n",
      "Epoch 159/200\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 2.0209e-04 - val_loss: 2.0550e-04\n",
      "Epoch 160/200\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 2.0205e-04 - val_loss: 2.0545e-04\n",
      "Epoch 161/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 2.0205e-04 - val_loss: 2.0547e-04\n",
      "Epoch 162/200\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 2.0202e-04 - val_loss: 2.0552e-04\n",
      "Epoch 163/200\n",
      "12800/12800 [==============================] - 1s 62us/step - loss: 2.0203e-04 - val_loss: 2.0555e-04\n",
      "Epoch 164/200\n",
      "12800/12800 [==============================] - 1s 56us/step - loss: 2.0201e-04 - val_loss: 2.0552e-04\n",
      "Epoch 165/200\n",
      "12800/12800 [==============================] - 1s 54us/step - loss: 2.0198e-04 - val_loss: 2.0550e-04\n",
      "Epoch 166/200\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 2.0200e-04 - val_loss: 2.0536e-04\n",
      "Epoch 167/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 2.0196e-04 - val_loss: 2.0552e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 168/200\n",
      "12800/12800 [==============================] - 1s 65us/step - loss: 2.0195e-04 - val_loss: 2.0540e-04\n",
      "Epoch 169/200\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 2.0195e-04 - val_loss: 2.0534e-04\n",
      "Epoch 170/200\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 2.0195e-04 - val_loss: 2.0535e-04\n",
      "Epoch 171/200\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 2.0194e-04 - val_loss: 2.0532e-04\n",
      "Epoch 172/200\n",
      "12800/12800 [==============================] - 1s 53us/step - loss: 2.0192e-04 - val_loss: 2.0537e-04\n",
      "Epoch 173/200\n",
      "12800/12800 [==============================] - 1s 52us/step - loss: 2.0190e-04 - val_loss: 2.0542e-04\n",
      "Epoch 174/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 2.0190e-04 - val_loss: 2.0532e-04\n",
      "Epoch 175/200\n",
      "12800/12800 [==============================] - 1s 48us/step - loss: 2.0187e-04 - val_loss: 2.0529e-04\n",
      "Epoch 176/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 2.0188e-04 - val_loss: 2.0533e-04\n",
      "Epoch 177/200\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 2.0185e-04 - val_loss: 2.0536e-04\n",
      "Epoch 178/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 2.0186e-04 - val_loss: 2.0529e-04\n",
      "Epoch 179/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 2.0184e-04 - val_loss: 2.0528e-04\n",
      "Epoch 180/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 2.0183e-04 - val_loss: 2.0526e-04\n",
      "Epoch 181/200\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 2.0180e-04 - val_loss: 2.0539e-04\n",
      "Epoch 182/200\n",
      "12800/12800 [==============================] - 1s 57us/step - loss: 2.0182e-04 - val_loss: 2.0529e-04\n",
      "Epoch 183/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 2.0180e-04 - val_loss: 2.0527e-04\n",
      "Epoch 184/200\n",
      "12800/12800 [==============================] - 1s 48us/step - loss: 2.0180e-04 - val_loss: 2.0526e-04\n",
      "Epoch 185/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 2.0180e-04 - val_loss: 2.0520e-04\n",
      "Epoch 186/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 2.0178e-04 - val_loss: 2.0528e-04\n",
      "Epoch 187/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 2.0176e-04 - val_loss: 2.0524e-04\n",
      "Epoch 188/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 2.0176e-04 - val_loss: 2.0518e-04\n",
      "Epoch 189/200\n",
      "12800/12800 [==============================] - 1s 52us/step - loss: 2.0173e-04 - val_loss: 2.0523e-04\n",
      "Epoch 190/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 2.0175e-04 - val_loss: 2.0528e-04\n",
      "Epoch 191/200\n",
      "12800/12800 [==============================] - 1s 48us/step - loss: 2.0174e-04 - val_loss: 2.0526e-04\n",
      "Epoch 192/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 2.0172e-04 - val_loss: 2.0515e-04\n",
      "Epoch 193/200\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 2.0171e-04 - val_loss: 2.0524e-04\n",
      "Epoch 194/200\n",
      "12800/12800 [==============================] - 1s 63us/step - loss: 2.0172e-04 - val_loss: 2.0518e-04\n",
      "Epoch 195/200\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 2.0170e-04 - val_loss: 2.0525e-04\n",
      "Epoch 196/200\n",
      "12800/12800 [==============================] - 1s 57us/step - loss: 2.0168e-04 - val_loss: 2.0513e-04\n",
      "Epoch 197/200\n",
      "12800/12800 [==============================] - 1s 56us/step - loss: 2.0168e-04 - val_loss: 2.0502e-04\n",
      "Epoch 198/200\n",
      "12800/12800 [==============================] - 1s 64us/step - loss: 2.0166e-04 - val_loss: 2.0509e-04\n",
      "Epoch 199/200\n",
      "12800/12800 [==============================] - 1s 63us/step - loss: 2.0167e-04 - val_loss: 2.0508e-04\n",
      "Epoch 200/200\n",
      "12800/12800 [==============================] - 1s 64us/step - loss: 2.0163e-04 - val_loss: 2.0512e-04\n",
      "Test MSE: 0.00021637676\n",
      "Total MSE: 0.00020491519\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input-layer (InputLayer)     (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "hidden-layer-1 (Dense)       (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "hidden-layer-2 (Dense)       (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "hidden-layer-3 (Dense)       (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "output-layer (Dense)         (None, 300)               30300     \n",
      "=================================================================\n",
      "Total params: 70,550\n",
      "Trainable params: 70,550\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/200\n",
      "12800/12800 [==============================] - 2s 172us/step - loss: 0.0024 - val_loss: 0.0014\n",
      "Epoch 2/200\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 3/200\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 9.8740e-04 - val_loss: 9.3848e-04\n",
      "Epoch 4/200\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 8.5852e-04 - val_loss: 8.3157e-04\n",
      "Epoch 5/200\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 7.6510e-04 - val_loss: 7.4949e-04\n",
      "Epoch 6/200\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 6.9178e-04 - val_loss: 6.8457e-04\n",
      "Epoch 7/200\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 6.3277e-04 - val_loss: 6.3156e-04\n",
      "Epoch 8/200\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 5.8454e-04 - val_loss: 5.8756e-04\n",
      "Epoch 9/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 5.4486e-04 - val_loss: 5.5140e-04\n",
      "Epoch 10/200\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 5.1181e-04 - val_loss: 5.2208e-04\n",
      "Epoch 11/200\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 4.8394e-04 - val_loss: 4.9553e-04\n",
      "Epoch 12/200\n",
      "12800/12800 [==============================] - 1s 72us/step - loss: 4.6023e-04 - val_loss: 4.7255e-04\n",
      "Epoch 13/200\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 4.3974e-04 - val_loss: 4.5384e-04\n",
      "Epoch 14/200\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 4.2185e-04 - val_loss: 4.3608e-04\n",
      "Epoch 15/200\n",
      "12800/12800 [==============================] - 1s 68us/step - loss: 4.0606e-04 - val_loss: 4.2005e-04\n",
      "Epoch 16/200\n",
      "12800/12800 [==============================] - 1s 65us/step - loss: 3.9179e-04 - val_loss: 4.0629e-04\n",
      "Epoch 17/200\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 3.7892e-04 - val_loss: 3.9360e-04\n",
      "Epoch 18/200\n",
      "12800/12800 [==============================] - 1s 99us/step - loss: 3.6711e-04 - val_loss: 3.8210e-04\n",
      "Epoch 19/200\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 3.5628e-04 - val_loss: 3.7180e-04\n",
      "Epoch 20/200\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 3.4640e-04 - val_loss: 3.6148e-04\n",
      "Epoch 21/200\n",
      "12800/12800 [==============================] - 1s 111us/step - loss: 3.3715e-04 - val_loss: 3.5218e-04\n",
      "Epoch 22/200\n",
      "12800/12800 [==============================] - 1s 88us/step - loss: 3.2863e-04 - val_loss: 3.4377e-04\n",
      "Epoch 23/200\n",
      "12800/12800 [==============================] - 1s 68us/step - loss: 3.2080e-04 - val_loss: 3.3619e-04\n",
      "Epoch 24/200\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 3.1351e-04 - val_loss: 3.2902e-04\n",
      "Epoch 25/200\n",
      "12800/12800 [==============================] - 1s 64us/step - loss: 3.0680e-04 - val_loss: 3.2173e-04\n",
      "Epoch 26/200\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 3.0048e-04 - val_loss: 3.1589e-04\n",
      "Epoch 27/200\n",
      "12800/12800 [==============================] - 1s 116us/step - loss: 2.9467e-04 - val_loss: 3.0994e-04\n",
      "Epoch 28/200\n",
      "12800/12800 [==============================] - 2s 132us/step - loss: 2.8929e-04 - val_loss: 3.0436e-04\n",
      "Epoch 29/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 1s 107us/step - loss: 2.8429e-04 - val_loss: 2.9916e-04\n",
      "Epoch 30/200\n",
      "12800/12800 [==============================] - 1s 106us/step - loss: 2.7959e-04 - val_loss: 2.9451e-04\n",
      "Epoch 31/200\n",
      "12800/12800 [==============================] - 1s 93us/step - loss: 2.7526e-04 - val_loss: 2.9044e-04\n",
      "Epoch 32/200\n",
      "12800/12800 [==============================] - 2s 122us/step - loss: 2.7134e-04 - val_loss: 2.8601e-04\n",
      "Epoch 33/200\n",
      "12800/12800 [==============================] - 1s 105us/step - loss: 2.6762e-04 - val_loss: 2.8232e-04\n",
      "Epoch 34/200\n",
      "12800/12800 [==============================] - 1s 111us/step - loss: 2.6419e-04 - val_loss: 2.7879e-04\n",
      "Epoch 35/200\n",
      "12800/12800 [==============================] - 1s 98us/step - loss: 2.6102e-04 - val_loss: 2.7600e-04\n",
      "Epoch 36/200\n",
      "12800/12800 [==============================] - 1s 107us/step - loss: 2.5808e-04 - val_loss: 2.7259e-04\n",
      "Epoch 37/200\n",
      "12800/12800 [==============================] - 1s 104us/step - loss: 2.5533e-04 - val_loss: 2.7018e-04\n",
      "Epoch 38/200\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 2.5279e-04 - val_loss: 2.6726e-04\n",
      "Epoch 39/200\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 2.5040e-04 - val_loss: 2.6499e-04\n",
      "Epoch 40/200\n",
      "12800/12800 [==============================] - 1s 97us/step - loss: 2.4819e-04 - val_loss: 2.6265e-04\n",
      "Epoch 41/200\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 2.4606e-04 - val_loss: 2.6075e-04\n",
      "Epoch 42/200\n",
      "12800/12800 [==============================] - 1s 88us/step - loss: 2.4415e-04 - val_loss: 2.5862e-04\n",
      "Epoch 43/200\n",
      "12800/12800 [==============================] - 1s 93us/step - loss: 2.4228e-04 - val_loss: 2.5679e-04\n",
      "Epoch 44/200\n",
      "12800/12800 [==============================] - 1s 92us/step - loss: 2.4057e-04 - val_loss: 2.5471e-04\n",
      "Epoch 45/200\n",
      "12800/12800 [==============================] - 1s 94us/step - loss: 2.3894e-04 - val_loss: 2.5344e-04\n",
      "Epoch 46/200\n",
      "12800/12800 [==============================] - 1s 92us/step - loss: 2.3741e-04 - val_loss: 2.5146e-04\n",
      "Epoch 47/200\n",
      "12800/12800 [==============================] - 2s 118us/step - loss: 2.3596e-04 - val_loss: 2.5000e-04\n",
      "Epoch 48/200\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 2.3457e-04 - val_loss: 2.4894e-04\n",
      "Epoch 49/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 2.3328e-04 - val_loss: 2.4758e-04\n",
      "Epoch 50/200\n",
      "12800/12800 [==============================] - 1s 101us/step - loss: 2.3208e-04 - val_loss: 2.4603e-04\n",
      "Epoch 51/200\n",
      "12800/12800 [==============================] - 1s 99us/step - loss: 2.3079e-04 - val_loss: 2.4477e-04\n",
      "Epoch 52/200\n",
      "12800/12800 [==============================] - 1s 101us/step - loss: 2.2978e-04 - val_loss: 2.4378e-04\n",
      "Epoch 53/200\n",
      "12800/12800 [==============================] - 1s 93us/step - loss: 2.2867e-04 - val_loss: 2.4249e-04\n",
      "Epoch 54/200\n",
      "12800/12800 [==============================] - 1s 94us/step - loss: 2.2763e-04 - val_loss: 2.4167e-04\n",
      "Epoch 55/200\n",
      "12800/12800 [==============================] - 1s 114us/step - loss: 2.2666e-04 - val_loss: 2.4036e-04\n",
      "Epoch 56/200\n",
      "12800/12800 [==============================] - 1s 107us/step - loss: 2.2571e-04 - val_loss: 2.3965e-04\n",
      "Epoch 57/200\n",
      "12800/12800 [==============================] - 1s 112us/step - loss: 2.2489e-04 - val_loss: 2.3884e-04\n",
      "Epoch 58/200\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 2.2402e-04 - val_loss: 2.3752e-04\n",
      "Epoch 59/200\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 2.2322e-04 - val_loss: 2.3672e-04\n",
      "Epoch 60/200\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 2.2240e-04 - val_loss: 2.3630e-04\n",
      "Epoch 61/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 2.2170e-04 - val_loss: 2.3532e-04\n",
      "Epoch 62/200\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 2.2095e-04 - val_loss: 2.3451e-04\n",
      "Epoch 63/200\n",
      "12800/12800 [==============================] - 1s 63us/step - loss: 2.2023e-04 - val_loss: 2.3377e-04\n",
      "Epoch 64/200\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 2.1961e-04 - val_loss: 2.3326e-04\n",
      "Epoch 65/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 2.1900e-04 - val_loss: 2.3269e-04\n",
      "Epoch 66/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 2.1842e-04 - val_loss: 2.3198e-04\n",
      "Epoch 67/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 2.1785e-04 - val_loss: 2.3140e-04\n",
      "Epoch 68/200\n",
      "12800/12800 [==============================] - 1s 64us/step - loss: 2.1729e-04 - val_loss: 2.3086e-04\n",
      "Epoch 69/200\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 2.1676e-04 - val_loss: 2.3037e-04\n",
      "Epoch 70/200\n",
      "12800/12800 [==============================] - 1s 62us/step - loss: 2.1624e-04 - val_loss: 2.2963e-04\n",
      "Epoch 71/200\n",
      "12800/12800 [==============================] - 1s 65us/step - loss: 2.1579e-04 - val_loss: 2.2906e-04\n",
      "Epoch 72/200\n",
      "12800/12800 [==============================] - 1s 62us/step - loss: 2.1530e-04 - val_loss: 2.2874e-04\n",
      "Epoch 73/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 2.1484e-04 - val_loss: 2.2828e-04\n",
      "Epoch 74/200\n",
      "12800/12800 [==============================] - 1s 62us/step - loss: 2.1439e-04 - val_loss: 2.2793e-04\n",
      "Epoch 75/200\n",
      "12800/12800 [==============================] - 1s 72us/step - loss: 2.1404e-04 - val_loss: 2.2742e-04\n",
      "Epoch 76/200\n",
      "12800/12800 [==============================] - 1s 65us/step - loss: 2.1362e-04 - val_loss: 2.2704e-04\n",
      "Epoch 77/200\n",
      "12800/12800 [==============================] - 1s 65us/step - loss: 2.1326e-04 - val_loss: 2.2649e-04\n",
      "Epoch 78/200\n",
      "12800/12800 [==============================] - 1s 63us/step - loss: 2.1289e-04 - val_loss: 2.2617e-04\n",
      "Epoch 79/200\n",
      "12800/12800 [==============================] - 1s 64us/step - loss: 2.1252e-04 - val_loss: 2.2572e-04\n",
      "Epoch 80/200\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 2.1217e-04 - val_loss: 2.2580e-04\n",
      "Epoch 81/200\n",
      "12800/12800 [==============================] - 1s 62us/step - loss: 2.1184e-04 - val_loss: 2.2520e-04\n",
      "Epoch 82/200\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 2.1156e-04 - val_loss: 2.2486e-04\n",
      "Epoch 83/200\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 2.1125e-04 - val_loss: 2.2469e-04\n",
      "Epoch 84/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 2.1096e-04 - val_loss: 2.2450e-04\n",
      "Epoch 85/200\n",
      "12800/12800 [==============================] - 1s 64us/step - loss: 2.1069e-04 - val_loss: 2.2402e-04\n",
      "Epoch 86/200\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 2.1041e-04 - val_loss: 2.2390e-04\n",
      "Epoch 87/200\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 2.1015e-04 - val_loss: 2.2354e-04\n",
      "Epoch 88/200\n",
      "12800/12800 [==============================] - 1s 64us/step - loss: 2.0993e-04 - val_loss: 2.2320e-04\n",
      "Epoch 89/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 2.0967e-04 - val_loss: 2.2306e-04\n",
      "Epoch 90/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 2.0943e-04 - val_loss: 2.2278e-04\n",
      "Epoch 91/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 2.0921e-04 - val_loss: 2.2252e-04\n",
      "Epoch 92/200\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 2.0902e-04 - val_loss: 2.2242e-04\n",
      "Epoch 93/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 2.0877e-04 - val_loss: 2.2230e-04\n",
      "Epoch 94/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 2.0857e-04 - val_loss: 2.2213e-04\n",
      "Epoch 95/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 2.0840e-04 - val_loss: 2.2192e-04\n",
      "Epoch 96/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 2.0821e-04 - val_loss: 2.2168e-04\n",
      "Epoch 97/200\n",
      "12800/12800 [==============================] - 1s 68us/step - loss: 2.0805e-04 - val_loss: 2.2127e-04\n",
      "Epoch 98/200\n",
      "12800/12800 [==============================] - 1s 64us/step - loss: 2.0786e-04 - val_loss: 2.2100e-04\n",
      "Epoch 99/200\n",
      "12800/12800 [==============================] - 1s 64us/step - loss: 2.0766e-04 - val_loss: 2.2091e-04\n",
      "Epoch 100/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 1s 66us/step - loss: 2.0750e-04 - val_loss: 2.2112e-04\n",
      "Epoch 101/200\n",
      "12800/12800 [==============================] - 1s 63us/step - loss: 2.0736e-04 - val_loss: 2.2076e-04\n",
      "Epoch 102/200\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 2.0719e-04 - val_loss: 2.2066e-04\n",
      "Epoch 103/200\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 2.0706e-04 - val_loss: 2.2056e-04\n",
      "Epoch 104/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 2.0689e-04 - val_loss: 2.2019e-04\n",
      "Epoch 105/200\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 2.0674e-04 - val_loss: 2.2009e-04\n",
      "Epoch 106/200\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 2.0663e-04 - val_loss: 2.1999e-04\n",
      "Epoch 107/200\n",
      "12800/12800 [==============================] - 1s 62us/step - loss: 2.0649e-04 - val_loss: 2.2008e-04\n",
      "Epoch 108/200\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 2.0637e-04 - val_loss: 2.1974e-04\n",
      "Epoch 109/200\n",
      "12800/12800 [==============================] - 1s 64us/step - loss: 2.0625e-04 - val_loss: 2.1970e-04\n",
      "Epoch 110/200\n",
      "12800/12800 [==============================] - 1s 65us/step - loss: 2.0612e-04 - val_loss: 2.1946e-04\n",
      "Epoch 111/200\n",
      "12800/12800 [==============================] - 1s 61us/step - loss: 2.0600e-04 - val_loss: 2.1940e-04\n",
      "Epoch 112/200\n",
      "12800/12800 [==============================] - 1s 61us/step - loss: 2.0590e-04 - val_loss: 2.1932e-04\n",
      "Epoch 113/200\n",
      "12800/12800 [==============================] - 1s 62us/step - loss: 2.0576e-04 - val_loss: 2.1907e-04\n",
      "Epoch 114/200\n",
      "12800/12800 [==============================] - 1s 61us/step - loss: 2.0569e-04 - val_loss: 2.1886e-04\n",
      "Epoch 115/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 2.0555e-04 - val_loss: 2.1868e-04\n",
      "Epoch 116/200\n",
      "12800/12800 [==============================] - 1s 62us/step - loss: 2.0547e-04 - val_loss: 2.1879e-04\n",
      "Epoch 117/200\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 2.0537e-04 - val_loss: 2.1896e-04\n",
      "Epoch 118/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 2.0527e-04 - val_loss: 2.1869e-04\n",
      "Epoch 119/200\n",
      "12800/12800 [==============================] - 1s 61us/step - loss: 2.0514e-04 - val_loss: 2.1847e-04\n",
      "Epoch 120/200\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 2.0506e-04 - val_loss: 2.1879e-04\n",
      "Epoch 121/200\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 2.0498e-04 - val_loss: 2.1840e-04\n",
      "Epoch 122/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 2.0490e-04 - val_loss: 2.1845e-04\n",
      "Epoch 123/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 2.0479e-04 - val_loss: 2.1836e-04\n",
      "Epoch 124/200\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 2.0472e-04 - val_loss: 2.1811e-04\n",
      "Epoch 125/200\n",
      "12800/12800 [==============================] - 1s 63us/step - loss: 2.0464e-04 - val_loss: 2.1817e-04\n",
      "Epoch 126/200\n",
      "12800/12800 [==============================] - 1s 62us/step - loss: 2.0457e-04 - val_loss: 2.1812e-04\n",
      "Epoch 127/200\n",
      "12800/12800 [==============================] - 1s 61us/step - loss: 2.0449e-04 - val_loss: 2.1795e-04\n",
      "Epoch 128/200\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 2.0441e-04 - val_loss: 2.1790e-04\n",
      "Epoch 129/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 2.0434e-04 - val_loss: 2.1779e-04\n",
      "Epoch 130/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 2.0428e-04 - val_loss: 2.1783e-04\n",
      "Epoch 131/200\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 2.0420e-04 - val_loss: 2.1752e-04\n",
      "Epoch 132/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 2.0412e-04 - val_loss: 2.1759e-04\n",
      "Epoch 133/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 2.0405e-04 - val_loss: 2.1757e-04\n",
      "Epoch 134/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 2.0399e-04 - val_loss: 2.1745e-04\n",
      "Epoch 135/200\n",
      "12800/12800 [==============================] - 1s 61us/step - loss: 2.0394e-04 - val_loss: 2.1742e-04\n",
      "Epoch 136/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 2.0386e-04 - val_loss: 2.1733e-04\n",
      "Epoch 137/200\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 2.0380e-04 - val_loss: 2.1714e-04\n",
      "Epoch 138/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 2.0372e-04 - val_loss: 2.1724e-04\n",
      "Epoch 139/200\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 2.0370e-04 - val_loss: 2.1728e-04\n",
      "Epoch 140/200\n",
      "12800/12800 [==============================] - 1s 64us/step - loss: 2.0364e-04 - val_loss: 2.1712e-04\n",
      "Epoch 141/200\n",
      "12800/12800 [==============================] - 1s 64us/step - loss: 2.0357e-04 - val_loss: 2.1711e-04\n",
      "Epoch 142/200\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 2.0350e-04 - val_loss: 2.1717e-04\n",
      "Epoch 143/200\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 2.0345e-04 - val_loss: 2.1683e-04\n",
      "Epoch 144/200\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 2.0341e-04 - val_loss: 2.1679e-04\n",
      "Epoch 145/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 2.0336e-04 - val_loss: 2.1683e-04\n",
      "Epoch 146/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 2.0328e-04 - val_loss: 2.1689e-04\n",
      "Epoch 147/200\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 2.0324e-04 - val_loss: 2.1708e-04\n",
      "Epoch 148/200\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 2.0322e-04 - val_loss: 2.1673e-04\n",
      "Epoch 149/200\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 2.0316e-04 - val_loss: 2.1670e-04\n",
      "Epoch 150/200\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 2.0312e-04 - val_loss: 2.1669e-04\n",
      "Epoch 151/200\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 2.0304e-04 - val_loss: 2.1675e-04\n",
      "Epoch 152/200\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 2.0301e-04 - val_loss: 2.1658e-04\n",
      "Epoch 153/200\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 2.0297e-04 - val_loss: 2.1637e-04\n",
      "Epoch 154/200\n",
      "12800/12800 [==============================] - 1s 61us/step - loss: 2.0294e-04 - val_loss: 2.1637e-04\n",
      "Epoch 155/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 2.0288e-04 - val_loss: 2.1651e-04\n",
      "Epoch 156/200\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 2.0286e-04 - val_loss: 2.1621e-04\n",
      "Epoch 157/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 2.0281e-04 - val_loss: 2.1638e-04\n",
      "Epoch 158/200\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 2.0275e-04 - val_loss: 2.1641e-04\n",
      "Epoch 159/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 2.0273e-04 - val_loss: 2.1616e-04\n",
      "Epoch 160/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 2.0269e-04 - val_loss: 2.1628e-04\n",
      "Epoch 161/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 2.0263e-04 - val_loss: 2.1618e-04\n",
      "Epoch 162/200\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 2.0260e-04 - val_loss: 2.1617e-04\n",
      "Epoch 163/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 2.0255e-04 - val_loss: 2.1617e-04\n",
      "Epoch 164/200\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 2.0252e-04 - val_loss: 2.1594e-04\n",
      "Epoch 165/200\n",
      "12800/12800 [==============================] - 1s 63us/step - loss: 2.0251e-04 - val_loss: 2.1592e-04\n",
      "Epoch 166/200\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 2.0246e-04 - val_loss: 2.1615e-04\n",
      "Epoch 167/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 2.0245e-04 - val_loss: 2.1599e-04\n",
      "Epoch 168/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 2.0240e-04 - val_loss: 2.1600e-04\n",
      "Epoch 169/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 2.0236e-04 - val_loss: 2.1600e-04\n",
      "Epoch 170/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 1s 58us/step - loss: 2.0233e-04 - val_loss: 2.1579e-04\n",
      "Epoch 171/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 2.0231e-04 - val_loss: 2.1585e-04\n",
      "Epoch 172/200\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 2.0227e-04 - val_loss: 2.1594e-04\n",
      "Epoch 173/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 2.0224e-04 - val_loss: 2.1582e-04\n",
      "Epoch 174/200\n",
      "12800/12800 [==============================] - 1s 61us/step - loss: 2.0220e-04 - val_loss: 2.1593e-04\n",
      "Epoch 175/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 2.0218e-04 - val_loss: 2.1585e-04\n",
      "Epoch 176/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 2.0215e-04 - val_loss: 2.1593e-04\n",
      "Test MSE: 0.00021499942\n",
      "Total MSE: 0.0002066324\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 42)                12642     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 300)               12900     \n",
      "=================================================================\n",
      "Total params: 25,542\n",
      "Trainable params: 25,542\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/200\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 0.0018 - val_loss: 0.0012\n",
      "Epoch 2/200\n",
      "12800/12800 [==============================] - 1s 43us/step - loss: 0.0010 - val_loss: 8.8417e-04\n",
      "Epoch 3/200\n",
      "12800/12800 [==============================] - 1s 43us/step - loss: 7.9852e-04 - val_loss: 7.3891e-04\n",
      "Epoch 4/200\n",
      "12800/12800 [==============================] - 1s 44us/step - loss: 6.8385e-04 - val_loss: 6.4689e-04\n",
      "Epoch 5/200\n",
      "12800/12800 [==============================] - 1s 43us/step - loss: 6.0714e-04 - val_loss: 5.8224e-04\n",
      "Epoch 6/200\n",
      "12800/12800 [==============================] - 1s 43us/step - loss: 5.5119e-04 - val_loss: 5.3418e-04\n",
      "Epoch 7/200\n",
      "12800/12800 [==============================] - 1s 43us/step - loss: 5.0876e-04 - val_loss: 4.9567e-04\n",
      "Epoch 8/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 4.7561e-04 - val_loss: 4.6623e-04\n",
      "Epoch 9/200\n",
      "12800/12800 [==============================] - 1s 44us/step - loss: 4.4954e-04 - val_loss: 4.4365e-04\n",
      "Epoch 10/200\n",
      "12800/12800 [==============================] - 1s 44us/step - loss: 4.2853e-04 - val_loss: 4.2489e-04\n",
      "Epoch 11/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 4.1138e-04 - val_loss: 4.0924e-04\n",
      "Epoch 12/200\n",
      "12800/12800 [==============================] - 1s 44us/step - loss: 3.9721e-04 - val_loss: 3.9612e-04\n",
      "Epoch 13/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 3.8553e-04 - val_loss: 3.8554e-04\n",
      "Epoch 14/200\n",
      "12800/12800 [==============================] - 1s 57us/step - loss: 3.7559e-04 - val_loss: 3.7606e-04\n",
      "Epoch 15/200\n",
      "12800/12800 [==============================] - 1s 53us/step - loss: 3.6717e-04 - val_loss: 3.6915e-04\n",
      "Epoch 16/200\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 3.6009e-04 - val_loss: 3.6197e-04\n",
      "Epoch 17/200\n",
      "12800/12800 [==============================] - 1s 113us/step - loss: 3.5404e-04 - val_loss: 3.5620e-04\n",
      "Epoch 18/200\n",
      "12800/12800 [==============================] - 1s 111us/step - loss: 3.4882e-04 - val_loss: 3.5140e-04\n",
      "Epoch 19/200\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 3.4438e-04 - val_loss: 3.4682e-04\n",
      "Epoch 20/200\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 3.4047e-04 - val_loss: 3.4326e-04\n",
      "Epoch 21/200\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 3.3713e-04 - val_loss: 3.3999e-04\n",
      "Epoch 22/200\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 3.3419e-04 - val_loss: 3.3723e-04\n",
      "Epoch 23/200\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 3.3157e-04 - val_loss: 3.3482e-04\n",
      "Epoch 24/200\n",
      "12800/12800 [==============================] - 1s 68us/step - loss: 3.2932e-04 - val_loss: 3.3248e-04\n",
      "Epoch 25/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 3.2727e-04 - val_loss: 3.3101e-04\n",
      "Epoch 26/200\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 3.2556e-04 - val_loss: 3.2925e-04\n",
      "Epoch 27/200\n",
      "12800/12800 [==============================] - 1s 112us/step - loss: 3.2395e-04 - val_loss: 3.2793e-04\n",
      "Epoch 28/200\n",
      "12800/12800 [==============================] - 1s 110us/step - loss: 3.2260e-04 - val_loss: 3.2619e-04\n",
      "Epoch 29/200\n",
      "12800/12800 [==============================] - 2s 119us/step - loss: 3.2134e-04 - val_loss: 3.2514e-04\n",
      "Epoch 30/200\n",
      "12800/12800 [==============================] - 2s 120us/step - loss: 3.2019e-04 - val_loss: 3.2407e-04\n",
      "Epoch 31/200\n",
      "12800/12800 [==============================] - 2s 142us/step - loss: 3.1920e-04 - val_loss: 3.2331e-04\n",
      "Epoch 32/200\n",
      "12800/12800 [==============================] - 2s 124us/step - loss: 3.1828e-04 - val_loss: 3.2209e-04\n",
      "Epoch 33/200\n",
      "12800/12800 [==============================] - 2s 141us/step - loss: 3.1746e-04 - val_loss: 3.2137e-04\n",
      "Epoch 34/200\n",
      "12800/12800 [==============================] - 2s 152us/step - loss: 3.1667e-04 - val_loss: 3.2069e-04\n",
      "Epoch 35/200\n",
      "12800/12800 [==============================] - 2s 125us/step - loss: 3.1601e-04 - val_loss: 3.1999e-04\n",
      "Epoch 36/200\n",
      "12800/12800 [==============================] - 2s 154us/step - loss: 3.1539e-04 - val_loss: 3.1944e-04\n",
      "Epoch 37/200\n",
      "12800/12800 [==============================] - 2s 128us/step - loss: 3.1481e-04 - val_loss: 3.1887e-04\n",
      "Epoch 38/200\n",
      "12800/12800 [==============================] - 1s 114us/step - loss: 3.1433e-04 - val_loss: 3.1838e-04\n",
      "Epoch 39/200\n",
      "12800/12800 [==============================] - 1s 98us/step - loss: 3.1378e-04 - val_loss: 3.1796e-04\n",
      "Epoch 40/200\n",
      "12800/12800 [==============================] - 2s 121us/step - loss: 3.1334e-04 - val_loss: 3.1783e-04\n",
      "Epoch 41/200\n",
      "12800/12800 [==============================] - 2s 139us/step - loss: 3.1287e-04 - val_loss: 3.1718e-04\n",
      "Epoch 42/200\n",
      "12800/12800 [==============================] - 1s 105us/step - loss: 3.1250e-04 - val_loss: 3.1662e-04\n",
      "Epoch 43/200\n",
      "12800/12800 [==============================] - 1s 115us/step - loss: 3.1218e-04 - val_loss: 3.1645e-04\n",
      "Epoch 44/200\n",
      "12800/12800 [==============================] - 2s 136us/step - loss: 3.1182e-04 - val_loss: 3.1588e-04\n",
      "Epoch 45/200\n",
      "12800/12800 [==============================] - 2s 133us/step - loss: 3.1151e-04 - val_loss: 3.1549e-04\n",
      "Epoch 46/200\n",
      "12800/12800 [==============================] - 2s 150us/step - loss: 3.1119e-04 - val_loss: 3.1535e-04\n",
      "Epoch 47/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 3.1092e-04 - val_loss: 3.1506e-04\n",
      "Epoch 48/200\n",
      "12800/12800 [==============================] - 1s 52us/step - loss: 3.1065e-04 - val_loss: 3.1503e-04\n",
      "Epoch 49/200\n",
      "12800/12800 [==============================] - 1s 69us/step - loss: 3.1041e-04 - val_loss: 3.1466e-04\n",
      "Epoch 50/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 3.1014e-04 - val_loss: 3.1419e-04\n",
      "Epoch 51/200\n",
      "12800/12800 [==============================] - 1s 109us/step - loss: 3.0991e-04 - val_loss: 3.1412e-04\n",
      "Epoch 52/200\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 3.0973e-04 - val_loss: 3.1382e-04\n",
      "Epoch 53/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 3.0956e-04 - val_loss: 3.1358e-04\n",
      "Epoch 54/200\n",
      "12800/12800 [==============================] - 1s 61us/step - loss: 3.0930e-04 - val_loss: 3.1374e-04\n",
      "Epoch 55/200\n",
      "12800/12800 [==============================] - 1s 54us/step - loss: 3.0915e-04 - val_loss: 3.1350e-04\n",
      "Epoch 56/200\n",
      "12800/12800 [==============================] - 1s 52us/step - loss: 3.0897e-04 - val_loss: 3.1336e-04\n",
      "Epoch 57/200\n",
      "12800/12800 [==============================] - 1s 52us/step - loss: 3.0882e-04 - val_loss: 3.1292e-04\n",
      "Epoch 58/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 1s 55us/step - loss: 3.0862e-04 - val_loss: 3.1326e-04\n",
      "Epoch 59/200\n",
      "12800/12800 [==============================] - 1s 61us/step - loss: 3.0852e-04 - val_loss: 3.1272e-04\n",
      "Epoch 60/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 3.0834e-04 - val_loss: 3.1255e-04\n",
      "Epoch 61/200\n",
      "12800/12800 [==============================] - 1s 62us/step - loss: 3.0821e-04 - val_loss: 3.1241e-04\n",
      "Epoch 62/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 3.0806e-04 - val_loss: 3.1251e-04\n",
      "Epoch 63/200\n",
      "12800/12800 [==============================] - 1s 57us/step - loss: 3.0792e-04 - val_loss: 3.1228e-04\n",
      "Epoch 64/200\n",
      "12800/12800 [==============================] - 1s 57us/step - loss: 3.0781e-04 - val_loss: 3.1205e-04\n",
      "Epoch 65/200\n",
      "12800/12800 [==============================] - 1s 54us/step - loss: 3.0771e-04 - val_loss: 3.1202e-04\n",
      "Epoch 66/200\n",
      "12800/12800 [==============================] - 1s 53us/step - loss: 3.0757e-04 - val_loss: 3.1204e-04\n",
      "Epoch 67/200\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 3.0748e-04 - val_loss: 3.1184e-04\n",
      "Epoch 68/200\n",
      "12800/12800 [==============================] - 1s 53us/step - loss: 3.0740e-04 - val_loss: 3.1157e-04\n",
      "Epoch 69/200\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 3.0727e-04 - val_loss: 3.1162e-04\n",
      "Epoch 70/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 3.0718e-04 - val_loss: 3.1159e-04\n",
      "Epoch 71/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 3.0708e-04 - val_loss: 3.1169e-04\n",
      "Epoch 72/200\n",
      "12800/12800 [==============================] - 1s 48us/step - loss: 3.0699e-04 - val_loss: 3.1130e-04\n",
      "Epoch 73/200\n",
      "12800/12800 [==============================] - 1s 44us/step - loss: 3.0687e-04 - val_loss: 3.1131e-04\n",
      "Epoch 74/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 3.0681e-04 - val_loss: 3.1134e-04\n",
      "Epoch 75/200\n",
      "12800/12800 [==============================] - 1s 52us/step - loss: 3.0673e-04 - val_loss: 3.1130e-04\n",
      "Epoch 76/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 3.0666e-04 - val_loss: 3.1104e-04\n",
      "Epoch 77/200\n",
      "12800/12800 [==============================] - 1s 54us/step - loss: 3.0657e-04 - val_loss: 3.1109e-04\n",
      "Epoch 78/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 3.0651e-04 - val_loss: 3.1090e-04\n",
      "Epoch 79/200\n",
      "12800/12800 [==============================] - 1s 53us/step - loss: 3.0643e-04 - val_loss: 3.1084e-04\n",
      "Epoch 80/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 3.0636e-04 - val_loss: 3.1098e-04\n",
      "Epoch 81/200\n",
      "12800/12800 [==============================] - 1s 48us/step - loss: 3.0633e-04 - val_loss: 3.1088e-04\n",
      "Epoch 82/200\n",
      "12800/12800 [==============================] - 1s 43us/step - loss: 3.0621e-04 - val_loss: 3.1073e-04\n",
      "Epoch 83/200\n",
      "12800/12800 [==============================] - 1s 44us/step - loss: 3.0617e-04 - val_loss: 3.1066e-04\n",
      "Epoch 84/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 3.0612e-04 - val_loss: 3.1068e-04\n",
      "Epoch 85/200\n",
      "12800/12800 [==============================] - 1s 56us/step - loss: 3.0607e-04 - val_loss: 3.1045e-04\n",
      "Epoch 86/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 3.0598e-04 - val_loss: 3.1042e-04\n",
      "Epoch 87/200\n",
      "12800/12800 [==============================] - 1s 48us/step - loss: 3.0595e-04 - val_loss: 3.1045e-04\n",
      "Epoch 88/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 3.0588e-04 - val_loss: 3.1033e-04\n",
      "Epoch 89/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 3.0585e-04 - val_loss: 3.1033e-04\n",
      "Epoch 90/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 3.0581e-04 - val_loss: 3.1032e-04\n",
      "Epoch 91/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 3.0572e-04 - val_loss: 3.1030e-04\n",
      "Epoch 92/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 3.0568e-04 - val_loss: 3.1001e-04\n",
      "Epoch 93/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 3.0563e-04 - val_loss: 3.1015e-04\n",
      "Epoch 94/200\n",
      "12800/12800 [==============================] - 1s 54us/step - loss: 3.0557e-04 - val_loss: 3.1012e-04\n",
      "Epoch 95/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 3.0551e-04 - val_loss: 3.1008e-04\n",
      "Epoch 96/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 3.0553e-04 - val_loss: 3.1001e-04\n",
      "Epoch 97/200\n",
      "12800/12800 [==============================] - 1s 52us/step - loss: 3.0546e-04 - val_loss: 3.1004e-04\n",
      "Epoch 98/200\n",
      "12800/12800 [==============================] - 1s 53us/step - loss: 3.0540e-04 - val_loss: 3.0991e-04\n",
      "Epoch 99/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 3.0538e-04 - val_loss: 3.0996e-04\n",
      "Epoch 100/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 3.0534e-04 - val_loss: 3.1006e-04\n",
      "Epoch 101/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 3.0528e-04 - val_loss: 3.0986e-04\n",
      "Epoch 102/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 3.0524e-04 - val_loss: 3.0993e-04\n",
      "Epoch 103/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 3.0521e-04 - val_loss: 3.0979e-04\n",
      "Epoch 104/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 3.0517e-04 - val_loss: 3.0979e-04\n",
      "Epoch 105/200\n",
      "12800/12800 [==============================] - 1s 44us/step - loss: 3.0510e-04 - val_loss: 3.0960e-04\n",
      "Epoch 106/200\n",
      "12800/12800 [==============================] - 1s 43us/step - loss: 3.0512e-04 - val_loss: 3.0965e-04\n",
      "Epoch 107/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 3.0512e-04 - val_loss: 3.0970e-04\n",
      "Epoch 108/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 3.0506e-04 - val_loss: 3.0952e-04\n",
      "Epoch 109/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 3.0501e-04 - val_loss: 3.0954e-04\n",
      "Epoch 110/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 3.0497e-04 - val_loss: 3.0961e-04\n",
      "Epoch 111/200\n",
      "12800/12800 [==============================] - 1s 48us/step - loss: 3.0496e-04 - val_loss: 3.0954e-04\n",
      "Epoch 112/200\n",
      "12800/12800 [==============================] - 1s 48us/step - loss: 3.0490e-04 - val_loss: 3.0952e-04\n",
      "Epoch 113/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 3.0484e-04 - val_loss: 3.0954e-04\n",
      "Epoch 114/200\n",
      "12800/12800 [==============================] - 1s 43us/step - loss: 3.0482e-04 - val_loss: 3.0947e-04\n",
      "Epoch 115/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 3.0481e-04 - val_loss: 3.0934e-04\n",
      "Epoch 116/200\n",
      "12800/12800 [==============================] - 1s 44us/step - loss: 3.0481e-04 - val_loss: 3.0944e-04\n",
      "Epoch 117/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 3.0476e-04 - val_loss: 3.0927e-04\n",
      "Epoch 118/200\n",
      "12800/12800 [==============================] - 1s 44us/step - loss: 3.0472e-04 - val_loss: 3.0929e-04\n",
      "Epoch 119/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 3.0471e-04 - val_loss: 3.0938e-04\n",
      "Epoch 120/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 3.0468e-04 - val_loss: 3.0917e-04\n",
      "Epoch 121/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 3.0465e-04 - val_loss: 3.0932e-04\n",
      "Epoch 122/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 3.0465e-04 - val_loss: 3.0926e-04\n",
      "Epoch 123/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 3.0461e-04 - val_loss: 3.0922e-04\n",
      "Epoch 124/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 3.0460e-04 - val_loss: 3.0919e-04\n",
      "Epoch 125/200\n",
      "12800/12800 [==============================] - 1s 48us/step - loss: 3.0458e-04 - val_loss: 3.0907e-04\n",
      "Epoch 126/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 3.0455e-04 - val_loss: 3.0918e-04\n",
      "Epoch 127/200\n",
      "12800/12800 [==============================] - 1s 44us/step - loss: 3.0452e-04 - val_loss: 3.0912e-04\n",
      "Epoch 128/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 3.0451e-04 - val_loss: 3.0902e-04\n",
      "Epoch 129/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 1s 44us/step - loss: 3.0449e-04 - val_loss: 3.0915e-04\n",
      "Epoch 130/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 3.0444e-04 - val_loss: 3.0918e-04\n",
      "Epoch 131/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 3.0443e-04 - val_loss: 3.0909e-04\n",
      "Epoch 132/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 3.0443e-04 - val_loss: 3.0910e-04\n",
      "Epoch 133/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 3.0441e-04 - val_loss: 3.0897e-04\n",
      "Epoch 134/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 3.0438e-04 - val_loss: 3.0908e-04\n",
      "Epoch 135/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 3.0434e-04 - val_loss: 3.0898e-04\n",
      "Epoch 136/200\n",
      "12800/12800 [==============================] - 1s 44us/step - loss: 3.0433e-04 - val_loss: 3.0900e-04\n",
      "Epoch 137/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 3.0431e-04 - val_loss: 3.0902e-04\n",
      "Epoch 138/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 3.0427e-04 - val_loss: 3.0888e-04\n",
      "Epoch 139/200\n",
      "12800/12800 [==============================] - 1s 43us/step - loss: 3.0429e-04 - val_loss: 3.0875e-04\n",
      "Epoch 140/200\n",
      "12800/12800 [==============================] - 1s 44us/step - loss: 3.0425e-04 - val_loss: 3.0887e-04\n",
      "Epoch 141/200\n",
      "12800/12800 [==============================] - 1s 44us/step - loss: 3.0424e-04 - val_loss: 3.0876e-04\n",
      "Epoch 142/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 3.0422e-04 - val_loss: 3.0887e-04\n",
      "Epoch 143/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 3.0420e-04 - val_loss: 3.0874e-04\n",
      "Epoch 144/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 3.0417e-04 - val_loss: 3.0881e-04\n",
      "Epoch 145/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 3.0418e-04 - val_loss: 3.0878e-04\n",
      "Epoch 146/200\n",
      "12800/12800 [==============================] - 1s 48us/step - loss: 3.0414e-04 - val_loss: 3.0872e-04\n",
      "Epoch 147/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 3.0414e-04 - val_loss: 3.0868e-04\n",
      "Epoch 148/200\n",
      "12800/12800 [==============================] - 1s 53us/step - loss: 3.0412e-04 - val_loss: 3.0874e-04\n",
      "Epoch 149/200\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 3.0409e-04 - val_loss: 3.0877e-04\n",
      "Epoch 150/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 3.0408e-04 - val_loss: 3.0871e-04\n",
      "Epoch 151/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 3.0408e-04 - val_loss: 3.0871e-04\n",
      "Epoch 152/200\n",
      "12800/12800 [==============================] - 1s 43us/step - loss: 3.0405e-04 - val_loss: 3.0874e-04\n",
      "Epoch 153/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 3.0405e-04 - val_loss: 3.0873e-04\n",
      "Test MSE: 0.0003092331\n",
      "Total MSE: 0.00030546259\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input-layer (InputLayer)     (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "hidden-layer-1 (Dense)       (None, 84)                25284     \n",
      "_________________________________________________________________\n",
      "hidden-layer-2 (Dense)       (None, 42)                3570      \n",
      "_________________________________________________________________\n",
      "hidden-layer-3 (Dense)       (None, 84)                3612      \n",
      "_________________________________________________________________\n",
      "output-layer (Dense)         (None, 300)               25500     \n",
      "=================================================================\n",
      "Total params: 57,966\n",
      "Trainable params: 57,966\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/200\n",
      "12800/12800 [==============================] - 1s 97us/step - loss: 0.0025 - val_loss: 0.0016\n",
      "Epoch 2/200\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 0.0014 - val_loss: 0.0013\n",
      "Epoch 3/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 4/200\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 9.8970e-04 - val_loss: 9.6787e-04\n",
      "Epoch 5/200\n",
      "12800/12800 [==============================] - 1s 61us/step - loss: 8.8958e-04 - val_loss: 8.8038e-04\n",
      "Epoch 6/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 8.1522e-04 - val_loss: 8.1448e-04\n",
      "Epoch 7/200\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 7.5723e-04 - val_loss: 7.6122e-04\n",
      "Epoch 8/200\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 7.0998e-04 - val_loss: 7.1627e-04\n",
      "Epoch 9/200\n",
      "12800/12800 [==============================] - 1s 63us/step - loss: 6.6997e-04 - val_loss: 6.7970e-04\n",
      "Epoch 10/200\n",
      "12800/12800 [==============================] - 1s 72us/step - loss: 6.3601e-04 - val_loss: 6.4670e-04\n",
      "Epoch 11/200\n",
      "12800/12800 [==============================] - 1s 63us/step - loss: 6.0680e-04 - val_loss: 6.1851e-04\n",
      "Epoch 12/200\n",
      "12800/12800 [==============================] - 1s 64us/step - loss: 5.8127e-04 - val_loss: 5.9484e-04\n",
      "Epoch 13/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 5.5933e-04 - val_loss: 5.7321e-04\n",
      "Epoch 14/200\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 5.3989e-04 - val_loss: 5.5369e-04\n",
      "Epoch 15/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 5.2270e-04 - val_loss: 5.3776e-04\n",
      "Epoch 16/200\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 5.0741e-04 - val_loss: 5.2224e-04\n",
      "Epoch 17/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 4.9354e-04 - val_loss: 5.0823e-04\n",
      "Epoch 18/200\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 4.8090e-04 - val_loss: 4.9628e-04\n",
      "Epoch 19/200\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 4.6932e-04 - val_loss: 4.8409e-04\n",
      "Epoch 20/200\n",
      "12800/12800 [==============================] - 1s 63us/step - loss: 4.5877e-04 - val_loss: 4.7353e-04\n",
      "Epoch 21/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 4.4890e-04 - val_loss: 4.6351e-04\n",
      "Epoch 22/200\n",
      "12800/12800 [==============================] - 1s 63us/step - loss: 4.3982e-04 - val_loss: 4.5364e-04\n",
      "Epoch 23/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 4.3137e-04 - val_loss: 4.4566e-04\n",
      "Epoch 24/200\n",
      "12800/12800 [==============================] - 1s 57us/step - loss: 4.2359e-04 - val_loss: 4.3735e-04\n",
      "Epoch 25/200\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 4.1630e-04 - val_loss: 4.2988e-04\n",
      "Epoch 26/200\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 4.0958e-04 - val_loss: 4.2323e-04\n",
      "Epoch 27/200\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 4.0341e-04 - val_loss: 4.1658e-04\n",
      "Epoch 28/200\n",
      "12800/12800 [==============================] - 1s 57us/step - loss: 3.9761e-04 - val_loss: 4.1118e-04\n",
      "Epoch 29/200\n",
      "12800/12800 [==============================] - 1s 65us/step - loss: 3.9227e-04 - val_loss: 4.0504e-04\n",
      "Epoch 30/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 3.8727e-04 - val_loss: 4.0029e-04\n",
      "Epoch 31/200\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 3.8264e-04 - val_loss: 3.9526e-04\n",
      "Epoch 32/200\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 3.7841e-04 - val_loss: 3.9126e-04\n",
      "Epoch 33/200\n",
      "12800/12800 [==============================] - 1s 63us/step - loss: 3.7437e-04 - val_loss: 3.8701e-04\n",
      "Epoch 34/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 3.7062e-04 - val_loss: 3.8324e-04\n",
      "Epoch 35/200\n",
      "12800/12800 [==============================] - 1s 65us/step - loss: 3.6718e-04 - val_loss: 3.7942e-04\n",
      "Epoch 36/200\n",
      "12800/12800 [==============================] - 1s 64us/step - loss: 3.6392e-04 - val_loss: 3.7587e-04\n",
      "Epoch 37/200\n",
      "12800/12800 [==============================] - 1s 62us/step - loss: 3.6085e-04 - val_loss: 3.7295e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/200\n",
      "12800/12800 [==============================] - 1s 63us/step - loss: 3.5799e-04 - val_loss: 3.7001e-04\n",
      "Epoch 39/200\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 3.5528e-04 - val_loss: 3.6720e-04\n",
      "Epoch 40/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 3.5280e-04 - val_loss: 3.6473e-04\n",
      "Epoch 41/200\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 3.5045e-04 - val_loss: 3.6229e-04\n",
      "Epoch 42/200\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 3.4821e-04 - val_loss: 3.5987e-04\n",
      "Epoch 43/200\n",
      "12800/12800 [==============================] - 1s 63us/step - loss: 3.4612e-04 - val_loss: 3.5770e-04\n",
      "Epoch 44/200\n",
      "12800/12800 [==============================] - 1s 65us/step - loss: 3.4420e-04 - val_loss: 3.5553e-04\n",
      "Epoch 45/200\n",
      "12800/12800 [==============================] - 1s 68us/step - loss: 3.4237e-04 - val_loss: 3.5389e-04\n",
      "Epoch 46/200\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 3.4064e-04 - val_loss: 3.5187e-04\n",
      "Epoch 47/200\n",
      "12800/12800 [==============================] - 1s 63us/step - loss: 3.3904e-04 - val_loss: 3.5010e-04\n",
      "Epoch 48/200\n",
      "12800/12800 [==============================] - 1s 63us/step - loss: 3.3748e-04 - val_loss: 3.4875e-04\n",
      "Epoch 49/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 3.3613e-04 - val_loss: 3.4688e-04\n",
      "Epoch 50/200\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 3.3477e-04 - val_loss: 3.4578e-04\n",
      "Epoch 51/200\n",
      "12800/12800 [==============================] - 1s 62us/step - loss: 3.3352e-04 - val_loss: 3.4453e-04\n",
      "Epoch 52/200\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 3.3235e-04 - val_loss: 3.4316e-04\n",
      "Epoch 53/200\n",
      "12800/12800 [==============================] - 1s 56us/step - loss: 3.3121e-04 - val_loss: 3.4203e-04\n",
      "Epoch 54/200\n",
      "12800/12800 [==============================] - 1s 57us/step - loss: 3.3017e-04 - val_loss: 3.4092e-04\n",
      "Epoch 55/200\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 3.2914e-04 - val_loss: 3.3967e-04\n",
      "Epoch 56/200\n",
      "12800/12800 [==============================] - 1s 65us/step - loss: 3.2825e-04 - val_loss: 3.3873e-04\n",
      "Epoch 57/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 3.2733e-04 - val_loss: 3.3784e-04\n",
      "Epoch 58/200\n",
      "12800/12800 [==============================] - 1s 62us/step - loss: 3.2652e-04 - val_loss: 3.3684e-04\n",
      "Epoch 59/200\n",
      "12800/12800 [==============================] - 1s 63us/step - loss: 3.2569e-04 - val_loss: 3.3584e-04\n",
      "Epoch 60/200\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 3.2496e-04 - val_loss: 3.3510e-04\n",
      "Epoch 61/200\n",
      "12800/12800 [==============================] - 1s 61us/step - loss: 3.2430e-04 - val_loss: 3.3476e-04\n",
      "Epoch 62/200\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 3.2361e-04 - val_loss: 3.3363e-04\n",
      "Epoch 63/200\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 3.2296e-04 - val_loss: 3.3276e-04\n",
      "Epoch 64/200\n",
      "12800/12800 [==============================] - 1s 62us/step - loss: 3.2234e-04 - val_loss: 3.3232e-04\n",
      "Epoch 65/200\n",
      "12800/12800 [==============================] - 1s 62us/step - loss: 3.2180e-04 - val_loss: 3.3154e-04\n",
      "Epoch 66/200\n",
      "12800/12800 [==============================] - 1s 64us/step - loss: 3.2125e-04 - val_loss: 3.3091e-04\n",
      "Epoch 67/200\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 3.2071e-04 - val_loss: 3.3048e-04\n",
      "Epoch 68/200\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 3.2021e-04 - val_loss: 3.3009e-04\n",
      "Epoch 69/200\n",
      "12800/12800 [==============================] - 1s 64us/step - loss: 3.1981e-04 - val_loss: 3.2944e-04\n",
      "Epoch 70/200\n",
      "12800/12800 [==============================] - 1s 65us/step - loss: 3.1932e-04 - val_loss: 3.2923e-04\n",
      "Epoch 71/200\n",
      "12800/12800 [==============================] - 1s 68us/step - loss: 3.1888e-04 - val_loss: 3.2854e-04\n",
      "Epoch 72/200\n",
      "12800/12800 [==============================] - 1s 62us/step - loss: 3.1846e-04 - val_loss: 3.2799e-04\n",
      "Epoch 73/200\n",
      "12800/12800 [==============================] - 1s 65us/step - loss: 3.1806e-04 - val_loss: 3.2756e-04\n",
      "Epoch 74/200\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 3.1770e-04 - val_loss: 3.2701e-04\n",
      "Epoch 75/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 3.1729e-04 - val_loss: 3.2722e-04\n",
      "Epoch 76/200\n",
      "12800/12800 [==============================] - 1s 62us/step - loss: 3.1700e-04 - val_loss: 3.2647e-04\n",
      "Epoch 77/200\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 3.1663e-04 - val_loss: 3.2645e-04\n",
      "Epoch 78/200\n",
      "12800/12800 [==============================] - 1s 61us/step - loss: 3.1632e-04 - val_loss: 3.2555e-04\n",
      "Epoch 79/200\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 3.1602e-04 - val_loss: 3.2515e-04\n",
      "Epoch 80/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 3.1572e-04 - val_loss: 3.2487e-04\n",
      "Epoch 81/200\n",
      "12800/12800 [==============================] - 1s 65us/step - loss: 3.1545e-04 - val_loss: 3.2471e-04\n",
      "Epoch 82/200\n",
      "12800/12800 [==============================] - 1s 64us/step - loss: 3.1514e-04 - val_loss: 3.2439e-04\n",
      "Epoch 83/200\n",
      "12800/12800 [==============================] - 1s 64us/step - loss: 3.1488e-04 - val_loss: 3.2415e-04\n",
      "Epoch 84/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 3.1465e-04 - val_loss: 3.2395e-04\n",
      "Epoch 85/200\n",
      "12800/12800 [==============================] - 1s 61us/step - loss: 3.1439e-04 - val_loss: 3.2377e-04\n",
      "Epoch 86/200\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 3.1417e-04 - val_loss: 3.2344e-04\n",
      "Epoch 87/200\n",
      "12800/12800 [==============================] - 1s 57us/step - loss: 3.1393e-04 - val_loss: 3.2310e-04\n",
      "Epoch 88/200\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 3.1371e-04 - val_loss: 3.2273e-04\n",
      "Epoch 89/200\n",
      "12800/12800 [==============================] - 1s 62us/step - loss: 3.1344e-04 - val_loss: 3.2264e-04\n",
      "Epoch 90/200\n",
      "12800/12800 [==============================] - 1s 65us/step - loss: 3.1328e-04 - val_loss: 3.2248e-04\n",
      "Epoch 91/200\n",
      "12800/12800 [==============================] - 1s 65us/step - loss: 3.1305e-04 - val_loss: 3.2196e-04\n",
      "Epoch 92/200\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 3.1288e-04 - val_loss: 3.2203e-04\n",
      "Epoch 93/200\n",
      "12800/12800 [==============================] - 1s 63us/step - loss: 3.1266e-04 - val_loss: 3.2183e-04\n",
      "Epoch 94/200\n",
      "12800/12800 [==============================] - 1s 63us/step - loss: 3.1247e-04 - val_loss: 3.2152e-04\n",
      "Epoch 95/200\n",
      "12800/12800 [==============================] - 1s 69us/step - loss: 3.1230e-04 - val_loss: 3.2144e-04\n",
      "Epoch 96/200\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 3.1214e-04 - val_loss: 3.2135e-04\n",
      "Epoch 97/200\n",
      "12800/12800 [==============================] - 1s 65us/step - loss: 3.1194e-04 - val_loss: 3.2094e-04\n",
      "Epoch 98/200\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 3.1177e-04 - val_loss: 3.2081e-04\n",
      "Epoch 99/200\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 3.1157e-04 - val_loss: 3.2059e-04\n",
      "Epoch 100/200\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 3.1140e-04 - val_loss: 3.2068e-04\n",
      "Epoch 101/200\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 3.1128e-04 - val_loss: 3.2036e-04\n",
      "Epoch 102/200\n",
      "12800/12800 [==============================] - 1s 68us/step - loss: 3.1112e-04 - val_loss: 3.2025e-04\n",
      "Epoch 103/200\n",
      "12800/12800 [==============================] - 1s 69us/step - loss: 3.1097e-04 - val_loss: 3.2016e-04\n",
      "Epoch 104/200\n",
      "12800/12800 [==============================] - 1s 61us/step - loss: 3.1084e-04 - val_loss: 3.1992e-04\n",
      "Epoch 105/200\n",
      "12800/12800 [==============================] - 1s 64us/step - loss: 3.1067e-04 - val_loss: 3.2005e-04\n",
      "Epoch 106/200\n",
      "12800/12800 [==============================] - 1s 68us/step - loss: 3.1054e-04 - val_loss: 3.1953e-04\n",
      "Epoch 107/200\n",
      "12800/12800 [==============================] - 1s 68us/step - loss: 3.1041e-04 - val_loss: 3.1953e-04\n",
      "Epoch 108/200\n",
      "12800/12800 [==============================] - 1s 65us/step - loss: 3.1026e-04 - val_loss: 3.1933e-04\n",
      "Epoch 109/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 1s 66us/step - loss: 3.1014e-04 - val_loss: 3.1925e-04\n",
      "Epoch 110/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 3.1002e-04 - val_loss: 3.1908e-04\n",
      "Epoch 111/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 3.0989e-04 - val_loss: 3.1890e-04\n",
      "Epoch 112/200\n",
      "12800/12800 [==============================] - 1s 64us/step - loss: 3.0978e-04 - val_loss: 3.1909e-04\n",
      "Epoch 113/200\n",
      "12800/12800 [==============================] - 1s 69us/step - loss: 3.0965e-04 - val_loss: 3.1877e-04\n",
      "Epoch 114/200\n",
      "12800/12800 [==============================] - 1s 65us/step - loss: 3.0955e-04 - val_loss: 3.1858e-04\n",
      "Epoch 115/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 3.0940e-04 - val_loss: 3.1873e-04\n",
      "Epoch 116/200\n",
      "12800/12800 [==============================] - 1s 62us/step - loss: 3.0937e-04 - val_loss: 3.1859e-04\n",
      "Epoch 117/200\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 3.0920e-04 - val_loss: 3.1840e-04\n",
      "Epoch 118/200\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 3.0910e-04 - val_loss: 3.1834e-04\n",
      "Epoch 119/200\n",
      "12800/12800 [==============================] - 1s 65us/step - loss: 3.0901e-04 - val_loss: 3.1829e-04\n",
      "Epoch 120/200\n",
      "12800/12800 [==============================] - 1s 63us/step - loss: 3.0893e-04 - val_loss: 3.1806e-04\n",
      "Epoch 121/200\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 3.0878e-04 - val_loss: 3.1815e-04\n",
      "Epoch 122/200\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 3.0872e-04 - val_loss: 3.1798e-04\n",
      "Epoch 123/200\n",
      "12800/12800 [==============================] - 1s 103us/step - loss: 3.0860e-04 - val_loss: 3.1817e-04\n",
      "Epoch 124/200\n",
      "12800/12800 [==============================] - 1s 105us/step - loss: 3.0852e-04 - val_loss: 3.1779e-04\n",
      "Epoch 125/200\n",
      "12800/12800 [==============================] - 1s 96us/step - loss: 3.0844e-04 - val_loss: 3.1765e-04\n",
      "Epoch 126/200\n",
      "12800/12800 [==============================] - 1s 93us/step - loss: 3.0834e-04 - val_loss: 3.1753e-04\n",
      "Epoch 127/200\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 3.0824e-04 - val_loss: 3.1752e-04\n",
      "Epoch 128/200\n",
      "12800/12800 [==============================] - 1s 64us/step - loss: 3.0816e-04 - val_loss: 3.1747e-04\n",
      "Epoch 129/200\n",
      "12800/12800 [==============================] - 1s 65us/step - loss: 3.0807e-04 - val_loss: 3.1751e-04\n",
      "Epoch 130/200\n",
      "12800/12800 [==============================] - 1s 64us/step - loss: 3.0802e-04 - val_loss: 3.1740e-04\n",
      "Epoch 131/200\n",
      "12800/12800 [==============================] - 1s 68us/step - loss: 3.0793e-04 - val_loss: 3.1733e-04\n",
      "Epoch 132/200\n",
      "12800/12800 [==============================] - 1s 111us/step - loss: 3.0780e-04 - val_loss: 3.1709e-04\n",
      "Epoch 133/200\n",
      "12800/12800 [==============================] - 1s 64us/step - loss: 3.0776e-04 - val_loss: 3.1709e-04\n",
      "Epoch 134/200\n",
      "12800/12800 [==============================] - 1s 64us/step - loss: 3.0768e-04 - val_loss: 3.1721e-04\n",
      "Epoch 135/200\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 3.0759e-04 - val_loss: 3.1708e-04\n",
      "Epoch 136/200\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 3.0752e-04 - val_loss: 3.1688e-04\n",
      "Epoch 137/200\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 3.0747e-04 - val_loss: 3.1684e-04\n",
      "Epoch 138/200\n",
      "12800/12800 [==============================] - 1s 69us/step - loss: 3.0737e-04 - val_loss: 3.1682e-04\n",
      "Epoch 139/200\n",
      "12800/12800 [==============================] - 1s 72us/step - loss: 3.0730e-04 - val_loss: 3.1674e-04\n",
      "Epoch 140/200\n",
      "12800/12800 [==============================] - 1s 68us/step - loss: 3.0723e-04 - val_loss: 3.1678e-04\n",
      "Epoch 141/200\n",
      "12800/12800 [==============================] - 1s 65us/step - loss: 3.0717e-04 - val_loss: 3.1668e-04\n",
      "Epoch 142/200\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 3.0714e-04 - val_loss: 3.1660e-04\n",
      "Epoch 143/200\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 3.0707e-04 - val_loss: 3.1653e-04\n",
      "Epoch 144/200\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 3.0698e-04 - val_loss: 3.1643e-04\n",
      "Epoch 145/200\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 3.0693e-04 - val_loss: 3.1636e-04\n",
      "Epoch 146/200\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 3.0690e-04 - val_loss: 3.1632e-04\n",
      "Epoch 147/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 3.0681e-04 - val_loss: 3.1641e-04\n",
      "Epoch 148/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 3.0674e-04 - val_loss: 3.1638e-04\n",
      "Epoch 149/200\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 3.0670e-04 - val_loss: 3.1618e-04\n",
      "Epoch 150/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 3.0663e-04 - val_loss: 3.1616e-04\n",
      "Epoch 151/200\n",
      "12800/12800 [==============================] - 1s 65us/step - loss: 3.0660e-04 - val_loss: 3.1607e-04\n",
      "Epoch 152/200\n",
      "12800/12800 [==============================] - 1s 69us/step - loss: 3.0652e-04 - val_loss: 3.1613e-04\n",
      "Epoch 153/200\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 3.0648e-04 - val_loss: 3.1616e-04\n",
      "Epoch 154/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 3.0642e-04 - val_loss: 3.1619e-04\n",
      "Epoch 155/200\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 3.0638e-04 - val_loss: 3.1607e-04\n",
      "Epoch 156/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 3.0630e-04 - val_loss: 3.1609e-04\n",
      "Epoch 157/200\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 3.0628e-04 - val_loss: 3.1586e-04\n",
      "Epoch 158/200\n",
      "12800/12800 [==============================] - 1s 61us/step - loss: 3.0625e-04 - val_loss: 3.1598e-04\n",
      "Epoch 159/200\n",
      "12800/12800 [==============================] - 1s 64us/step - loss: 3.0614e-04 - val_loss: 3.1592e-04\n",
      "Epoch 160/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 3.0613e-04 - val_loss: 3.1575e-04\n",
      "Epoch 161/200\n",
      "12800/12800 [==============================] - 1s 63us/step - loss: 3.0610e-04 - val_loss: 3.1570e-04\n",
      "Epoch 162/200\n",
      "12800/12800 [==============================] - 1s 63us/step - loss: 3.0602e-04 - val_loss: 3.1576e-04\n",
      "Epoch 163/200\n",
      "12800/12800 [==============================] - 1s 62us/step - loss: 3.0604e-04 - val_loss: 3.1564e-04\n",
      "Epoch 164/200\n",
      "12800/12800 [==============================] - 1s 68us/step - loss: 3.0594e-04 - val_loss: 3.1564e-04\n",
      "Epoch 165/200\n",
      "12800/12800 [==============================] - 1s 69us/step - loss: 3.0593e-04 - val_loss: 3.1548e-04\n",
      "Epoch 166/200\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 3.0585e-04 - val_loss: 3.1555e-04\n",
      "Epoch 167/200\n",
      "12800/12800 [==============================] - 1s 68us/step - loss: 3.0583e-04 - val_loss: 3.1573e-04\n",
      "Epoch 168/200\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 3.0580e-04 - val_loss: 3.1573e-04\n",
      "Epoch 169/200\n",
      "12800/12800 [==============================] - 1s 63us/step - loss: 3.0578e-04 - val_loss: 3.1543e-04\n",
      "Epoch 170/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 3.0572e-04 - val_loss: 3.1560e-04\n",
      "Epoch 171/200\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 3.0570e-04 - val_loss: 3.1549e-04\n",
      "Epoch 172/200\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 3.0564e-04 - val_loss: 3.1548e-04\n",
      "Epoch 173/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 3.0559e-04 - val_loss: 3.1559e-04\n",
      "Epoch 174/200\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 3.0557e-04 - val_loss: 3.1548e-04\n",
      "Epoch 175/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 3.0550e-04 - val_loss: 3.1531e-04\n",
      "Epoch 176/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 3.0550e-04 - val_loss: 3.1538e-04\n",
      "Epoch 177/200\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 3.0546e-04 - val_loss: 3.1526e-04\n",
      "Epoch 178/200\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 3.0542e-04 - val_loss: 3.1522e-04\n",
      "Epoch 179/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 1s 72us/step - loss: 3.0541e-04 - val_loss: 3.1525e-04\n",
      "Epoch 180/200\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 3.0536e-04 - val_loss: 3.1527e-04\n",
      "Epoch 181/200\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 3.0532e-04 - val_loss: 3.1521e-04\n",
      "Epoch 182/200\n",
      "12800/12800 [==============================] - 1s 65us/step - loss: 3.0531e-04 - val_loss: 3.1523e-04\n",
      "Epoch 183/200\n",
      "12800/12800 [==============================] - 1s 69us/step - loss: 3.0529e-04 - val_loss: 3.1534e-04\n",
      "Epoch 184/200\n",
      "12800/12800 [==============================] - 1s 68us/step - loss: 3.0523e-04 - val_loss: 3.1521e-04\n",
      "Epoch 185/200\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 3.0518e-04 - val_loss: 3.1519e-04\n",
      "Epoch 186/200\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 3.0519e-04 - val_loss: 3.1520e-04\n",
      "Epoch 187/200\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 3.0513e-04 - val_loss: 3.1524e-04\n",
      "Epoch 188/200\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 3.0512e-04 - val_loss: 3.1527e-04\n",
      "Epoch 189/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 3.0509e-04 - val_loss: 3.1499e-04\n",
      "Epoch 190/200\n",
      "12800/12800 [==============================] - 1s 62us/step - loss: 3.0508e-04 - val_loss: 3.1496e-04\n",
      "Epoch 191/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 3.0503e-04 - val_loss: 3.1505e-04\n",
      "Epoch 192/200\n",
      "12800/12800 [==============================] - 1s 69us/step - loss: 3.0503e-04 - val_loss: 3.1491e-04\n",
      "Epoch 193/200\n",
      "12800/12800 [==============================] - 1s 65us/step - loss: 3.0500e-04 - val_loss: 3.1503e-04\n",
      "Epoch 194/200\n",
      "12800/12800 [==============================] - 1s 69us/step - loss: 3.0497e-04 - val_loss: 3.1505e-04\n",
      "Epoch 195/200\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 3.0496e-04 - val_loss: 3.1500e-04\n",
      "Epoch 196/200\n",
      "12800/12800 [==============================] - 1s 57us/step - loss: 3.0493e-04 - val_loss: 3.1498e-04\n",
      "Epoch 197/200\n",
      "12800/12800 [==============================] - 1s 56us/step - loss: 3.0489e-04 - val_loss: 3.1492e-04\n",
      "Epoch 198/200\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 3.0486e-04 - val_loss: 3.1492e-04\n",
      "Test MSE: 0.00030467274\n",
      "Total MSE: 0.00030601932\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 37)                11137     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 300)               11400     \n",
      "=================================================================\n",
      "Total params: 22,537\n",
      "Trainable params: 22,537\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/200\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 0.0018 - val_loss: 0.0012\n",
      "Epoch 2/200\n",
      "12800/12800 [==============================] - 1s 44us/step - loss: 0.0011 - val_loss: 9.5980e-04\n",
      "Epoch 3/200\n",
      "12800/12800 [==============================] - 1s 48us/step - loss: 8.9640e-04 - val_loss: 8.1458e-04\n",
      "Epoch 4/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 7.7538e-04 - val_loss: 7.1880e-04\n",
      "Epoch 5/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 6.9345e-04 - val_loss: 6.5091e-04\n",
      "Epoch 6/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 6.3455e-04 - val_loss: 6.0052e-04\n",
      "Epoch 7/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 5.9105e-04 - val_loss: 5.6399e-04\n",
      "Epoch 8/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 5.5788e-04 - val_loss: 5.3467e-04\n",
      "Epoch 9/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 5.3200e-04 - val_loss: 5.1149e-04\n",
      "Epoch 10/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 5.1139e-04 - val_loss: 4.9278e-04\n",
      "Epoch 11/200\n",
      "12800/12800 [==============================] - 1s 52us/step - loss: 4.9465e-04 - val_loss: 4.7757e-04\n",
      "Epoch 12/200\n",
      "12800/12800 [==============================] - 1s 48us/step - loss: 4.8094e-04 - val_loss: 4.6534e-04\n",
      "Epoch 13/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 4.6953e-04 - val_loss: 4.5440e-04\n",
      "Epoch 14/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 4.6004e-04 - val_loss: 4.4635e-04\n",
      "Epoch 15/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 4.5194e-04 - val_loss: 4.3849e-04\n",
      "Epoch 16/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 4.4526e-04 - val_loss: 4.3180e-04\n",
      "Epoch 17/200\n",
      "12800/12800 [==============================] - 1s 48us/step - loss: 4.3948e-04 - val_loss: 4.2608e-04\n",
      "Epoch 18/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 4.3450e-04 - val_loss: 4.2201e-04\n",
      "Epoch 19/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 4.3032e-04 - val_loss: 4.1781e-04\n",
      "Epoch 20/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 4.2671e-04 - val_loss: 4.1429e-04\n",
      "Epoch 21/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 4.2351e-04 - val_loss: 4.1156e-04\n",
      "Epoch 22/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 4.2079e-04 - val_loss: 4.0895e-04\n",
      "Epoch 23/200\n",
      "12800/12800 [==============================] - 1s 44us/step - loss: 4.1843e-04 - val_loss: 4.0656e-04\n",
      "Epoch 24/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 4.1630e-04 - val_loss: 4.0437e-04\n",
      "Epoch 25/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 4.1444e-04 - val_loss: 4.0286e-04\n",
      "Epoch 26/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 4.1278e-04 - val_loss: 4.0129e-04\n",
      "Epoch 27/200\n",
      "12800/12800 [==============================] - 1s 48us/step - loss: 4.1123e-04 - val_loss: 3.9976e-04\n",
      "Epoch 28/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 4.0992e-04 - val_loss: 3.9878e-04\n",
      "Epoch 29/200\n",
      "12800/12800 [==============================] - 1s 52us/step - loss: 4.0873e-04 - val_loss: 3.9771e-04\n",
      "Epoch 30/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 4.0759e-04 - val_loss: 3.9673e-04\n",
      "Epoch 31/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 4.0667e-04 - val_loss: 3.9583e-04\n",
      "Epoch 32/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 4.0580e-04 - val_loss: 3.9467e-04\n",
      "Epoch 33/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 4.0498e-04 - val_loss: 3.9395e-04\n",
      "Epoch 34/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 4.0419e-04 - val_loss: 3.9326e-04\n",
      "Epoch 35/200\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 4.0351e-04 - val_loss: 3.9286e-04\n",
      "Epoch 36/200\n",
      "12800/12800 [==============================] - 1s 52us/step - loss: 4.0291e-04 - val_loss: 3.9194e-04\n",
      "Epoch 37/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 4.0227e-04 - val_loss: 3.9183e-04\n",
      "Epoch 38/200\n",
      "12800/12800 [==============================] - 1s 54us/step - loss: 4.0175e-04 - val_loss: 3.9105e-04\n",
      "Epoch 39/200\n",
      "12800/12800 [==============================] - 1s 56us/step - loss: 4.0120e-04 - val_loss: 3.9065e-04\n",
      "Epoch 40/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 4.0075e-04 - val_loss: 3.9038e-04\n",
      "Epoch 41/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 4.0034e-04 - val_loss: 3.8984e-04\n",
      "Epoch 42/200\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 3.9992e-04 - val_loss: 3.8941e-04\n",
      "Epoch 43/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 3.9952e-04 - val_loss: 3.8904e-04\n",
      "Epoch 44/200\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 3.9917e-04 - val_loss: 3.8843e-04\n",
      "Epoch 45/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 1s 66us/step - loss: 3.9881e-04 - val_loss: 3.8827e-04\n",
      "Epoch 46/200\n",
      "12800/12800 [==============================] - 1s 64us/step - loss: 3.9847e-04 - val_loss: 3.8804e-04\n",
      "Epoch 47/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 3.9819e-04 - val_loss: 3.8762e-04\n",
      "Epoch 48/200\n",
      "12800/12800 [==============================] - 1s 63us/step - loss: 3.9786e-04 - val_loss: 3.8757e-04\n",
      "Epoch 49/200\n",
      "12800/12800 [==============================] - 1s 101us/step - loss: 3.9761e-04 - val_loss: 3.8734e-04\n",
      "Epoch 50/200\n",
      "12800/12800 [==============================] - 1s 112us/step - loss: 3.9732e-04 - val_loss: 3.8702e-04\n",
      "Epoch 51/200\n",
      "12800/12800 [==============================] - 1s 99us/step - loss: 3.9710e-04 - val_loss: 3.8685e-04\n",
      "Epoch 52/200\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 3.9681e-04 - val_loss: 3.8681e-04\n",
      "Epoch 53/200\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 3.9661e-04 - val_loss: 3.8652e-04\n",
      "Epoch 54/200\n",
      "12800/12800 [==============================] - 1s 92us/step - loss: 3.9642e-04 - val_loss: 3.8607e-04\n",
      "Epoch 55/200\n",
      "12800/12800 [==============================] - 1s 69us/step - loss: 3.9622e-04 - val_loss: 3.8591e-04\n",
      "Epoch 56/200\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 3.9601e-04 - val_loss: 3.8574e-04\n",
      "Epoch 57/200\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 3.9582e-04 - val_loss: 3.8547e-04\n",
      "Epoch 58/200\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 3.9565e-04 - val_loss: 3.8547e-04\n",
      "Epoch 59/200\n",
      "12800/12800 [==============================] - 2s 118us/step - loss: 3.9548e-04 - val_loss: 3.8513e-04\n",
      "Epoch 60/200\n",
      "12800/12800 [==============================] - 1s 96us/step - loss: 3.9528e-04 - val_loss: 3.8508e-04\n",
      "Epoch 61/200\n",
      "12800/12800 [==============================] - 1s 100us/step - loss: 3.9513e-04 - val_loss: 3.8518e-04\n",
      "Epoch 62/200\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 3.9502e-04 - val_loss: 3.8486e-04\n",
      "Epoch 63/200\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 3.9486e-04 - val_loss: 3.8441e-04\n",
      "Epoch 64/200\n",
      "12800/12800 [==============================] - 1s 72us/step - loss: 3.9470e-04 - val_loss: 3.8437e-04\n",
      "Epoch 65/200\n",
      "12800/12800 [==============================] - 1s 94us/step - loss: 3.9458e-04 - val_loss: 3.8466e-04\n",
      "Epoch 66/200\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 3.9444e-04 - val_loss: 3.8428e-04\n",
      "Epoch 67/200\n",
      "12800/12800 [==============================] - 2s 129us/step - loss: 3.9434e-04 - val_loss: 3.8412e-04\n",
      "Epoch 68/200\n",
      "12800/12800 [==============================] - 1s 65us/step - loss: 3.9424e-04 - val_loss: 3.8399e-04\n",
      "Epoch 69/200\n",
      "12800/12800 [==============================] - 1s 63us/step - loss: 3.9409e-04 - val_loss: 3.8402e-04\n",
      "Epoch 70/200\n",
      "12800/12800 [==============================] - 1s 62us/step - loss: 3.9399e-04 - val_loss: 3.8400e-04\n",
      "Epoch 71/200\n",
      "12800/12800 [==============================] - 1s 62us/step - loss: 3.9388e-04 - val_loss: 3.8379e-04\n",
      "Epoch 72/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 3.9377e-04 - val_loss: 3.8356e-04\n",
      "Epoch 73/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 3.9368e-04 - val_loss: 3.8357e-04\n",
      "Epoch 74/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 3.9358e-04 - val_loss: 3.8332e-04\n",
      "Epoch 75/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 3.9349e-04 - val_loss: 3.8310e-04\n",
      "Epoch 76/200\n",
      "12800/12800 [==============================] - 1s 62us/step - loss: 3.9338e-04 - val_loss: 3.8322e-04\n",
      "Epoch 77/200\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 3.9329e-04 - val_loss: 3.8317e-04\n",
      "Epoch 78/200\n",
      "12800/12800 [==============================] - 1s 61us/step - loss: 3.9319e-04 - val_loss: 3.8323e-04\n",
      "Epoch 79/200\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 3.9315e-04 - val_loss: 3.8304e-04\n",
      "Epoch 80/200\n",
      "12800/12800 [==============================] - 1s 62us/step - loss: 3.9304e-04 - val_loss: 3.8306e-04\n",
      "Epoch 81/200\n",
      "12800/12800 [==============================] - 1s 61us/step - loss: 3.9298e-04 - val_loss: 3.8274e-04\n",
      "Epoch 82/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 3.9288e-04 - val_loss: 3.8284e-04\n",
      "Epoch 83/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 3.9284e-04 - val_loss: 3.8268e-04\n",
      "Epoch 84/200\n",
      "12800/12800 [==============================] - 1s 69us/step - loss: 3.9276e-04 - val_loss: 3.8287e-04\n",
      "Epoch 85/200\n",
      "12800/12800 [==============================] - 1s 68us/step - loss: 3.9268e-04 - val_loss: 3.8263e-04\n",
      "Epoch 86/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 3.9261e-04 - val_loss: 3.8241e-04\n",
      "Epoch 87/200\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 3.9255e-04 - val_loss: 3.8245e-04\n",
      "Epoch 88/200\n",
      "12800/12800 [==============================] - 1s 96us/step - loss: 3.9247e-04 - val_loss: 3.8228e-04\n",
      "Epoch 89/200\n",
      "12800/12800 [==============================] - 1s 62us/step - loss: 3.9245e-04 - val_loss: 3.8230e-04\n",
      "Epoch 90/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 3.9236e-04 - val_loss: 3.8220e-04\n",
      "Epoch 91/200\n",
      "12800/12800 [==============================] - 1s 57us/step - loss: 3.9229e-04 - val_loss: 3.8221e-04\n",
      "Epoch 92/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 3.9224e-04 - val_loss: 3.8208e-04\n",
      "Epoch 93/200\n",
      "12800/12800 [==============================] - 1s 48us/step - loss: 3.9219e-04 - val_loss: 3.8197e-04\n",
      "Epoch 94/200\n",
      "12800/12800 [==============================] - 1s 48us/step - loss: 3.9213e-04 - val_loss: 3.8203e-04\n",
      "Epoch 95/200\n",
      "12800/12800 [==============================] - 1s 57us/step - loss: 3.9207e-04 - val_loss: 3.8193e-04\n",
      "Epoch 96/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 3.9204e-04 - val_loss: 3.8195e-04\n",
      "Epoch 97/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 3.9199e-04 - val_loss: 3.8192e-04\n",
      "Epoch 98/200\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 3.9196e-04 - val_loss: 3.8177e-04\n",
      "Epoch 99/200\n",
      "12800/12800 [==============================] - 1s 53us/step - loss: 3.9188e-04 - val_loss: 3.8193e-04\n",
      "Epoch 100/200\n",
      "12800/12800 [==============================] - 1s 61us/step - loss: 3.9183e-04 - val_loss: 3.8164e-04\n",
      "Epoch 101/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 3.9179e-04 - val_loss: 3.8173e-04\n",
      "Epoch 102/200\n",
      "12800/12800 [==============================] - 1s 52us/step - loss: 3.9173e-04 - val_loss: 3.8180e-04\n",
      "Epoch 103/200\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 3.9169e-04 - val_loss: 3.8160e-04\n",
      "Epoch 104/200\n",
      "12800/12800 [==============================] - 1s 54us/step - loss: 3.9167e-04 - val_loss: 3.8154e-04\n",
      "Epoch 105/200\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 3.9161e-04 - val_loss: 3.8147e-04\n",
      "Epoch 106/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 3.9156e-04 - val_loss: 3.8148e-04\n",
      "Epoch 107/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 3.9151e-04 - val_loss: 3.8130e-04\n",
      "Epoch 108/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 3.9147e-04 - val_loss: 3.8135e-04\n",
      "Epoch 109/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 3.9145e-04 - val_loss: 3.8139e-04\n",
      "Epoch 110/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 3.9139e-04 - val_loss: 3.8119e-04\n",
      "Epoch 111/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 3.9137e-04 - val_loss: 3.8140e-04\n",
      "Epoch 112/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 3.9132e-04 - val_loss: 3.8118e-04\n",
      "Epoch 113/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 3.9128e-04 - val_loss: 3.8122e-04\n",
      "Epoch 114/200\n",
      "12800/12800 [==============================] - 1s 63us/step - loss: 3.9125e-04 - val_loss: 3.8110e-04\n",
      "Epoch 115/200\n",
      "12800/12800 [==============================] - 1s 48us/step - loss: 3.9118e-04 - val_loss: 3.8119e-04\n",
      "Epoch 116/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 1s 48us/step - loss: 3.9119e-04 - val_loss: 3.8106e-04\n",
      "Epoch 117/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 3.9114e-04 - val_loss: 3.8120e-04\n",
      "Epoch 118/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 3.9112e-04 - val_loss: 3.8096e-04\n",
      "Epoch 119/200\n",
      "12800/12800 [==============================] - 1s 64us/step - loss: 3.9107e-04 - val_loss: 3.8101e-04\n",
      "Epoch 120/200\n",
      "12800/12800 [==============================] - 1s 61us/step - loss: 3.9103e-04 - val_loss: 3.8098e-04\n",
      "Epoch 121/200\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 3.9100e-04 - val_loss: 3.8097e-04\n",
      "Epoch 122/200\n",
      "12800/12800 [==============================] - 1s 69us/step - loss: 3.9100e-04 - val_loss: 3.8075e-04\n",
      "Epoch 123/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 3.9094e-04 - val_loss: 3.8090e-04\n",
      "Epoch 124/200\n",
      "12800/12800 [==============================] - 1s 53us/step - loss: 3.9094e-04 - val_loss: 3.8078e-04\n",
      "Epoch 125/200\n",
      "12800/12800 [==============================] - 1s 53us/step - loss: 3.9088e-04 - val_loss: 3.8079e-04\n",
      "Epoch 126/200\n",
      "12800/12800 [==============================] - 1s 54us/step - loss: 3.9085e-04 - val_loss: 3.8086e-04\n",
      "Epoch 127/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 3.9084e-04 - val_loss: 3.8075e-04\n",
      "Epoch 128/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 3.9080e-04 - val_loss: 3.8071e-04\n",
      "Epoch 129/200\n",
      "12800/12800 [==============================] - 1s 52us/step - loss: 3.9078e-04 - val_loss: 3.8052e-04\n",
      "Epoch 130/200\n",
      "12800/12800 [==============================] - 1s 61us/step - loss: 3.9075e-04 - val_loss: 3.8080e-04\n",
      "Epoch 131/200\n",
      "12800/12800 [==============================] - 1s 52us/step - loss: 3.9073e-04 - val_loss: 3.8063e-04\n",
      "Epoch 132/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 3.9069e-04 - val_loss: 3.8064e-04\n",
      "Epoch 133/200\n",
      "12800/12800 [==============================] - 1s 52us/step - loss: 3.9068e-04 - val_loss: 3.8051e-04\n",
      "Epoch 134/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 3.9063e-04 - val_loss: 3.8057e-04\n",
      "Epoch 135/200\n",
      "12800/12800 [==============================] - 1s 57us/step - loss: 3.9061e-04 - val_loss: 3.8061e-04\n",
      "Epoch 136/200\n",
      "12800/12800 [==============================] - 1s 48us/step - loss: 3.9061e-04 - val_loss: 3.8048e-04\n",
      "Epoch 137/200\n",
      "12800/12800 [==============================] - 1s 44us/step - loss: 3.9057e-04 - val_loss: 3.8042e-04\n",
      "Epoch 138/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 3.9054e-04 - val_loss: 3.8050e-04\n",
      "Epoch 139/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 3.9054e-04 - val_loss: 3.8037e-04\n",
      "Epoch 140/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 3.9047e-04 - val_loss: 3.8052e-04\n",
      "Epoch 141/200\n",
      "12800/12800 [==============================] - 1s 48us/step - loss: 3.9044e-04 - val_loss: 3.8029e-04\n",
      "Epoch 142/200\n",
      "12800/12800 [==============================] - 1s 57us/step - loss: 3.9044e-04 - val_loss: 3.8037e-04\n",
      "Epoch 143/200\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 3.9042e-04 - val_loss: 3.8046e-04\n",
      "Epoch 144/200\n",
      "12800/12800 [==============================] - 1s 57us/step - loss: 3.9040e-04 - val_loss: 3.8037e-04\n",
      "Epoch 145/200\n",
      "12800/12800 [==============================] - 1s 65us/step - loss: 3.9034e-04 - val_loss: 3.8021e-04\n",
      "Epoch 146/200\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 3.9036e-04 - val_loss: 3.8039e-04\n",
      "Epoch 147/200\n",
      "12800/12800 [==============================] - 1s 56us/step - loss: 3.9030e-04 - val_loss: 3.8014e-04\n",
      "Epoch 148/200\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 3.9026e-04 - val_loss: 3.8043e-04\n",
      "Epoch 149/200\n",
      "12800/12800 [==============================] - 1s 54us/step - loss: 3.9028e-04 - val_loss: 3.8012e-04\n",
      "Epoch 150/200\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 3.9025e-04 - val_loss: 3.8006e-04\n",
      "Epoch 151/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 3.9023e-04 - val_loss: 3.8006e-04\n",
      "Epoch 152/200\n",
      "12800/12800 [==============================] - 1s 43us/step - loss: 3.9020e-04 - val_loss: 3.8020e-04\n",
      "Epoch 153/200\n",
      "12800/12800 [==============================] - 1s 53us/step - loss: 3.9018e-04 - val_loss: 3.8016e-04\n",
      "Epoch 154/200\n",
      "12800/12800 [==============================] - 1s 62us/step - loss: 3.9017e-04 - val_loss: 3.8010e-04\n",
      "Epoch 155/200\n",
      "12800/12800 [==============================] - 1s 54us/step - loss: 3.9014e-04 - val_loss: 3.8027e-04\n",
      "Epoch 156/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 3.9013e-04 - val_loss: 3.8008e-04\n",
      "Epoch 157/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 3.9010e-04 - val_loss: 3.7999e-04\n",
      "Epoch 158/200\n",
      "12800/12800 [==============================] - 1s 54us/step - loss: 3.9006e-04 - val_loss: 3.8008e-04\n",
      "Epoch 159/200\n",
      "12800/12800 [==============================] - 1s 52us/step - loss: 3.9006e-04 - val_loss: 3.8004e-04\n",
      "Epoch 160/200\n",
      "12800/12800 [==============================] - 1s 54us/step - loss: 3.9007e-04 - val_loss: 3.7982e-04\n",
      "Epoch 161/200\n",
      "12800/12800 [==============================] - 1s 52us/step - loss: 3.9002e-04 - val_loss: 3.8003e-04\n",
      "Epoch 162/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 3.9000e-04 - val_loss: 3.8000e-04\n",
      "Epoch 163/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 3.8999e-04 - val_loss: 3.7990e-04\n",
      "Epoch 164/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 3.8998e-04 - val_loss: 3.7979e-04\n",
      "Epoch 165/200\n",
      "12800/12800 [==============================] - 1s 52us/step - loss: 3.8995e-04 - val_loss: 3.7983e-04\n",
      "Epoch 166/200\n",
      "12800/12800 [==============================] - 1s 43us/step - loss: 3.8993e-04 - val_loss: 3.7978e-04\n",
      "Epoch 167/200\n",
      "12800/12800 [==============================] - 1s 48us/step - loss: 3.8991e-04 - val_loss: 3.7999e-04\n",
      "Epoch 168/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 3.8993e-04 - val_loss: 3.7964e-04\n",
      "Epoch 169/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 3.8986e-04 - val_loss: 3.7978e-04\n",
      "Epoch 170/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 3.8985e-04 - val_loss: 3.7973e-04\n",
      "Epoch 171/200\n",
      "12800/12800 [==============================] - 1s 42us/step - loss: 3.8983e-04 - val_loss: 3.7973e-04\n",
      "Epoch 172/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 3.8983e-04 - val_loss: 3.7987e-04\n",
      "Epoch 173/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 3.8977e-04 - val_loss: 3.7993e-04\n",
      "Epoch 174/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 3.8978e-04 - val_loss: 3.7975e-04\n",
      "Test MSE: 0.000381654\n",
      "Total MSE: 0.00038618364\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input-layer (InputLayer)     (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "hidden-layer-1 (Dense)       (None, 74)                22274     \n",
      "_________________________________________________________________\n",
      "hidden-layer-2 (Dense)       (None, 37)                2775      \n",
      "_________________________________________________________________\n",
      "hidden-layer-3 (Dense)       (None, 74)                2812      \n",
      "_________________________________________________________________\n",
      "output-layer (Dense)         (None, 300)               22500     \n",
      "=================================================================\n",
      "Total params: 50,361\n",
      "Trainable params: 50,361\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/200\n",
      "12800/12800 [==============================] - 2s 145us/step - loss: 0.0023 - val_loss: 0.0015\n",
      "Epoch 2/200\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 3/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 2s 128us/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 4/200\n",
      "12800/12800 [==============================] - 1s 97us/step - loss: 9.8881e-04 - val_loss: 9.3383e-04\n",
      "Epoch 5/200\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 8.9158e-04 - val_loss: 8.4837e-04\n",
      "Epoch 6/200\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 8.1653e-04 - val_loss: 7.8027e-04\n",
      "Epoch 7/200\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 7.5764e-04 - val_loss: 7.2771e-04\n",
      "Epoch 8/200\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 7.1098e-04 - val_loss: 6.8679e-04\n",
      "Epoch 9/200\n",
      "12800/12800 [==============================] - 1s 113us/step - loss: 6.7363e-04 - val_loss: 6.5435e-04\n",
      "Epoch 10/200\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 6.4333e-04 - val_loss: 6.2562e-04\n",
      "Epoch 11/200\n",
      "12800/12800 [==============================] - 1s 72us/step - loss: 6.1830e-04 - val_loss: 6.0429e-04\n",
      "Epoch 12/200\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 5.9734e-04 - val_loss: 5.8461e-04\n",
      "Epoch 13/200\n",
      "12800/12800 [==============================] - 1s 65us/step - loss: 5.7960e-04 - val_loss: 5.6875e-04\n",
      "Epoch 14/200\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 5.6438e-04 - val_loss: 5.5509e-04\n",
      "Epoch 15/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 5.5099e-04 - val_loss: 5.4203e-04\n",
      "Epoch 16/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 5.3934e-04 - val_loss: 5.3116e-04\n",
      "Epoch 17/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 5.2904e-04 - val_loss: 5.2139e-04\n",
      "Epoch 18/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 5.1964e-04 - val_loss: 5.1291e-04\n",
      "Epoch 19/200\n",
      "12800/12800 [==============================] - 1s 62us/step - loss: 5.1137e-04 - val_loss: 5.0515e-04\n",
      "Epoch 20/200\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 5.0396e-04 - val_loss: 4.9802e-04\n",
      "Epoch 21/200\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 4.9719e-04 - val_loss: 4.9137e-04\n",
      "Epoch 22/200\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 4.9098e-04 - val_loss: 4.8544e-04\n",
      "Epoch 23/200\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 4.8539e-04 - val_loss: 4.8065e-04\n",
      "Epoch 24/200\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 4.8014e-04 - val_loss: 4.7557e-04\n",
      "Epoch 25/200\n",
      "12800/12800 [==============================] - 1s 100us/step - loss: 4.7536e-04 - val_loss: 4.7085e-04\n",
      "Epoch 26/200\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 4.7089e-04 - val_loss: 4.6691e-04\n",
      "Epoch 27/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 4.6676e-04 - val_loss: 4.6231e-04\n",
      "Epoch 28/200\n",
      "12800/12800 [==============================] - 1s 68us/step - loss: 4.6278e-04 - val_loss: 4.5908e-04\n",
      "Epoch 29/200\n",
      "12800/12800 [==============================] - 1s 69us/step - loss: 4.5911e-04 - val_loss: 4.5533e-04\n",
      "Epoch 30/200\n",
      "12800/12800 [==============================] - 1s 65us/step - loss: 4.5560e-04 - val_loss: 4.5191e-04\n",
      "Epoch 31/200\n",
      "12800/12800 [==============================] - 1s 69us/step - loss: 4.5222e-04 - val_loss: 4.4838e-04\n",
      "Epoch 32/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 4.4896e-04 - val_loss: 4.4560e-04\n",
      "Epoch 33/200\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 4.4594e-04 - val_loss: 4.4250e-04\n",
      "Epoch 34/200\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 4.4294e-04 - val_loss: 4.3958e-04\n",
      "Epoch 35/200\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 4.4007e-04 - val_loss: 4.3641e-04\n",
      "Epoch 36/200\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 4.3739e-04 - val_loss: 4.3394e-04\n",
      "Epoch 37/200\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 4.3480e-04 - val_loss: 4.3162e-04\n",
      "Epoch 38/200\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 4.3229e-04 - val_loss: 4.2929e-04- ETA: 0s - loss: 4.28\n",
      "Epoch 39/200\n",
      "12800/12800 [==============================] - 1s 72us/step - loss: 4.2989e-04 - val_loss: 4.2714e-04\n",
      "Epoch 40/200\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 4.2756e-04 - val_loss: 4.2442e-04\n",
      "Epoch 41/200\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 4.2549e-04 - val_loss: 4.2228e-04\n",
      "Epoch 42/200\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 4.2334e-04 - val_loss: 4.2084e-04\n",
      "Epoch 43/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 4.2142e-04 - val_loss: 4.1848e-04\n",
      "Epoch 44/200\n",
      "12800/12800 [==============================] - 2s 122us/step - loss: 4.1967e-04 - val_loss: 4.1723e-04\n",
      "Epoch 45/200\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 4.1792e-04 - val_loss: 4.1573e-04\n",
      "Epoch 46/200\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 4.1638e-04 - val_loss: 4.1387e-04\n",
      "Epoch 47/200\n",
      "12800/12800 [==============================] - 1s 88us/step - loss: 4.1483e-04 - val_loss: 4.1269e-04\n",
      "Epoch 48/200\n",
      "12800/12800 [==============================] - 1s 98us/step - loss: 4.1339e-04 - val_loss: 4.1118e-04\n",
      "Epoch 49/200\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 4.1214e-04 - val_loss: 4.0986e-04\n",
      "Epoch 50/200\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 4.1085e-04 - val_loss: 4.0827e-04\n",
      "Epoch 51/200\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 4.0962e-04 - val_loss: 4.0764e-04\n",
      "Epoch 52/200\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 4.0860e-04 - val_loss: 4.0642e-04\n",
      "Epoch 53/200\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 4.0763e-04 - val_loss: 4.0550e-04\n",
      "Epoch 54/200\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 4.0661e-04 - val_loss: 4.0449e-04\n",
      "Epoch 55/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 4.0573e-04 - val_loss: 4.0359e-04\n",
      "Epoch 56/200\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 4.0491e-04 - val_loss: 4.0293e-04\n",
      "Epoch 57/200\n",
      "12800/12800 [==============================] - 1s 88us/step - loss: 4.0413e-04 - val_loss: 4.0223e-04\n",
      "Epoch 58/200\n",
      "12800/12800 [==============================] - 1s 110us/step - loss: 4.0339e-04 - val_loss: 4.0176e-04\n",
      "Epoch 59/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 4.0267e-04 - val_loss: 4.0100e-04\n",
      "Epoch 60/200\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 4.0200e-04 - val_loss: 4.0026e-04\n",
      "Epoch 61/200\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 4.0142e-04 - val_loss: 4.0013e-04\n",
      "Epoch 62/200\n",
      "12800/12800 [==============================] - 1s 56us/step - loss: 4.0088e-04 - val_loss: 3.9921e-04\n",
      "Epoch 63/200\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 4.0025e-04 - val_loss: 3.9901e-04\n",
      "Epoch 64/200\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 3.9978e-04 - val_loss: 3.9860e-04\n",
      "Epoch 65/200\n",
      "12800/12800 [==============================] - 1s 54us/step - loss: 3.9922e-04 - val_loss: 3.9792e-04\n",
      "Epoch 66/200\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 3.9874e-04 - val_loss: 3.9746e-04\n",
      "Epoch 67/200\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 3.9832e-04 - val_loss: 3.9683e-04\n",
      "Epoch 68/200\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 3.9793e-04 - val_loss: 3.9651e-04\n",
      "Epoch 69/200\n",
      "12800/12800 [==============================] - 1s 56us/step - loss: 3.9757e-04 - val_loss: 3.9609e-04\n",
      "Epoch 70/200\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 3.9710e-04 - val_loss: 3.9590e-04\n",
      "Epoch 71/200\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 3.9678e-04 - val_loss: 3.9572e-04\n",
      "Epoch 72/200\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 3.9638e-04 - val_loss: 3.9506e-04\n",
      "Epoch 73/200\n",
      "12800/12800 [==============================] - 1s 57us/step - loss: 3.9605e-04 - val_loss: 3.9477e-04\n",
      "Epoch 74/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 1s 55us/step - loss: 3.9572e-04 - val_loss: 3.9458e-04\n",
      "Epoch 75/200\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 3.9541e-04 - val_loss: 3.9424e-04\n",
      "Epoch 76/200\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 3.9516e-04 - val_loss: 3.9383e-04\n",
      "Epoch 77/200\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 3.9485e-04 - val_loss: 3.9369e-04\n",
      "Epoch 78/200\n",
      "12800/12800 [==============================] - 1s 54us/step - loss: 3.9460e-04 - val_loss: 3.9362e-04\n",
      "Epoch 79/200\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 3.9429e-04 - val_loss: 3.9342e-04\n",
      "Epoch 80/200\n",
      "12800/12800 [==============================] - 1s 54us/step - loss: 3.9407e-04 - val_loss: 3.9300e-04\n",
      "Epoch 81/200\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 3.9383e-04 - val_loss: 3.9272e-04\n",
      "Epoch 82/200\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 3.9353e-04 - val_loss: 3.9276e-04\n",
      "Epoch 83/200\n",
      "12800/12800 [==============================] - 1s 56us/step - loss: 3.9339e-04 - val_loss: 3.9241e-04\n",
      "Epoch 84/200\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 3.9310e-04 - val_loss: 3.9229e-04\n",
      "Epoch 85/200\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 3.9290e-04 - val_loss: 3.9220e-04\n",
      "Epoch 86/200\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 3.9273e-04 - val_loss: 3.9185e-04\n",
      "Epoch 87/200\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 3.9250e-04 - val_loss: 3.9157e-04\n",
      "Epoch 88/200\n",
      "12800/12800 [==============================] - 1s 68us/step - loss: 3.9232e-04 - val_loss: 3.9123e-04\n",
      "Epoch 89/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 3.9210e-04 - val_loss: 3.9126e-04\n",
      "Epoch 90/200\n",
      "12800/12800 [==============================] - 1s 63us/step - loss: 3.9192e-04 - val_loss: 3.9113e-04\n",
      "Epoch 91/200\n",
      "12800/12800 [==============================] - 1s 64us/step - loss: 3.9178e-04 - val_loss: 3.9103e-04\n",
      "Epoch 92/200\n",
      "12800/12800 [==============================] - 1s 65us/step - loss: 3.9155e-04 - val_loss: 3.9080e-04\n",
      "Epoch 93/200\n",
      "12800/12800 [==============================] - 1s 57us/step - loss: 3.9140e-04 - val_loss: 3.9071e-04\n",
      "Epoch 94/200\n",
      "12800/12800 [==============================] - 1s 56us/step - loss: 3.9120e-04 - val_loss: 3.9068e-04\n",
      "Epoch 95/200\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 3.9106e-04 - val_loss: 3.9057e-04\n",
      "Epoch 96/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 3.9093e-04 - val_loss: 3.9037e-04\n",
      "Epoch 97/200\n",
      "12800/12800 [==============================] - 1s 65us/step - loss: 3.9076e-04 - val_loss: 3.8998e-04\n",
      "Epoch 98/200\n",
      "12800/12800 [==============================] - 1s 61us/step - loss: 3.9062e-04 - val_loss: 3.8996e-04\n",
      "Epoch 99/200\n",
      "12800/12800 [==============================] - 1s 63us/step - loss: 3.9048e-04 - val_loss: 3.8983e-04\n",
      "Epoch 100/200\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 3.9034e-04 - val_loss: 3.8977e-04\n",
      "Epoch 101/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 3.9020e-04 - val_loss: 3.8943e-04\n",
      "Epoch 102/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 3.9008e-04 - val_loss: 3.8933e-04\n",
      "Epoch 103/200\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 3.8992e-04 - val_loss: 3.8965e-04\n",
      "Epoch 104/200\n",
      "12800/12800 [==============================] - 1s 56us/step - loss: 3.8983e-04 - val_loss: 3.8920e-04\n",
      "Epoch 105/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 3.8968e-04 - val_loss: 3.8908e-04\n",
      "Epoch 106/200\n",
      "12800/12800 [==============================] - 1s 62us/step - loss: 3.8956e-04 - val_loss: 3.8892e-04\n",
      "Epoch 107/200\n",
      "12800/12800 [==============================] - 1s 56us/step - loss: 3.8944e-04 - val_loss: 3.8907e-04\n",
      "Epoch 108/200\n",
      "12800/12800 [==============================] - 1s 62us/step - loss: 3.8935e-04 - val_loss: 3.8895e-04\n",
      "Epoch 109/200\n",
      "12800/12800 [==============================] - 1s 65us/step - loss: 3.8927e-04 - val_loss: 3.8859e-04\n",
      "Epoch 110/200\n",
      "12800/12800 [==============================] - 1s 62us/step - loss: 3.8910e-04 - val_loss: 3.8851e-04\n",
      "Epoch 111/200\n",
      "12800/12800 [==============================] - 1s 65us/step - loss: 3.8903e-04 - val_loss: 3.8875e-04\n",
      "Epoch 112/200\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 3.8891e-04 - val_loss: 3.8839e-04\n",
      "Epoch 113/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 3.8879e-04 - val_loss: 3.8853e-04\n",
      "Epoch 114/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 3.8875e-04 - val_loss: 3.8824e-04\n",
      "Epoch 115/200\n",
      "12800/12800 [==============================] - 1s 61us/step - loss: 3.8863e-04 - val_loss: 3.8824e-04\n",
      "Epoch 116/200\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 3.8855e-04 - val_loss: 3.8826e-04\n",
      "Epoch 117/200\n",
      "12800/12800 [==============================] - 1s 57us/step - loss: 3.8848e-04 - val_loss: 3.8793e-04\n",
      "Epoch 118/200\n",
      "12800/12800 [==============================] - 1s 57us/step - loss: 3.8839e-04 - val_loss: 3.8787e-04\n",
      "Epoch 119/200\n",
      "12800/12800 [==============================] - 1s 57us/step - loss: 3.8825e-04 - val_loss: 3.8787e-04\n",
      "Epoch 120/200\n",
      "12800/12800 [==============================] - 1s 56us/step - loss: 3.8820e-04 - val_loss: 3.8779e-04\n",
      "Epoch 121/200\n",
      "12800/12800 [==============================] - 1s 61us/step - loss: 3.8814e-04 - val_loss: 3.8780e-04\n",
      "Epoch 122/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 3.8803e-04 - val_loss: 3.8775e-04\n",
      "Epoch 123/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 3.8793e-04 - val_loss: 3.8780e-04\n",
      "Epoch 124/200\n",
      "12800/12800 [==============================] - 1s 62us/step - loss: 3.8788e-04 - val_loss: 3.8753e-04\n",
      "Epoch 125/200\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 3.8778e-04 - val_loss: 3.8755e-04\n",
      "Epoch 126/200\n",
      "12800/12800 [==============================] - 1s 56us/step - loss: 3.8770e-04 - val_loss: 3.8735e-04\n",
      "Epoch 127/200\n",
      "12800/12800 [==============================] - 1s 64us/step - loss: 3.8765e-04 - val_loss: 3.8721e-04\n",
      "Epoch 128/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 3.8758e-04 - val_loss: 3.8724e-04\n",
      "Epoch 129/200\n",
      "12800/12800 [==============================] - 1s 68us/step - loss: 3.8752e-04 - val_loss: 3.8716e-04\n",
      "Epoch 130/200\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 3.8746e-04 - val_loss: 3.8725e-04\n",
      "Epoch 131/200\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 3.8738e-04 - val_loss: 3.8717e-04\n",
      "Epoch 132/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 3.8731e-04 - val_loss: 3.8711e-04\n",
      "Epoch 133/200\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 3.8720e-04 - val_loss: 3.8717e-04\n",
      "Epoch 134/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 3.8717e-04 - val_loss: 3.8705e-04\n",
      "Epoch 135/200\n",
      "12800/12800 [==============================] - 1s 61us/step - loss: 3.8712e-04 - val_loss: 3.8721e-04\n",
      "Epoch 136/200\n",
      "12800/12800 [==============================] - 1s 57us/step - loss: 3.8705e-04 - val_loss: 3.8688e-04\n",
      "Epoch 137/200\n",
      "12800/12800 [==============================] - 1s 64us/step - loss: 3.8698e-04 - val_loss: 3.8680e-04\n",
      "Epoch 138/200\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 3.8692e-04 - val_loss: 3.8665e-04\n",
      "Epoch 139/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 3.8686e-04 - val_loss: 3.8653e-04\n",
      "Epoch 140/200\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 3.8680e-04 - val_loss: 3.8660e-04\n",
      "Epoch 141/200\n",
      "12800/12800 [==============================] - 1s 72us/step - loss: 3.8676e-04 - val_loss: 3.8673e-04\n",
      "Epoch 142/200\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 3.8666e-04 - val_loss: 3.8639e-04\n",
      "Epoch 143/200\n",
      "12800/12800 [==============================] - 1s 64us/step - loss: 3.8666e-04 - val_loss: 3.8648e-04\n",
      "Epoch 144/200\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 3.8661e-04 - val_loss: 3.8647e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145/200\n",
      "12800/12800 [==============================] - 1s 68us/step - loss: 3.8655e-04 - val_loss: 3.8647e-04\n",
      "Epoch 146/200\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 3.8648e-04 - val_loss: 3.8636e-04\n",
      "Epoch 147/200\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 3.8648e-04 - val_loss: 3.8623e-04\n",
      "Epoch 148/200\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 3.8640e-04 - val_loss: 3.8612e-04\n",
      "Epoch 149/200\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 3.8633e-04 - val_loss: 3.8629e-04\n",
      "Epoch 150/200\n",
      "12800/12800 [==============================] - 1s 62us/step - loss: 3.8629e-04 - val_loss: 3.8627e-04\n",
      "Epoch 151/200\n",
      "12800/12800 [==============================] - 1s 57us/step - loss: 3.8627e-04 - val_loss: 3.8619e-04\n",
      "Epoch 152/200\n",
      "12800/12800 [==============================] - 1s 65us/step - loss: 3.8622e-04 - val_loss: 3.8615e-04\n",
      "Epoch 153/200\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 3.8616e-04 - val_loss: 3.8616e-04\n",
      "Epoch 154/200\n",
      "12800/12800 [==============================] - 1s 64us/step - loss: 3.8614e-04 - val_loss: 3.8601e-04\n",
      "Epoch 155/200\n",
      "12800/12800 [==============================] - 1s 94us/step - loss: 3.8611e-04 - val_loss: 3.8608e-04\n",
      "Epoch 156/200\n",
      "12800/12800 [==============================] - 2s 126us/step - loss: 3.8603e-04 - val_loss: 3.8585e-04\n",
      "Epoch 157/200\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 3.8599e-04 - val_loss: 3.8611e-04\n",
      "Epoch 158/200\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 3.8596e-04 - val_loss: 3.8601e-04\n",
      "Epoch 159/200\n",
      "12800/12800 [==============================] - 1s 99us/step - loss: 3.8594e-04 - val_loss: 3.8576e-04\n",
      "Epoch 160/200\n",
      "12800/12800 [==============================] - 1s 100us/step - loss: 3.8584e-04 - val_loss: 3.8602e-04\n",
      "Epoch 161/200\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 3.8584e-04 - val_loss: 3.8607e-04\n",
      "Epoch 162/200\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 3.8584e-04 - val_loss: 3.8564e-04\n",
      "Epoch 163/200\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 3.8575e-04 - val_loss: 3.8578e-04\n",
      "Epoch 164/200\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 3.8572e-04 - val_loss: 3.8570e-04\n",
      "Epoch 165/200\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 3.8567e-04 - val_loss: 3.8562e-04\n",
      "Epoch 166/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 3.8566e-04 - val_loss: 3.8581e-04\n",
      "Epoch 167/200\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 3.8564e-04 - val_loss: 3.8587e-04\n",
      "Epoch 168/200\n",
      "12800/12800 [==============================] - 1s 99us/step - loss: 3.8559e-04 - val_loss: 3.8562e-04\n",
      "Epoch 169/200\n",
      "12800/12800 [==============================] - 1s 94us/step - loss: 3.8554e-04 - val_loss: 3.8563e-04\n",
      "Epoch 170/200\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 3.8555e-04 - val_loss: 3.8574e-04\n",
      "Epoch 171/200\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 3.8546e-04 - val_loss: 3.8566e-04\n",
      "Test MSE: 0.00039058302\n",
      "Total MSE: 0.00038610698\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 33)                9933      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 300)               10200     \n",
      "=================================================================\n",
      "Total params: 20,133\n",
      "Trainable params: 20,133\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/200\n",
      "12800/12800 [==============================] - 1s 106us/step - loss: 0.0018 - val_loss: 0.0013\n",
      "Epoch 2/200\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 3/200\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 9.4615e-04 - val_loss: 9.0042e-04\n",
      "Epoch 4/200\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 8.3055e-04 - val_loss: 8.0670e-04\n",
      "Epoch 5/200\n",
      "12800/12800 [==============================] - 1s 64us/step - loss: 7.5298e-04 - val_loss: 7.4223e-04\n",
      "Epoch 6/200\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 6.9730e-04 - val_loss: 6.9362e-04\n",
      "Epoch 7/200\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 6.5591e-04 - val_loss: 6.5733e-04\n",
      "Epoch 8/200\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 6.2427e-04 - val_loss: 6.2900e-04\n",
      "Epoch 9/200\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 5.9958e-04 - val_loss: 6.0736e-04\n",
      "Epoch 10/200\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 5.7976e-04 - val_loss: 5.8929e-04\n",
      "Epoch 11/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 5.6348e-04 - val_loss: 5.7518e-04\n",
      "Epoch 12/200\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 5.5017e-04 - val_loss: 5.6235e-04\n",
      "Epoch 13/200\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 5.3880e-04 - val_loss: 5.5190e-04\n",
      "Epoch 14/200\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 5.2924e-04 - val_loss: 5.4317e-04\n",
      "Epoch 15/200\n",
      "12800/12800 [==============================] - 1s 106us/step - loss: 5.2107e-04 - val_loss: 5.3558e-04\n",
      "Epoch 16/200\n",
      "12800/12800 [==============================] - 1s 100us/step - loss: 5.1410e-04 - val_loss: 5.2921e-04\n",
      "Epoch 17/200\n",
      "12800/12800 [==============================] - 1s 111us/step - loss: 5.0801e-04 - val_loss: 5.2347e-04\n",
      "Epoch 18/200\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 5.0279e-04 - val_loss: 5.1846e-04\n",
      "Epoch 19/200\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 4.9834e-04 - val_loss: 5.1413e-04\n",
      "Epoch 20/200\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 4.9443e-04 - val_loss: 5.1068e-04\n",
      "Epoch 21/200\n",
      "12800/12800 [==============================] - 1s 69us/step - loss: 4.9103e-04 - val_loss: 5.0727e-04\n",
      "Epoch 22/200\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 4.8808e-04 - val_loss: 5.0449e-04\n",
      "Epoch 23/200\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 4.8549e-04 - val_loss: 5.0223e-04\n",
      "Epoch 24/200\n",
      "12800/12800 [==============================] - 1s 64us/step - loss: 4.8325e-04 - val_loss: 4.9980e-04\n",
      "Epoch 25/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 4.8127e-04 - val_loss: 4.9810e-04\n",
      "Epoch 26/200\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 4.7956e-04 - val_loss: 4.9644e-04\n",
      "Epoch 27/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 4.7802e-04 - val_loss: 4.9507e-04\n",
      "Epoch 28/200\n",
      "12800/12800 [==============================] - 1s 54us/step - loss: 4.7658e-04 - val_loss: 4.9357e-04\n",
      "Epoch 29/200\n",
      "12800/12800 [==============================] - 1s 54us/step - loss: 4.7542e-04 - val_loss: 4.9223e-04\n",
      "Epoch 30/200\n",
      "12800/12800 [==============================] - 1s 53us/step - loss: 4.7431e-04 - val_loss: 4.9152e-04\n",
      "Epoch 31/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 4.7337e-04 - val_loss: 4.9058e-04\n",
      "Epoch 32/200\n",
      "12800/12800 [==============================] - 1s 53us/step - loss: 4.7252e-04 - val_loss: 4.8969e-04\n",
      "Epoch 33/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 4.7173e-04 - val_loss: 4.8839e-04\n",
      "Epoch 34/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 4.7101e-04 - val_loss: 4.8799e-04\n",
      "Epoch 35/200\n",
      "12800/12800 [==============================] - 1s 53us/step - loss: 4.7042e-04 - val_loss: 4.8731e-04\n",
      "Epoch 36/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 4.6980e-04 - val_loss: 4.8703e-04\n",
      "Epoch 37/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 4.6922e-04 - val_loss: 4.8613e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/200\n",
      "12800/12800 [==============================] - 1s 52us/step - loss: 4.6870e-04 - val_loss: 4.8582e-04\n",
      "Epoch 39/200\n",
      "12800/12800 [==============================] - 1s 52us/step - loss: 4.6829e-04 - val_loss: 4.8537e-04\n",
      "Epoch 40/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 4.6781e-04 - val_loss: 4.8518e-04\n",
      "Epoch 41/200\n",
      "12800/12800 [==============================] - 1s 48us/step - loss: 4.6744e-04 - val_loss: 4.8452e-04\n",
      "Epoch 42/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 4.6710e-04 - val_loss: 4.8420e-04\n",
      "Epoch 43/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 4.6674e-04 - val_loss: 4.8374e-04\n",
      "Epoch 44/200\n",
      "12800/12800 [==============================] - 1s 61us/step - loss: 4.6641e-04 - val_loss: 4.8363e-04\n",
      "Epoch 45/200\n",
      "12800/12800 [==============================] - 1s 63us/step - loss: 4.6617e-04 - val_loss: 4.8329e-04\n",
      "Epoch 46/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 4.6582e-04 - val_loss: 4.8291e-04\n",
      "Epoch 47/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 4.6560e-04 - val_loss: 4.8257e-04\n",
      "Epoch 48/200\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 4.6532e-04 - val_loss: 4.8250e-04\n",
      "Epoch 49/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 4.6508e-04 - val_loss: 4.8211e-04\n",
      "Epoch 50/200\n",
      "12800/12800 [==============================] - 1s 53us/step - loss: 4.6489e-04 - val_loss: 4.8212e-04\n",
      "Epoch 51/200\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 4.6467e-04 - val_loss: 4.8173e-04\n",
      "Epoch 52/200\n",
      "12800/12800 [==============================] - 1s 54us/step - loss: 4.6447e-04 - val_loss: 4.8152e-04\n",
      "Epoch 53/200\n",
      "12800/12800 [==============================] - 1s 63us/step - loss: 4.6423e-04 - val_loss: 4.8138e-04\n",
      "Epoch 54/200\n",
      "12800/12800 [==============================] - 1s 65us/step - loss: 4.6407e-04 - val_loss: 4.8126e-04\n",
      "Epoch 55/200\n",
      "12800/12800 [==============================] - 1s 57us/step - loss: 4.6390e-04 - val_loss: 4.8079e-04\n",
      "Epoch 56/200\n",
      "12800/12800 [==============================] - 1s 62us/step - loss: 4.6370e-04 - val_loss: 4.8093e-04\n",
      "Epoch 57/200\n",
      "12800/12800 [==============================] - 1s 56us/step - loss: 4.6356e-04 - val_loss: 4.8074e-04\n",
      "Epoch 58/200\n",
      "12800/12800 [==============================] - 1s 56us/step - loss: 4.6341e-04 - val_loss: 4.8059e-04\n",
      "Epoch 59/200\n",
      "12800/12800 [==============================] - 1s 68us/step - loss: 4.6327e-04 - val_loss: 4.8044e-04\n",
      "Epoch 60/200\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 4.6314e-04 - val_loss: 4.8004e-04\n",
      "Epoch 61/200\n",
      "12800/12800 [==============================] - 1s 61us/step - loss: 4.6299e-04 - val_loss: 4.8025e-04\n",
      "Epoch 62/200\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 4.6287e-04 - val_loss: 4.8016e-04\n",
      "Epoch 63/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 4.6271e-04 - val_loss: 4.7982e-04\n",
      "Epoch 64/200\n",
      "12800/12800 [==============================] - 1s 48us/step - loss: 4.6261e-04 - val_loss: 4.7962e-04\n",
      "Epoch 65/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 4.6246e-04 - val_loss: 4.7950e-04\n",
      "Epoch 66/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 4.6237e-04 - val_loss: 4.7942e-04\n",
      "Epoch 67/200\n",
      "12800/12800 [==============================] - 1s 52us/step - loss: 4.6220e-04 - val_loss: 4.7942e-04\n",
      "Epoch 68/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 4.6215e-04 - val_loss: 4.7919e-04\n",
      "Epoch 69/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 4.6199e-04 - val_loss: 4.7928e-04\n",
      "Epoch 70/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 4.6196e-04 - val_loss: 4.7894e-04\n",
      "Epoch 71/200\n",
      "12800/12800 [==============================] - 1s 48us/step - loss: 4.6177e-04 - val_loss: 4.7920e-04\n",
      "Epoch 72/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 4.6176e-04 - val_loss: 4.7891e-04\n",
      "Epoch 73/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 4.6168e-04 - val_loss: 4.7893e-04\n",
      "Epoch 74/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 4.6159e-04 - val_loss: 4.7870e-04\n",
      "Epoch 75/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 4.6149e-04 - val_loss: 4.7868e-04\n",
      "Epoch 76/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 4.6137e-04 - val_loss: 4.7853e-04\n",
      "Epoch 77/200\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 4.6130e-04 - val_loss: 4.7841e-04\n",
      "Epoch 78/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 4.6120e-04 - val_loss: 4.7855e-04\n",
      "Epoch 79/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 4.6117e-04 - val_loss: 4.7845e-04\n",
      "Epoch 80/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 4.6108e-04 - val_loss: 4.7829e-04\n",
      "Epoch 81/200\n",
      "12800/12800 [==============================] - 1s 48us/step - loss: 4.6103e-04 - val_loss: 4.7811e-04\n",
      "Epoch 82/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 4.6089e-04 - val_loss: 4.7811e-04\n",
      "Epoch 83/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 4.6082e-04 - val_loss: 4.7809e-04\n",
      "Epoch 84/200\n",
      "12800/12800 [==============================] - 1s 48us/step - loss: 4.6079e-04 - val_loss: 4.7803e-04\n",
      "Epoch 85/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 4.6072e-04 - val_loss: 4.7789e-04\n",
      "Epoch 86/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 4.6061e-04 - val_loss: 4.7781e-04\n",
      "Epoch 87/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 4.6057e-04 - val_loss: 4.7779e-04\n",
      "Epoch 88/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 4.6049e-04 - val_loss: 4.7803e-04\n",
      "Epoch 89/200\n",
      "12800/12800 [==============================] - 1s 53us/step - loss: 4.6048e-04 - val_loss: 4.7757e-04\n",
      "Epoch 90/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 4.6040e-04 - val_loss: 4.7758e-04\n",
      "Epoch 91/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 4.6031e-04 - val_loss: 4.7745e-04\n",
      "Epoch 92/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 4.6026e-04 - val_loss: 4.7768e-04\n",
      "Epoch 93/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 4.6022e-04 - val_loss: 4.7746e-04\n",
      "Epoch 94/200\n",
      "12800/12800 [==============================] - 1s 61us/step - loss: 4.6016e-04 - val_loss: 4.7737e-04\n",
      "Epoch 95/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 4.6013e-04 - val_loss: 4.7721e-04\n",
      "Epoch 96/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 4.6006e-04 - val_loss: 4.7728e-04\n",
      "Epoch 97/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 4.5999e-04 - val_loss: 4.7734e-04\n",
      "Epoch 98/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 4.5999e-04 - val_loss: 4.7711e-04\n",
      "Epoch 99/200\n",
      "12800/12800 [==============================] - 1s 48us/step - loss: 4.5989e-04 - val_loss: 4.7716e-04\n",
      "Epoch 100/200\n",
      "12800/12800 [==============================] - 1s 48us/step - loss: 4.5983e-04 - val_loss: 4.7712e-04\n",
      "Epoch 101/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 4.5977e-04 - val_loss: 4.7723e-04\n",
      "Epoch 102/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 4.5977e-04 - val_loss: 4.7699e-04\n",
      "Epoch 103/200\n",
      "12800/12800 [==============================] - 1s 64us/step - loss: 4.5968e-04 - val_loss: 4.7688e-04\n",
      "Epoch 104/200\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 4.5968e-04 - val_loss: 4.7699e-04\n",
      "Epoch 105/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 4.5962e-04 - val_loss: 4.7692e-04\n",
      "Epoch 106/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 4.5961e-04 - val_loss: 4.7685e-04\n",
      "Epoch 107/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 4.5957e-04 - val_loss: 4.7668e-04\n",
      "Epoch 108/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 4.5947e-04 - val_loss: 4.7690e-04\n",
      "Epoch 109/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 1s 63us/step - loss: 4.5947e-04 - val_loss: 4.7666e-04\n",
      "Epoch 110/200\n",
      "12800/12800 [==============================] - 1s 72us/step - loss: 4.5946e-04 - val_loss: 4.7651e-04\n",
      "Epoch 111/200\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 4.5939e-04 - val_loss: 4.7672e-04\n",
      "Epoch 112/200\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 4.5931e-04 - val_loss: 4.7666e-04\n",
      "Epoch 113/200\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 4.5931e-04 - val_loss: 4.7648e-04\n",
      "Epoch 114/200\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 4.5925e-04 - val_loss: 4.7670e-04\n",
      "Epoch 115/200\n",
      "12800/12800 [==============================] - 1s 61us/step - loss: 4.5924e-04 - val_loss: 4.7666e-04\n",
      "Epoch 116/200\n",
      "12800/12800 [==============================] - 1s 64us/step - loss: 4.5921e-04 - val_loss: 4.7653e-04\n",
      "Epoch 117/200\n",
      "12800/12800 [==============================] - 1s 69us/step - loss: 4.5918e-04 - val_loss: 4.7648e-04\n",
      "Epoch 118/200\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 4.5912e-04 - val_loss: 4.7647e-04\n",
      "Epoch 119/200\n",
      "12800/12800 [==============================] - 1s 64us/step - loss: 4.5912e-04 - val_loss: 4.7632e-04\n",
      "Epoch 120/200\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 4.5906e-04 - val_loss: 4.7628e-04\n",
      "Epoch 121/200\n",
      "12800/12800 [==============================] - 1s 104us/step - loss: 4.5904e-04 - val_loss: 4.7631e-04\n",
      "Epoch 122/200\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 4.5899e-04 - val_loss: 4.7626e-04\n",
      "Epoch 123/200\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 4.5897e-04 - val_loss: 4.7629e-04\n",
      "Epoch 124/200\n",
      "12800/12800 [==============================] - 1s 62us/step - loss: 4.5895e-04 - val_loss: 4.7614e-04\n",
      "Epoch 125/200\n",
      "12800/12800 [==============================] - 1s 64us/step - loss: 4.5892e-04 - val_loss: 4.7612e-04\n",
      "Epoch 126/200\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 4.5890e-04 - val_loss: 4.7621e-04\n",
      "Epoch 127/200\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 4.5885e-04 - val_loss: 4.7616e-04\n",
      "Epoch 128/200\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 4.5883e-04 - val_loss: 4.7604e-04\n",
      "Epoch 129/200\n",
      "12800/12800 [==============================] - 1s 57us/step - loss: 4.5879e-04 - val_loss: 4.7611e-04\n",
      "Epoch 130/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 4.5874e-04 - val_loss: 4.7604e-04\n",
      "Epoch 131/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 4.5874e-04 - val_loss: 4.7608e-04\n",
      "Epoch 132/200\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 4.5870e-04 - val_loss: 4.7604e-04\n",
      "Epoch 133/200\n",
      "12800/12800 [==============================] - 1s 56us/step - loss: 4.5870e-04 - val_loss: 4.7593e-04\n",
      "Epoch 134/200\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 4.5866e-04 - val_loss: 4.7592e-04\n",
      "Epoch 135/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 4.5862e-04 - val_loss: 4.7590e-04\n",
      "Epoch 136/200\n",
      "12800/12800 [==============================] - 1s 65us/step - loss: 4.5861e-04 - val_loss: 4.7592e-04\n",
      "Epoch 137/200\n",
      "12800/12800 [==============================] - 1s 69us/step - loss: 4.5858e-04 - val_loss: 4.7576e-04\n",
      "Epoch 138/200\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 4.5857e-04 - val_loss: 4.7578e-04\n",
      "Epoch 139/200\n",
      "12800/12800 [==============================] - 1s 57us/step - loss: 4.5853e-04 - val_loss: 4.7578e-04\n",
      "Epoch 140/200\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 4.5848e-04 - val_loss: 4.7590e-04\n",
      "Epoch 141/200\n",
      "12800/12800 [==============================] - 1s 65us/step - loss: 4.5845e-04 - val_loss: 4.7581e-04\n",
      "Epoch 142/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 4.5848e-04 - val_loss: 4.7569e-04\n",
      "Epoch 143/200\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 4.5845e-04 - val_loss: 4.7563e-04\n",
      "Epoch 144/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 4.5840e-04 - val_loss: 4.7580e-04\n",
      "Epoch 145/200\n",
      "12800/12800 [==============================] - 1s 48us/step - loss: 4.5837e-04 - val_loss: 4.7571e-04\n",
      "Epoch 146/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 4.5839e-04 - val_loss: 4.7566e-04\n",
      "Epoch 147/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 4.5834e-04 - val_loss: 4.7560e-04\n",
      "Epoch 148/200\n",
      "12800/12800 [==============================] - 1s 62us/step - loss: 4.5834e-04 - val_loss: 4.7556e-04\n",
      "Epoch 149/200\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 4.5830e-04 - val_loss: 4.7564e-04\n",
      "Epoch 150/200\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 4.5828e-04 - val_loss: 4.7554e-04\n",
      "Epoch 151/200\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 4.5827e-04 - val_loss: 4.7545e-04\n",
      "Epoch 152/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 4.5822e-04 - val_loss: 4.7556e-04\n",
      "Epoch 153/200\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 4.5824e-04 - val_loss: 4.7546e-04\n",
      "Epoch 154/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 4.5820e-04 - val_loss: 4.7553e-04\n",
      "Epoch 155/200\n",
      "12800/12800 [==============================] - 1s 61us/step - loss: 4.5817e-04 - val_loss: 4.7540e-04\n",
      "Epoch 156/200\n",
      "12800/12800 [==============================] - 1s 69us/step - loss: 4.5814e-04 - val_loss: 4.7541e-04\n",
      "Epoch 157/200\n",
      "12800/12800 [==============================] - 1s 61us/step - loss: 4.5813e-04 - val_loss: 4.7535e-04\n",
      "Epoch 158/200\n",
      "12800/12800 [==============================] - 1s 62us/step - loss: 4.5814e-04 - val_loss: 4.7542e-04\n",
      "Epoch 159/200\n",
      "12800/12800 [==============================] - 1s 63us/step - loss: 4.5810e-04 - val_loss: 4.7538e-04\n",
      "Epoch 160/200\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 4.5808e-04 - val_loss: 4.7549e-04\n",
      "Epoch 161/200\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 4.5807e-04 - val_loss: 4.7537e-04\n",
      "Epoch 162/200\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 4.5803e-04 - val_loss: 4.7545e-04\n",
      "Epoch 163/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 4.5802e-04 - val_loss: 4.7532e-04\n",
      "Epoch 164/200\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 4.5801e-04 - val_loss: 4.7522e-04\n",
      "Epoch 165/200\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 4.5797e-04 - val_loss: 4.7527e-04\n",
      "Epoch 166/200\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 4.5798e-04 - val_loss: 4.7531e-04\n",
      "Epoch 167/200\n",
      "12800/12800 [==============================] - 1s 57us/step - loss: 4.5797e-04 - val_loss: 4.7523e-04\n",
      "Epoch 168/200\n",
      "12800/12800 [==============================] - 1s 68us/step - loss: 4.5797e-04 - val_loss: 4.7534e-04\n",
      "Epoch 169/200\n",
      "12800/12800 [==============================] - 1s 62us/step - loss: 4.5792e-04 - val_loss: 4.7522e-04\n",
      "Epoch 170/200\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 4.5790e-04 - val_loss: 4.7516e-04\n",
      "Epoch 171/200\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 4.5789e-04 - val_loss: 4.7524e-04\n",
      "Epoch 172/200\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 4.5787e-04 - val_loss: 4.7520e-04\n",
      "Epoch 173/200\n",
      "12800/12800 [==============================] - 1s 63us/step - loss: 4.5787e-04 - val_loss: 4.7518e-04\n",
      "Epoch 174/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 4.5784e-04 - val_loss: 4.7516e-04\n",
      "Epoch 175/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 4.5782e-04 - val_loss: 4.7511e-04\n",
      "Epoch 176/200\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 4.5782e-04 - val_loss: 4.7522e-04\n",
      "Epoch 177/200\n",
      "12800/12800 [==============================] - 1s 53us/step - loss: 4.5780e-04 - val_loss: 4.7521e-04\n",
      "Epoch 178/200\n",
      "12800/12800 [==============================] - 1s 112us/step - loss: 4.5780e-04 - val_loss: 4.7514e-04\n",
      "Epoch 179/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 1s 103us/step - loss: 4.5775e-04 - val_loss: 4.7501e-04\n",
      "Epoch 180/200\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 4.5776e-04 - val_loss: 4.7499e-04\n",
      "Epoch 181/200\n",
      "12800/12800 [==============================] - 1s 68us/step - loss: 4.5777e-04 - val_loss: 4.7501e-04\n",
      "Epoch 182/200\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 4.5775e-04 - val_loss: 4.7508e-04\n",
      "Epoch 183/200\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 4.5775e-04 - val_loss: 4.7488e-04\n",
      "Epoch 184/200\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 4.5771e-04 - val_loss: 4.7500e-04\n",
      "Epoch 185/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 4.5771e-04 - val_loss: 4.7494e-04\n",
      "Epoch 186/200\n",
      "12800/12800 [==============================] - 1s 69us/step - loss: 4.5768e-04 - val_loss: 4.7502e-04\n",
      "Epoch 187/200\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 4.5769e-04 - val_loss: 4.7495e-04\n",
      "Epoch 188/200\n",
      "12800/12800 [==============================] - 1s 72us/step - loss: 4.5767e-04 - val_loss: 4.7501e-04\n",
      "Epoch 189/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 4.5764e-04 - val_loss: 4.7483e-04\n",
      "Epoch 190/200\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 4.5764e-04 - val_loss: 4.7507e-04\n",
      "Epoch 191/200\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 4.5761e-04 - val_loss: 4.7485e-04\n",
      "Epoch 192/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 4.5761e-04 - val_loss: 4.7493e-04\n",
      "Epoch 193/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 4.5762e-04 - val_loss: 4.7490e-04\n",
      "Epoch 194/200\n",
      "12800/12800 [==============================] - 1s 68us/step - loss: 4.5760e-04 - val_loss: 4.7479e-04\n",
      "Epoch 195/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 4.5757e-04 - val_loss: 4.7483e-04\n",
      "Epoch 196/200\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 4.5758e-04 - val_loss: 4.7483e-04\n",
      "Epoch 197/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 4.5755e-04 - val_loss: 4.7488e-04\n",
      "Epoch 198/200\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 4.5755e-04 - val_loss: 4.7470e-04\n",
      "Epoch 199/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 4.5753e-04 - val_loss: 4.7484e-04\n",
      "Epoch 200/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 4.5751e-04 - val_loss: 4.7490e-04\n",
      "Test MSE: 0.00046437225\n",
      "Total MSE: 0.00046135308\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input-layer (InputLayer)     (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "hidden-layer-1 (Dense)       (None, 66)                19866     \n",
      "_________________________________________________________________\n",
      "hidden-layer-2 (Dense)       (None, 33)                2211      \n",
      "_________________________________________________________________\n",
      "hidden-layer-3 (Dense)       (None, 66)                2244      \n",
      "_________________________________________________________________\n",
      "output-layer (Dense)         (None, 300)               20100     \n",
      "=================================================================\n",
      "Total params: 44,421\n",
      "Trainable params: 44,421\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/200\n",
      "12800/12800 [==============================] - 2s 148us/step - loss: 0.0022 - val_loss: 0.0015\n",
      "Epoch 2/200\n",
      "12800/12800 [==============================] - 1s 94us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 3/200\n",
      "12800/12800 [==============================] - 1s 103us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 4/200\n",
      "12800/12800 [==============================] - 1s 93us/step - loss: 9.8353e-04 - val_loss: 9.7973e-04\n",
      "Epoch 5/200\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 8.9106e-04 - val_loss: 8.9889e-04\n",
      "Epoch 6/200\n",
      "12800/12800 [==============================] - 1s 101us/step - loss: 8.2316e-04 - val_loss: 8.3883e-04\n",
      "Epoch 7/200\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 7.7167e-04 - val_loss: 7.9224e-04\n",
      "Epoch 8/200\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 7.3191e-04 - val_loss: 7.5596e-04\n",
      "Epoch 9/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 6.9979e-04 - val_loss: 7.2604e-04\n",
      "Epoch 10/200\n",
      "12800/12800 [==============================] - 1s 88us/step - loss: 6.7322e-04 - val_loss: 7.0043e-04\n",
      "Epoch 11/200\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 6.5084e-04 - val_loss: 6.7941e-04\n",
      "Epoch 12/200\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 6.3182e-04 - val_loss: 6.6105e-04\n",
      "Epoch 13/200\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 6.1562e-04 - val_loss: 6.4547e-04\n",
      "Epoch 14/200\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 6.0142e-04 - val_loss: 6.3118e-04\n",
      "Epoch 15/200\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 5.8903e-04 - val_loss: 6.1988e-04\n",
      "Epoch 16/200\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 5.7814e-04 - val_loss: 6.0903e-04\n",
      "Epoch 17/200\n",
      "12800/12800 [==============================] - 2s 134us/step - loss: 5.6860e-04 - val_loss: 5.9954e-04\n",
      "Epoch 18/200\n",
      "12800/12800 [==============================] - 2s 126us/step - loss: 5.5985e-04 - val_loss: 5.9057e-04\n",
      "Epoch 19/200\n",
      "12800/12800 [==============================] - 2s 119us/step - loss: 5.5205e-04 - val_loss: 5.8318e-04\n",
      "Epoch 20/200\n",
      "12800/12800 [==============================] - 1s 105us/step - loss: 5.4506e-04 - val_loss: 5.7612e-04\n",
      "Epoch 21/200\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 5.3842e-04 - val_loss: 5.7019e-04\n",
      "Epoch 22/200\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 5.3253e-04 - val_loss: 5.6357e-04\n",
      "Epoch 23/200\n",
      "12800/12800 [==============================] - 1s 69us/step - loss: 5.2703e-04 - val_loss: 5.5807e-04\n",
      "Epoch 24/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 5.2198e-04 - val_loss: 5.5323e-04\n",
      "Epoch 25/200\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 5.1730e-04 - val_loss: 5.4933e-04\n",
      "Epoch 26/200\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 5.1305e-04 - val_loss: 5.4520e-04\n",
      "Epoch 27/200\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 5.0914e-04 - val_loss: 5.4095e-04\n",
      "Epoch 28/200\n",
      "12800/12800 [==============================] - 1s 111us/step - loss: 5.0547e-04 - val_loss: 5.3792e-04\n",
      "Epoch 29/200\n",
      "12800/12800 [==============================] - 1s 92us/step - loss: 5.0219e-04 - val_loss: 5.3408e-04\n",
      "Epoch 30/200\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 4.9913e-04 - val_loss: 5.3194e-04\n",
      "Epoch 31/200\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 4.9634e-04 - val_loss: 5.2863e-04\n",
      "Epoch 32/200\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 4.9377e-04 - val_loss: 5.2620e-04\n",
      "Epoch 33/200\n",
      "12800/12800 [==============================] - 1s 61us/step - loss: 4.9144e-04 - val_loss: 5.2379e-04\n",
      "Epoch 34/200\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 4.8933e-04 - val_loss: 5.2226e-04\n",
      "Epoch 35/200\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 4.8733e-04 - val_loss: 5.2001e-04\n",
      "Epoch 36/200\n",
      "12800/12800 [==============================] - 1s 68us/step - loss: 4.8557e-04 - val_loss: 5.1851e-04\n",
      "Epoch 37/200\n",
      "12800/12800 [==============================] - 1s 61us/step - loss: 4.8397e-04 - val_loss: 5.1671e-04\n",
      "Epoch 38/200\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 4.8239e-04 - val_loss: 5.1514e-04\n",
      "Epoch 39/200\n",
      "12800/12800 [==============================] - 1s 61us/step - loss: 4.8099e-04 - val_loss: 5.1370e-04\n",
      "Epoch 40/200\n",
      "12800/12800 [==============================] - 1s 65us/step - loss: 4.7966e-04 - val_loss: 5.1250e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/200\n",
      "12800/12800 [==============================] - ETA: 0s - loss: 4.7826e-0 - 1s 68us/step - loss: 4.7848e-04 - val_loss: 5.1154e-04\n",
      "Epoch 42/200\n",
      "12800/12800 [==============================] - 1s 88us/step - loss: 4.7736e-04 - val_loss: 5.0987e-04\n",
      "Epoch 43/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 4.7641e-04 - val_loss: 5.0926e-04\n",
      "Epoch 44/200\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 4.7540e-04 - val_loss: 5.0810e-04\n",
      "Epoch 45/200\n",
      "12800/12800 [==============================] - 1s 61us/step - loss: 4.7452e-04 - val_loss: 5.0726e-04\n",
      "Epoch 46/200\n",
      "12800/12800 [==============================] - 1s 68us/step - loss: 4.7366e-04 - val_loss: 5.0655e-04\n",
      "Epoch 47/200\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 4.7289e-04 - val_loss: 5.0595e-04\n",
      "Epoch 48/200\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 4.7212e-04 - val_loss: 5.0454e-04\n",
      "Epoch 49/200\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 4.7141e-04 - val_loss: 5.0387e-04\n",
      "Epoch 50/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 4.7074e-04 - val_loss: 5.0345e-04\n",
      "Epoch 51/200\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 4.7017e-04 - val_loss: 5.0277e-04\n",
      "Epoch 52/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 4.6961e-04 - val_loss: 5.0234e-04\n",
      "Epoch 53/200\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 4.6897e-04 - val_loss: 5.0197e-04\n",
      "Epoch 54/200\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 4.6851e-04 - val_loss: 5.0127e-04\n",
      "Epoch 55/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 4.6804e-04 - val_loss: 5.0054e-04\n",
      "Epoch 56/200\n",
      "12800/12800 [==============================] - 1s 68us/step - loss: 4.6752e-04 - val_loss: 5.0032e-04\n",
      "Epoch 57/200\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 4.6707e-04 - val_loss: 4.9958e-04\n",
      "Epoch 58/200\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 4.6669e-04 - val_loss: 4.9949e-04\n",
      "Epoch 59/200\n",
      "12800/12800 [==============================] - 1s 69us/step - loss: 4.6624e-04 - val_loss: 4.9899e-04\n",
      "Epoch 60/200\n",
      "12800/12800 [==============================] - 1s 68us/step - loss: 4.6585e-04 - val_loss: 4.9879e-04\n",
      "Epoch 61/200\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 4.6550e-04 - val_loss: 4.9799e-04\n",
      "Epoch 62/200\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 4.6513e-04 - val_loss: 4.9788e-04 loss: 4.6544e-0\n",
      "Epoch 63/200\n",
      "12800/12800 [==============================] - 2s 118us/step - loss: 4.6480e-04 - val_loss: 4.9752e-04\n",
      "Epoch 64/200\n",
      "12800/12800 [==============================] - 2s 125us/step - loss: 4.6443e-04 - val_loss: 4.9733e-04\n",
      "Epoch 65/200\n",
      "12800/12800 [==============================] - 2s 139us/step - loss: 4.6411e-04 - val_loss: 4.9705e-04\n",
      "Epoch 66/200\n",
      "12800/12800 [==============================] - 2s 137us/step - loss: 4.6381e-04 - val_loss: 4.9693e-04\n",
      "Epoch 67/200\n",
      "12800/12800 [==============================] - 2s 133us/step - loss: 4.6356e-04 - val_loss: 4.9633e-04\n",
      "Epoch 68/200\n",
      "12800/12800 [==============================] - 1s 115us/step - loss: 4.6328e-04 - val_loss: 4.9585e-04\n",
      "Epoch 69/200\n",
      "12800/12800 [==============================] - 1s 111us/step - loss: 4.6297e-04 - val_loss: 4.9590e-04\n",
      "Epoch 70/200\n",
      "12800/12800 [==============================] - 1s 112us/step - loss: 4.6274e-04 - val_loss: 4.9539e-04\n",
      "Epoch 71/200\n",
      "12800/12800 [==============================] - 2s 133us/step - loss: 4.6249e-04 - val_loss: 4.9504e-04\n",
      "Epoch 72/200\n",
      "12800/12800 [==============================] - 2s 141us/step - loss: 4.6226e-04 - val_loss: 4.9487e-04\n",
      "Epoch 73/200\n",
      "12800/12800 [==============================] - 2s 126us/step - loss: 4.6200e-04 - val_loss: 4.9467e-04\n",
      "Epoch 74/200\n",
      "12800/12800 [==============================] - 2s 127us/step - loss: 4.6179e-04 - val_loss: 4.9463e-04\n",
      "Epoch 75/200\n",
      "12800/12800 [==============================] - 2s 122us/step - loss: 4.6156e-04 - val_loss: 4.9425e-04\n",
      "Epoch 76/200\n",
      "12800/12800 [==============================] - 1s 116us/step - loss: 4.6137e-04 - val_loss: 4.9386e-04\n",
      "Epoch 77/200\n",
      "12800/12800 [==============================] - 1s 113us/step - loss: 4.6115e-04 - val_loss: 4.9414e-04\n",
      "Epoch 78/200\n",
      "12800/12800 [==============================] - 2s 127us/step - loss: 4.6095e-04 - val_loss: 4.9366e-04\n",
      "Epoch 79/200\n",
      "12800/12800 [==============================] - 2s 170us/step - loss: 4.6080e-04 - val_loss: 4.9366e-04\n",
      "Epoch 80/200\n",
      "12800/12800 [==============================] - 1s 106us/step - loss: 4.6060e-04 - val_loss: 4.9348e-04\n",
      "Epoch 81/200\n",
      "12800/12800 [==============================] - 1s 114us/step - loss: 4.6045e-04 - val_loss: 4.9330e-04\n",
      "Epoch 82/200\n",
      "12800/12800 [==============================] - 2s 119us/step - loss: 4.6023e-04 - val_loss: 4.9303e-04\n",
      "Epoch 83/200\n",
      "12800/12800 [==============================] - 1s 88us/step - loss: 4.6014e-04 - val_loss: 4.9294e-04\n",
      "Epoch 84/200\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 4.5990e-04 - val_loss: 4.9287e-04\n",
      "Epoch 85/200\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 4.5977e-04 - val_loss: 4.9277e-04\n",
      "Epoch 86/200\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 4.5962e-04 - val_loss: 4.9270e-04\n",
      "Epoch 87/200\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 4.5948e-04 - val_loss: 4.9264e-04\n",
      "Epoch 88/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 4.5936e-04 - val_loss: 4.9226e-04\n",
      "Epoch 89/200\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 4.5923e-04 - val_loss: 4.9216e-04\n",
      "Epoch 90/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 4.5909e-04 - val_loss: 4.9205e-04\n",
      "Epoch 91/200\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 4.5895e-04 - val_loss: 4.9167e-04\n",
      "Epoch 92/200\n",
      "12800/12800 [==============================] - 1s 99us/step - loss: 4.5884e-04 - val_loss: 4.9179e-04\n",
      "Epoch 93/200\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 4.5872e-04 - val_loss: 4.9171e-04\n",
      "Epoch 94/200\n",
      "12800/12800 [==============================] - 1s 72us/step - loss: 4.5860e-04 - val_loss: 4.9174e-04\n",
      "Epoch 95/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 4.5855e-04 - val_loss: 4.9138e-04\n",
      "Epoch 96/200\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 4.5840e-04 - val_loss: 4.9130e-04\n",
      "Epoch 97/200\n",
      "12800/12800 [==============================] - 2s 145us/step - loss: 4.5830e-04 - val_loss: 4.9132e-04\n",
      "Epoch 98/200\n",
      "12800/12800 [==============================] - 2s 134us/step - loss: 4.5821e-04 - val_loss: 4.9123e-04\n",
      "Epoch 99/200\n",
      "12800/12800 [==============================] - 2s 126us/step - loss: 4.5811e-04 - val_loss: 4.9102e-04\n",
      "Epoch 100/200\n",
      "12800/12800 [==============================] - 1s 112us/step - loss: 4.5802e-04 - val_loss: 4.9123e-04\n",
      "Epoch 101/200\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 4.5789e-04 - val_loss: 4.9091e-04\n",
      "Epoch 102/200\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 4.5783e-04 - val_loss: 4.9059e-04\n",
      "Epoch 103/200\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 4.5770e-04 - val_loss: 4.9102e-04\n",
      "Epoch 104/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 4.5764e-04 - val_loss: 4.9101e-04\n",
      "Epoch 105/200\n",
      "12800/12800 [==============================] - 1s 72us/step - loss: 4.5761e-04 - val_loss: 4.9051e-04\n",
      "Epoch 106/200\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 4.5746e-04 - val_loss: 4.9049e-04\n",
      "Epoch 107/200\n",
      "12800/12800 [==============================] - 1s 111us/step - loss: 4.5741e-04 - val_loss: 4.9051e-04\n",
      "Epoch 108/200\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 4.5732e-04 - val_loss: 4.9028e-04\n",
      "Epoch 109/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 4.5726e-04 - val_loss: 4.9029e-04\n",
      "Epoch 110/200\n",
      "12800/12800 [==============================] - 1s 102us/step - loss: 4.5723e-04 - val_loss: 4.9024e-04\n",
      "Epoch 111/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 1s 87us/step - loss: 4.5709e-04 - val_loss: 4.9034e-04\n",
      "Epoch 112/200\n",
      "12800/12800 [==============================] - 1s 100us/step - loss: 4.5708e-04 - val_loss: 4.8999e-04\n",
      "Epoch 113/200\n",
      "12800/12800 [==============================] - 1s 114us/step - loss: 4.5701e-04 - val_loss: 4.9008e-04\n",
      "Epoch 114/200\n",
      "12800/12800 [==============================] - 1s 104us/step - loss: 4.5694e-04 - val_loss: 4.8990e-04\n",
      "Epoch 115/200\n",
      "12800/12800 [==============================] - 1s 112us/step - loss: 4.5685e-04 - val_loss: 4.9003e-04\n",
      "Epoch 116/200\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 4.5679e-04 - val_loss: 4.8968e-04\n",
      "Epoch 117/200\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 4.5678e-04 - val_loss: 4.8972e-04\n",
      "Epoch 118/200\n",
      "12800/12800 [==============================] - 1s 69us/step - loss: 4.5668e-04 - val_loss: 4.8972e-04\n",
      "Epoch 119/200\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 4.5662e-04 - val_loss: 4.8983e-04\n",
      "Epoch 120/200\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 4.5658e-04 - val_loss: 4.8954e-04\n",
      "Epoch 121/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 4.5649e-04 - val_loss: 4.8990e-04\n",
      "Epoch 122/200\n",
      "12800/12800 [==============================] - 1s 72us/step - loss: 4.5645e-04 - val_loss: 4.8979e-04\n",
      "Epoch 123/200\n",
      "12800/12800 [==============================] - 1s 69us/step - loss: 4.5643e-04 - val_loss: 4.8973e-04\n",
      "Epoch 124/200\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 4.5634e-04 - val_loss: 4.8945e-04\n",
      "Epoch 125/200\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 4.5630e-04 - val_loss: 4.8935e-04\n",
      "Epoch 126/200\n",
      "12800/12800 [==============================] - 1s 69us/step - loss: 4.5628e-04 - val_loss: 4.8920e-04\n",
      "Epoch 127/200\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 4.5620e-04 - val_loss: 4.8951e-04\n",
      "Epoch 128/200\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 4.5619e-04 - val_loss: 4.8905e-04\n",
      "Epoch 129/200\n",
      "12800/12800 [==============================] - 1s 72us/step - loss: 4.5610e-04 - val_loss: 4.8927e-04\n",
      "Epoch 130/200\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 4.5605e-04 - val_loss: 4.8918e-04\n",
      "Epoch 131/200\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 4.5606e-04 - val_loss: 4.8938e-04\n",
      "Epoch 132/200\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 4.5601e-04 - val_loss: 4.8922e-04\n",
      "Epoch 133/200\n",
      "12800/12800 [==============================] - 1s 56us/step - loss: 4.5595e-04 - val_loss: 4.8928e-04\n",
      "Epoch 134/200\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 4.5594e-04 - val_loss: 4.8900e-04\n",
      "Epoch 135/200\n",
      "12800/12800 [==============================] - 1s 59us/step - loss: 4.5589e-04 - val_loss: 4.8875e-04\n",
      "Epoch 136/200\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 4.5590e-04 - val_loss: 4.8887e-04\n",
      "Epoch 137/200\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 4.5580e-04 - val_loss: 4.8895e-04\n",
      "Epoch 138/200\n",
      "12800/12800 [==============================] - 1s 62us/step - loss: 4.5578e-04 - val_loss: 4.8893e-04\n",
      "Epoch 139/200\n",
      "12800/12800 [==============================] - 1s 56us/step - loss: 4.5575e-04 - val_loss: 4.8890e-04\n",
      "Epoch 140/200\n",
      "12800/12800 [==============================] - 1s 64us/step - loss: 4.5568e-04 - val_loss: 4.8883e-04\n",
      "Epoch 141/200\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 4.5568e-04 - val_loss: 4.8876e-04\n",
      "Test MSE: 0.00047356497\n",
      "Total MSE: 0.000464026\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 30)                9030      \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 300)               9300      \n",
      "=================================================================\n",
      "Total params: 18,330\n",
      "Trainable params: 18,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/200\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 0.0020 - val_loss: 0.0014\n",
      "Epoch 2/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 3/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 0.0010 - val_loss: 9.6477e-04\n",
      "Epoch 4/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 9.1780e-04 - val_loss: 8.6775e-04\n",
      "Epoch 5/200\n",
      "12800/12800 [==============================] - 1s 44us/step - loss: 8.3660e-04 - val_loss: 8.0064e-04\n",
      "Epoch 6/200\n",
      "12800/12800 [==============================] - 1s 48us/step - loss: 7.7801e-04 - val_loss: 7.4984e-04\n",
      "Epoch 7/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 7.3377e-04 - val_loss: 7.1105e-04\n",
      "Epoch 8/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 6.9905e-04 - val_loss: 6.8072e-04\n",
      "Epoch 9/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 6.7179e-04 - val_loss: 6.5689e-04\n",
      "Epoch 10/200\n",
      "12800/12800 [==============================] - 1s 44us/step - loss: 6.4998e-04 - val_loss: 6.3807e-04\n",
      "Epoch 11/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 6.3248e-04 - val_loss: 6.2223e-04\n",
      "Epoch 12/200\n",
      "12800/12800 [==============================] - 1s 44us/step - loss: 6.1838e-04 - val_loss: 6.0980e-04\n",
      "Epoch 13/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 6.0685e-04 - val_loss: 5.9973e-04\n",
      "Epoch 14/200\n",
      "12800/12800 [==============================] - 1s 44us/step - loss: 5.9738e-04 - val_loss: 5.9057e-04\n",
      "Epoch 15/200\n",
      "12800/12800 [==============================] - 1s 43us/step - loss: 5.8949e-04 - val_loss: 5.8428e-04\n",
      "Epoch 16/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 5.8309e-04 - val_loss: 5.7754e-04\n",
      "Epoch 17/200\n",
      "12800/12800 [==============================] - 1s 43us/step - loss: 5.7748e-04 - val_loss: 5.7270e-04\n",
      "Epoch 18/200\n",
      "12800/12800 [==============================] - 1s 44us/step - loss: 5.7282e-04 - val_loss: 5.6802e-04\n",
      "Epoch 19/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 5.6874e-04 - val_loss: 5.6480e-04\n",
      "Epoch 20/200\n",
      "12800/12800 [==============================] - 1s 44us/step - loss: 5.6523e-04 - val_loss: 5.6121e-04\n",
      "Epoch 21/200\n",
      "12800/12800 [==============================] - 1s 58us/step - loss: 5.6225e-04 - val_loss: 5.5828e-04\n",
      "Epoch 22/200\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 5.5956e-04 - val_loss: 5.5574e-04\n",
      "Epoch 23/200\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 5.5725e-04 - val_loss: 5.5396e-04\n",
      "Epoch 24/200\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 5.5514e-04 - val_loss: 5.5164e-04\n",
      "Epoch 25/200\n",
      "12800/12800 [==============================] - 1s 69us/step - loss: 5.5341e-04 - val_loss: 5.4964e-04\n",
      "Epoch 26/200\n",
      "12800/12800 [==============================] - 1s 57us/step - loss: 5.5184e-04 - val_loss: 5.4817e-04\n",
      "Epoch 27/200\n",
      "12800/12800 [==============================] - 1s 53us/step - loss: 5.5035e-04 - val_loss: 5.4673e-04\n",
      "Epoch 28/200\n",
      "12800/12800 [==============================] - 1s 52us/step - loss: 5.4904e-04 - val_loss: 5.4552e-04\n",
      "Epoch 29/200\n",
      "12800/12800 [==============================] - 1s 61us/step - loss: 5.4784e-04 - val_loss: 5.4457e-04\n",
      "Epoch 30/200\n",
      "12800/12800 [==============================] - 1s 62us/step - loss: 5.4674e-04 - val_loss: 5.4354e-04\n",
      "Epoch 31/200\n",
      "12800/12800 [==============================] - 1s 61us/step - loss: 5.4581e-04 - val_loss: 5.4256e-04\n",
      "Epoch 32/200\n",
      "12800/12800 [==============================] - 1s 64us/step - loss: 5.4497e-04 - val_loss: 5.4180e-04\n",
      "Epoch 33/200\n",
      "12800/12800 [==============================] - 1s 63us/step - loss: 5.4411e-04 - val_loss: 5.4091e-04\n",
      "Epoch 34/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 1s 65us/step - loss: 5.4337e-04 - val_loss: 5.4019e-04\n",
      "Epoch 35/200\n",
      "12800/12800 [==============================] - 1s 69us/step - loss: 5.4276e-04 - val_loss: 5.3944e-04\n",
      "Epoch 36/200\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 5.4208e-04 - val_loss: 5.3929e-04\n",
      "Epoch 37/200\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 5.4159e-04 - val_loss: 5.3851e-04\n",
      "Epoch 38/200\n",
      "12800/12800 [==============================] - 1s 57us/step - loss: 5.4098e-04 - val_loss: 5.3801e-04\n",
      "Epoch 39/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 5.4049e-04 - val_loss: 5.3758e-04\n",
      "Epoch 40/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 5.4002e-04 - val_loss: 5.3688e-04\n",
      "Epoch 41/200\n",
      "12800/12800 [==============================] - 1s 54us/step - loss: 5.3957e-04 - val_loss: 5.3687e-04\n",
      "Epoch 42/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 5.3921e-04 - val_loss: 5.3631e-04\n",
      "Epoch 43/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 5.3886e-04 - val_loss: 5.3593e-04\n",
      "Epoch 44/200\n",
      "12800/12800 [==============================] - 1s 56us/step - loss: 5.3848e-04 - val_loss: 5.3576e-04\n",
      "Epoch 45/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 5.3822e-04 - val_loss: 5.3527e-04\n",
      "Epoch 46/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 5.3784e-04 - val_loss: 5.3511e-04\n",
      "Epoch 47/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 5.3755e-04 - val_loss: 5.3479e-04\n",
      "Epoch 48/200\n",
      "12800/12800 [==============================] - 1s 44us/step - loss: 5.3731e-04 - val_loss: 5.3454e-04\n",
      "Epoch 49/200\n",
      "12800/12800 [==============================] - 1s 44us/step - loss: 5.3700e-04 - val_loss: 5.3477e-04\n",
      "Epoch 50/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 5.3683e-04 - val_loss: 5.3387e-04\n",
      "Epoch 51/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 5.3658e-04 - val_loss: 5.3375e-04\n",
      "Epoch 52/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 5.3634e-04 - val_loss: 5.3353e-04\n",
      "Epoch 53/200\n",
      "12800/12800 [==============================] - 1s 52us/step - loss: 5.3616e-04 - val_loss: 5.3353e-04\n",
      "Epoch 54/200\n",
      "12800/12800 [==============================] - 1s 52us/step - loss: 5.3599e-04 - val_loss: 5.3333e-04\n",
      "Epoch 55/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 5.3576e-04 - val_loss: 5.3311e-04\n",
      "Epoch 56/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 5.3551e-04 - val_loss: 5.3282e-04\n",
      "Epoch 57/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 5.3542e-04 - val_loss: 5.3258e-04\n",
      "Epoch 58/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 5.3523e-04 - val_loss: 5.3283e-04\n",
      "Epoch 59/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 5.3510e-04 - val_loss: 5.3230e-04\n",
      "Epoch 60/200\n",
      "12800/12800 [==============================] - 1s 53us/step - loss: 5.3495e-04 - val_loss: 5.3203e-04\n",
      "Epoch 61/200\n",
      "12800/12800 [==============================] - 1s 44us/step - loss: 5.3476e-04 - val_loss: 5.3196e-04\n",
      "Epoch 62/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 5.3468e-04 - val_loss: 5.3203e-04\n",
      "Epoch 63/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 5.3453e-04 - val_loss: 5.3191e-04\n",
      "Epoch 64/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 5.3435e-04 - val_loss: 5.3177e-04\n",
      "Epoch 65/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 5.3423e-04 - val_loss: 5.3153e-04\n",
      "Epoch 66/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 5.3413e-04 - val_loss: 5.3154e-04\n",
      "Epoch 67/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 5.3403e-04 - val_loss: 5.3164e-04\n",
      "Epoch 68/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 5.3390e-04 - val_loss: 5.3136e-04\n",
      "Epoch 69/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 5.3382e-04 - val_loss: 5.3120e-04\n",
      "Epoch 70/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 5.3375e-04 - val_loss: 5.3103e-04\n",
      "Epoch 71/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 5.3363e-04 - val_loss: 5.3091e-04\n",
      "Epoch 72/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 5.3350e-04 - val_loss: 5.3078e-04\n",
      "Epoch 73/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 5.3342e-04 - val_loss: 5.3087e-04\n",
      "Epoch 74/200\n",
      "12800/12800 [==============================] - 1s 44us/step - loss: 5.3336e-04 - val_loss: 5.3065e-04\n",
      "Epoch 75/200\n",
      "12800/12800 [==============================] - 1s 44us/step - loss: 5.3324e-04 - val_loss: 5.3065e-04\n",
      "Epoch 76/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 5.3320e-04 - val_loss: 5.3051e-04\n",
      "Epoch 77/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 5.3309e-04 - val_loss: 5.3049e-04\n",
      "Epoch 78/200\n",
      "12800/12800 [==============================] - 1s 44us/step - loss: 5.3293e-04 - val_loss: 5.3034e-04\n",
      "Epoch 79/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 5.3292e-04 - val_loss: 5.3037e-04\n",
      "Epoch 80/200\n",
      "12800/12800 [==============================] - 1s 44us/step - loss: 5.3282e-04 - val_loss: 5.3032e-04\n",
      "Epoch 81/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 5.3276e-04 - val_loss: 5.3024e-04\n",
      "Epoch 82/200\n",
      "12800/12800 [==============================] - 1s 44us/step - loss: 5.3269e-04 - val_loss: 5.3018e-04\n",
      "Epoch 83/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 5.3261e-04 - val_loss: 5.3018e-04\n",
      "Epoch 84/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 5.3258e-04 - val_loss: 5.3001e-04\n",
      "Epoch 85/200\n",
      "12800/12800 [==============================] - 1s 44us/step - loss: 5.3246e-04 - val_loss: 5.3010e-04\n",
      "Epoch 86/200\n",
      "12800/12800 [==============================] - 1s 43us/step - loss: 5.3240e-04 - val_loss: 5.2990e-04\n",
      "Epoch 87/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 5.3231e-04 - val_loss: 5.3005e-04\n",
      "Epoch 88/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 5.3229e-04 - val_loss: 5.2978e-04\n",
      "Epoch 89/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 5.3225e-04 - val_loss: 5.2976e-04\n",
      "Epoch 90/200\n",
      "12800/12800 [==============================] - 1s 93us/step - loss: 5.3223e-04 - val_loss: 5.2945e-04\n",
      "Epoch 91/200\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 5.3213e-04 - val_loss: 5.2976e-04\n",
      "Epoch 92/200\n",
      "12800/12800 [==============================] - 1s 57us/step - loss: 5.3205e-04 - val_loss: 5.2957e-04\n",
      "Epoch 93/200\n",
      "12800/12800 [==============================] - 1s 60us/step - loss: 5.3201e-04 - val_loss: 5.2936e-04\n",
      "Epoch 94/200\n",
      "12800/12800 [==============================] - 1s 56us/step - loss: 5.3195e-04 - val_loss: 5.2978e-04\n",
      "Epoch 95/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 5.3191e-04 - val_loss: 5.2944e-04\n",
      "Epoch 96/200\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 5.3186e-04 - val_loss: 5.2969e-04\n",
      "Epoch 97/200\n",
      "12800/12800 [==============================] - 1s 50us/step - loss: 5.3184e-04 - val_loss: 5.2940e-04\n",
      "Epoch 98/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 5.3177e-04 - val_loss: 5.2932e-04\n",
      "Epoch 99/200\n",
      "12800/12800 [==============================] - 1s 53us/step - loss: 5.3173e-04 - val_loss: 5.2917e-04\n",
      "Epoch 100/200\n",
      "12800/12800 [==============================] - 1s 45us/step - loss: 5.3166e-04 - val_loss: 5.2948e-04\n",
      "Epoch 101/200\n",
      "12800/12800 [==============================] - 1s 47us/step - loss: 5.3163e-04 - val_loss: 5.2929e-04\n",
      "Epoch 102/200\n",
      "12800/12800 [==============================] - 1s 46us/step - loss: 5.3156e-04 - val_loss: 5.2905e-04\n",
      "Epoch 103/200\n",
      "12800/12800 [==============================] - 1s 49us/step - loss: 5.3155e-04 - val_loss: 5.2926e-04\n",
      "Epoch 104/200\n",
      "12800/12800 [==============================] - 1s 53us/step - loss: 5.3150e-04 - val_loss: 5.2903e-04\n",
      "Epoch 105/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 1s 53us/step - loss: 5.3141e-04 - val_loss: 5.2917e-04\n",
      "Epoch 106/200\n",
      "12800/12800 [==============================] - 1s 51us/step - loss: 5.3139e-04 - val_loss: 5.2886e-04\n",
      "Epoch 107/200\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 5.3134e-04 - val_loss: 5.2877e-04\n",
      "Epoch 108/200\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 5.3133e-04 - val_loss: 5.2863e-04\n",
      "Epoch 109/200\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 5.3127e-04 - val_loss: 5.2887e-04\n",
      "Epoch 110/200\n",
      "12800/12800 [==============================] - 1s 54us/step - loss: 5.3124e-04 - val_loss: 5.2864e-04\n",
      "Epoch 111/200\n",
      "12800/12800 [==============================] - 1s 62us/step - loss: 5.3118e-04 - val_loss: 5.2880e-04\n",
      "Epoch 112/200\n",
      "12800/12800 [==============================] - 1s 69us/step - loss: 5.3120e-04 - val_loss: 5.2856e-04\n",
      "Epoch 113/200\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 5.3111e-04 - val_loss: 5.2856e-04\n",
      "Epoch 114/200\n",
      "12800/12800 [==============================] - 1s 53us/step - loss: 5.3110e-04 - val_loss: 5.2868e-04\n",
      "Epoch 115/200\n",
      "12800/12800 [==============================] - 1s 48us/step - loss: 5.3106e-04 - val_loss: 5.2886e-04\n",
      "Epoch 116/200\n",
      "12800/12800 [==============================] - 1s 55us/step - loss: 5.3105e-04 - val_loss: 5.2863e-04\n",
      "Epoch 117/200\n",
      "12800/12800 [==============================] - 1s 110us/step - loss: 5.3099e-04 - val_loss: 5.2847e-04\n",
      "Epoch 118/200\n",
      "12800/12800 [==============================] - 1s 108us/step - loss: 5.3095e-04 - val_loss: 5.2848e-04\n",
      "Epoch 119/200\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 5.3093e-04 - val_loss: 5.2864e-04\n",
      "Epoch 120/200\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 5.3089e-04 - val_loss: 5.2853e-04\n",
      "Epoch 121/200\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 5.3086e-04 - val_loss: 5.2848e-04\n",
      "Epoch 122/200\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 5.3081e-04 - val_loss: 5.2862e-04\n",
      "Epoch 123/200\n",
      "12800/12800 [==============================] - 1s 64us/step - loss: 5.3081e-04 - val_loss: 5.2833e-04\n",
      "Epoch 124/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 5.3076e-04 - val_loss: 5.2844e-04\n",
      "Epoch 125/200\n",
      "12800/12800 [==============================] - 1s 65us/step - loss: 5.3073e-04 - val_loss: 5.2848e-04\n",
      "Epoch 126/200\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 5.3068e-04 - val_loss: 5.2850e-04\n",
      "Epoch 127/200\n",
      "12800/12800 [==============================] - 1s 97us/step - loss: 5.3069e-04 - val_loss: 5.2819e-04\n",
      "Epoch 128/200\n",
      "12800/12800 [==============================] - 2s 134us/step - loss: 5.3064e-04 - val_loss: 5.2822e-04\n",
      "Epoch 129/200\n",
      "12800/12800 [==============================] - 1s 99us/step - loss: 5.3062e-04 - val_loss: 5.2828e-04\n",
      "Epoch 130/200\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 5.3057e-04 - val_loss: 5.2828e-04\n",
      "Epoch 131/200\n",
      "12800/12800 [==============================] - 1s 72us/step - loss: 5.3055e-04 - val_loss: 5.2802e-04\n",
      "Epoch 132/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 5.3054e-04 - val_loss: 5.2811e-04\n",
      "Epoch 133/200\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 5.3050e-04 - val_loss: 5.2819e-04\n",
      "Epoch 134/200\n",
      "12800/12800 [==============================] - 2s 132us/step - loss: 5.3046e-04 - val_loss: 5.2821e-04\n",
      "Epoch 135/200\n",
      "12800/12800 [==============================] - 1s 103us/step - loss: 5.3048e-04 - val_loss: 5.2812e-04\n",
      "Epoch 136/200\n",
      "12800/12800 [==============================] - 1s 93us/step - loss: 5.3043e-04 - val_loss: 5.2805e-04\n",
      "Epoch 137/200\n",
      "12800/12800 [==============================] - 1s 108us/step - loss: 5.3039e-04 - val_loss: 5.2796e-04\n",
      "Epoch 138/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 5.3036e-04 - val_loss: 5.2796e-04\n",
      "Epoch 139/200\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 5.3033e-04 - val_loss: 5.2791e-04\n",
      "Epoch 140/200\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 5.3033e-04 - val_loss: 5.2813e-04\n",
      "Epoch 141/200\n",
      "12800/12800 [==============================] - 1s 69us/step - loss: 5.3028e-04 - val_loss: 5.2806e-04\n",
      "Epoch 142/200\n",
      "12800/12800 [==============================] - 1s 62us/step - loss: 5.3029e-04 - val_loss: 5.2785e-04\n",
      "Epoch 143/200\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 5.3028e-04 - val_loss: 5.2793e-04\n",
      "Epoch 144/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 5.3019e-04 - val_loss: 5.2783e-04\n",
      "Epoch 145/200\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 5.3023e-04 - val_loss: 5.2783e-04\n",
      "Epoch 146/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 5.3018e-04 - val_loss: 5.2788e-04\n",
      "Epoch 147/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 5.3012e-04 - val_loss: 5.2776e-04\n",
      "Epoch 148/200\n",
      "12800/12800 [==============================] - 1s 65us/step - loss: 5.3012e-04 - val_loss: 5.2772e-04\n",
      "Epoch 149/200\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 5.3011e-04 - val_loss: 5.2783e-04\n",
      "Epoch 150/200\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 5.3008e-04 - val_loss: 5.2777e-04\n",
      "Epoch 151/200\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 5.3008e-04 - val_loss: 5.2779e-04\n",
      "Epoch 152/200\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 5.3005e-04 - val_loss: 5.2776e-04\n",
      "Epoch 153/200\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 5.3000e-04 - val_loss: 5.2765e-04\n",
      "Epoch 154/200\n",
      "12800/12800 [==============================] - 1s 68us/step - loss: 5.3001e-04 - val_loss: 5.2776e-04\n",
      "Epoch 155/200\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 5.2997e-04 - val_loss: 5.2767e-04\n",
      "Epoch 156/200\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 5.2996e-04 - val_loss: 5.2754e-04\n",
      "Epoch 157/200\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 5.2995e-04 - val_loss: 5.2767e-04\n",
      "Epoch 158/200\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 5.2992e-04 - val_loss: 5.2761e-04\n",
      "Epoch 159/200\n",
      "12800/12800 [==============================] - 1s 72us/step - loss: 5.2987e-04 - val_loss: 5.2773e-04\n",
      "Epoch 160/200\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 5.2991e-04 - val_loss: 5.2767e-04\n",
      "Epoch 161/200\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 5.2986e-04 - val_loss: 5.2768e-04\n",
      "Epoch 162/200\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 5.2983e-04 - val_loss: 5.2765e-04\n",
      "Test MSE: 0.00052085687\n",
      "Total MSE: 0.00052730165\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input-layer (InputLayer)     (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "hidden-layer-1 (Dense)       (None, 60)                18060     \n",
      "_________________________________________________________________\n",
      "hidden-layer-2 (Dense)       (None, 30)                1830      \n",
      "_________________________________________________________________\n",
      "hidden-layer-3 (Dense)       (None, 60)                1860      \n",
      "_________________________________________________________________\n",
      "output-layer (Dense)         (None, 300)               18300     \n",
      "=================================================================\n",
      "Total params: 40,050\n",
      "Trainable params: 40,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/200\n",
      "12800/12800 [==============================] - 2s 146us/step - loss: 0.0021 - val_loss: 0.0014\n",
      "Epoch 2/200\n",
      "12800/12800 [==============================] - 1s 94us/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 3/200\n",
      "12800/12800 [==============================] - 1s 100us/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 4/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 1s 82us/step - loss: 9.3728e-04 - val_loss: 9.0690e-04\n",
      "Epoch 5/200\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 8.5454e-04 - val_loss: 8.3680e-04\n",
      "Epoch 6/200\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 7.9478e-04 - val_loss: 7.8388e-04\n",
      "Epoch 7/200\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 7.4961e-04 - val_loss: 7.4403e-04\n",
      "Epoch 8/200\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 7.1454e-04 - val_loss: 7.1231e-04\n",
      "Epoch 9/200\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 6.8662e-04 - val_loss: 6.8811e-04\n",
      "Epoch 10/200\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 6.6414e-04 - val_loss: 6.6638e-04\n",
      "Epoch 11/200\n",
      "12800/12800 [==============================] - 1s 72us/step - loss: 6.4576e-04 - val_loss: 6.5055e-04\n",
      "Epoch 12/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 6.3069e-04 - val_loss: 6.3686e-04\n",
      "Epoch 13/200\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 6.1810e-04 - val_loss: 6.2638e-04\n",
      "Epoch 14/200\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 6.0773e-04 - val_loss: 6.1631e-04\n",
      "Epoch 15/200\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 5.9892e-04 - val_loss: 6.0839e-04\n",
      "Epoch 16/200\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 5.9143e-04 - val_loss: 6.0187e-04\n",
      "Epoch 17/200\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 5.8489e-04 - val_loss: 5.9562e-04\n",
      "Epoch 18/200\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 5.7940e-04 - val_loss: 5.9071e-04\n",
      "Epoch 19/200\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 5.7454e-04 - val_loss: 5.8625e-04\n",
      "Epoch 20/200\n",
      "12800/12800 [==============================] - 1s 88us/step - loss: 5.7053e-04 - val_loss: 5.8284e-04\n",
      "Epoch 21/200\n",
      "12800/12800 [==============================] - 1s 88us/step - loss: 5.6688e-04 - val_loss: 5.7901e-04\n",
      "Epoch 22/200\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 5.6366e-04 - val_loss: 5.7677e-04\n",
      "Epoch 23/200\n",
      "12800/12800 [==============================] - 1s 80us/step - loss: 5.6090e-04 - val_loss: 5.7381e-04\n",
      "Epoch 24/200\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 5.5847e-04 - val_loss: 5.7174e-04\n",
      "Epoch 25/200\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 5.5629e-04 - val_loss: 5.6968e-04\n",
      "Epoch 26/200\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 5.5444e-04 - val_loss: 5.6747e-04\n",
      "Epoch 27/200\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 5.5274e-04 - val_loss: 5.6656e-04\n",
      "Epoch 28/200\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 5.5114e-04 - val_loss: 5.6495e-04\n",
      "Epoch 29/200\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 5.4978e-04 - val_loss: 5.6391e-04\n",
      "Epoch 30/200\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 5.4854e-04 - val_loss: 5.6327e-04\n",
      "Epoch 31/200\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 5.4760e-04 - val_loss: 5.6164e-04\n",
      "Epoch 32/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 5.4649e-04 - val_loss: 5.6101e-04\n",
      "Epoch 33/200\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 5.4558e-04 - val_loss: 5.6019e-04\n",
      "Epoch 34/200\n",
      "12800/12800 [==============================] - 1s 69us/step - loss: 5.4467e-04 - val_loss: 5.5940e-04\n",
      "Epoch 35/200\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 5.4396e-04 - val_loss: 5.5912e-04\n",
      "Epoch 36/200\n",
      "12800/12800 [==============================] - 1s 112us/step - loss: 5.4322e-04 - val_loss: 5.5846e-04\n",
      "Epoch 37/200\n",
      "12800/12800 [==============================] - 1s 108us/step - loss: 5.4253e-04 - val_loss: 5.5758e-04\n",
      "Epoch 38/200\n",
      "12800/12800 [==============================] - 1s 97us/step - loss: 5.4195e-04 - val_loss: 5.5723e-04\n",
      "Epoch 39/200\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 5.4138e-04 - val_loss: 5.5683e-04\n",
      "Epoch 40/200\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 5.4086e-04 - val_loss: 5.5634e-04\n",
      "Epoch 41/200\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 5.4036e-04 - val_loss: 5.5530e-04\n",
      "Epoch 42/200\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 5.3981e-04 - val_loss: 5.5548e-04\n",
      "Epoch 43/200\n",
      "12800/12800 [==============================] - 2s 119us/step - loss: 5.3943e-04 - val_loss: 5.5467e-04\n",
      "Epoch 44/200\n",
      "12800/12800 [==============================] - 1s 116us/step - loss: 5.3906e-04 - val_loss: 5.5418e-04\n",
      "Epoch 45/200\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 5.3857e-04 - val_loss: 5.5438e-04\n",
      "Epoch 46/200\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 5.3829e-04 - val_loss: 5.5370e-04\n",
      "Epoch 47/200\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 5.3791e-04 - val_loss: 5.5366e-04\n",
      "Epoch 48/200\n",
      "12800/12800 [==============================] - 2s 145us/step - loss: 5.3755e-04 - val_loss: 5.5373e-04\n",
      "Epoch 49/200\n",
      "12800/12800 [==============================] - 2s 140us/step - loss: 5.3726e-04 - val_loss: 5.5319e-04\n",
      "Epoch 50/200\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 5.3693e-04 - val_loss: 5.5332e-04\n",
      "Epoch 51/200\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 5.3666e-04 - val_loss: 5.5283e-04\n",
      "Epoch 52/200\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 5.3641e-04 - val_loss: 5.5230e-04\n",
      "Epoch 53/200\n",
      "12800/12800 [==============================] - 1s 66us/step - loss: 5.3616e-04 - val_loss: 5.5171e-04\n",
      "Epoch 54/200\n",
      "12800/12800 [==============================] - 1s 110us/step - loss: 5.3582e-04 - val_loss: 5.5169e-04\n",
      "Epoch 55/200\n",
      "12800/12800 [==============================] - 1s 95us/step - loss: 5.3562e-04 - val_loss: 5.5159e-04\n",
      "Epoch 56/200\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 5.3538e-04 - val_loss: 5.5119e-04\n",
      "Epoch 57/200\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 5.3517e-04 - val_loss: 5.5110e-04\n",
      "Epoch 58/200\n",
      "12800/12800 [==============================] - 1s 107us/step - loss: 5.3494e-04 - val_loss: 5.5125e-04\n",
      "Epoch 59/200\n",
      "12800/12800 [==============================] - 1s 99us/step - loss: 5.3477e-04 - val_loss: 5.5057e-04\n",
      "Epoch 60/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 5.3454e-04 - val_loss: 5.5066e-04\n",
      "Epoch 61/200\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 5.3431e-04 - val_loss: 5.5052e-04\n",
      "Epoch 62/200\n",
      "12800/12800 [==============================] - 1s 97us/step - loss: 5.3413e-04 - val_loss: 5.5081e-04\n",
      "Epoch 63/200\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 5.3400e-04 - val_loss: 5.5014e-04\n",
      "Epoch 64/200\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 5.3375e-04 - val_loss: 5.5011e-04\n",
      "Epoch 65/200\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 5.3363e-04 - val_loss: 5.5021e-04\n",
      "Epoch 66/200\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 5.3345e-04 - val_loss: 5.4981e-04\n",
      "Epoch 67/200\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 5.3329e-04 - val_loss: 5.4951e-04\n",
      "Epoch 68/200\n",
      "12800/12800 [==============================] - 1s 63us/step - loss: 5.3316e-04 - val_loss: 5.4954e-04\n",
      "Epoch 69/200\n",
      "12800/12800 [==============================] - 1s 68us/step - loss: 5.3299e-04 - val_loss: 5.4957e-04\n",
      "Epoch 70/200\n",
      "12800/12800 [==============================] - ETA: 0s - loss: 5.3362e-0 - 1s 85us/step - loss: 5.3285e-04 - val_loss: 5.4967e-04\n",
      "Epoch 71/200\n",
      "12800/12800 [==============================] - 1s 101us/step - loss: 5.3272e-04 - val_loss: 5.4904e-04\n",
      "Epoch 72/200\n",
      "12800/12800 [==============================] - 1s 68us/step - loss: 5.3255e-04 - val_loss: 5.4921e-04\n",
      "Epoch 73/200\n",
      "12800/12800 [==============================] - 1s 63us/step - loss: 5.3243e-04 - val_loss: 5.4882e-04\n",
      "Epoch 74/200\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 5.3225e-04 - val_loss: 5.4917e-04\n",
      "Epoch 75/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 1s 93us/step - loss: 5.3211e-04 - val_loss: 5.4880e-04\n",
      "Epoch 76/200\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 5.3204e-04 - val_loss: 5.4862e-04\n",
      "Epoch 77/200\n",
      "12800/12800 [==============================] - 1s 68us/step - loss: 5.3191e-04 - val_loss: 5.4854e-04\n",
      "Epoch 78/200\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 5.3178e-04 - val_loss: 5.4868e-04\n",
      "Epoch 79/200\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 5.3171e-04 - val_loss: 5.4830e-04\n",
      "Epoch 80/200\n",
      "12800/12800 [==============================] - 1s 92us/step - loss: 5.3153e-04 - val_loss: 5.4845e-04\n",
      "Epoch 81/200\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 5.3143e-04 - val_loss: 5.4822e-04\n",
      "Epoch 82/200\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 5.3135e-04 - val_loss: 5.4791e-04\n",
      "Epoch 83/200\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 5.3117e-04 - val_loss: 5.4814e-04\n",
      "Epoch 84/200\n",
      "12800/12800 [==============================] - 1s 72us/step - loss: 5.3111e-04 - val_loss: 5.4785e-04\n",
      "Epoch 85/200\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 5.3098e-04 - val_loss: 5.4787e-04\n",
      "Epoch 86/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 5.3088e-04 - val_loss: 5.4789e-04\n",
      "Epoch 87/200\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 5.3078e-04 - val_loss: 5.4754e-04\n",
      "Epoch 88/200\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 5.3063e-04 - val_loss: 5.4787e-04\n",
      "Epoch 89/200\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 5.3060e-04 - val_loss: 5.4760e-04\n",
      "Epoch 90/200\n",
      "12800/12800 [==============================] - 2s 126us/step - loss: 5.3046e-04 - val_loss: 5.4738e-04\n",
      "Epoch 91/200\n",
      "12800/12800 [==============================] - 1s 102us/step - loss: 5.3039e-04 - val_loss: 5.4742e-04\n",
      "Epoch 92/200\n",
      "12800/12800 [==============================] - 2s 124us/step - loss: 5.3030e-04 - val_loss: 5.4725e-04\n",
      "Epoch 93/200\n",
      "12800/12800 [==============================] - 1s 107us/step - loss: 5.3013e-04 - val_loss: 5.4744e-04\n",
      "Epoch 94/200\n",
      "12800/12800 [==============================] - 1s 99us/step - loss: 5.3008e-04 - val_loss: 5.4706e-04\n",
      "Epoch 95/200\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 5.2999e-04 - val_loss: 5.4694e-04\n",
      "Epoch 96/200\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 5.2982e-04 - val_loss: 5.4720e-04\n",
      "Epoch 97/200\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 5.2981e-04 - val_loss: 5.4672e-04\n",
      "Epoch 98/200\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 5.2974e-04 - val_loss: 5.4682e-04\n",
      "Epoch 99/200\n",
      "12800/12800 [==============================] - 1s 88us/step - loss: 5.2962e-04 - val_loss: 5.4669e-04\n",
      "Epoch 100/200\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 5.2954e-04 - val_loss: 5.4657e-04\n",
      "Epoch 101/200\n",
      "12800/12800 [==============================] - 1s 99us/step - loss: 5.2944e-04 - val_loss: 5.4651e-04\n",
      "Epoch 102/200\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 5.2940e-04 - val_loss: 5.4651e-04\n",
      "Epoch 103/200\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 5.2925e-04 - val_loss: 5.4639e-04\n",
      "Epoch 104/200\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 5.2919e-04 - val_loss: 5.4663e-04\n",
      "Epoch 105/200\n",
      "12800/12800 [==============================] - 1s 72us/step - loss: 5.2912e-04 - val_loss: 5.4655e-04\n",
      "Epoch 106/200\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 5.2901e-04 - val_loss: 5.4648e-04\n",
      "Epoch 107/200\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 5.2900e-04 - val_loss: 5.4615e-04\n",
      "Epoch 108/200\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 5.2891e-04 - val_loss: 5.4628e-04\n",
      "Epoch 109/200\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 5.2882e-04 - val_loss: 5.4602e-04\n",
      "Epoch 110/200\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 5.2870e-04 - val_loss: 5.4586e-04\n",
      "Epoch 111/200\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 5.2861e-04 - val_loss: 5.4601e-04\n",
      "Epoch 112/200\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 5.2857e-04 - val_loss: 5.4593e-04\n",
      "Epoch 113/200\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 5.2853e-04 - val_loss: 5.4574e-04\n",
      "Epoch 114/200\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 5.2839e-04 - val_loss: 5.4594e-04\n",
      "Epoch 115/200\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 5.2832e-04 - val_loss: 5.4590e-04\n",
      "Epoch 116/200\n",
      "12800/12800 [==============================] - 1s 76us/step - loss: 5.2827e-04 - val_loss: 5.4567e-04\n",
      "Epoch 117/200\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 5.2818e-04 - val_loss: 5.4562e-04\n",
      "Epoch 118/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 5.2810e-04 - val_loss: 5.4552e-04\n",
      "Epoch 119/200\n",
      "12800/12800 [==============================] - 1s 72us/step - loss: 5.2805e-04 - val_loss: 5.4555e-04\n",
      "Epoch 120/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 5.2800e-04 - val_loss: 5.4548e-04\n",
      "Epoch 121/200\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 5.2793e-04 - val_loss: 5.4533e-04\n",
      "Epoch 122/200\n",
      "12800/12800 [==============================] - 1s 75us/step - loss: 5.2783e-04 - val_loss: 5.4533e-04\n",
      "Epoch 123/200\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 5.2778e-04 - val_loss: 5.4509e-04\n",
      "Epoch 124/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 5.2773e-04 - val_loss: 5.4485e-04\n",
      "Epoch 125/200\n",
      "12800/12800 [==============================] - 1s 72us/step - loss: 5.2757e-04 - val_loss: 5.4525e-04\n",
      "Epoch 126/200\n",
      "12800/12800 [==============================] - 1s 72us/step - loss: 5.2756e-04 - val_loss: 5.4488e-04\n",
      "Epoch 127/200\n",
      "12800/12800 [==============================] - 1s 73us/step - loss: 5.2752e-04 - val_loss: 5.4501e-04\n",
      "Epoch 128/200\n",
      "12800/12800 [==============================] - 1s 72us/step - loss: 5.2746e-04 - val_loss: 5.4505e-04\n",
      "Epoch 129/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 5.2737e-04 - val_loss: 5.4488e-04\n",
      "Epoch 130/200\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 5.2733e-04 - val_loss: 5.4478e-04\n",
      "Epoch 131/200\n",
      "12800/12800 [==============================] - 1s 77us/step - loss: 5.2724e-04 - val_loss: 5.4504e-04\n",
      "Epoch 132/200\n",
      "12800/12800 [==============================] - 1s 74us/step - loss: 5.2721e-04 - val_loss: 5.4473e-04\n",
      "Epoch 133/200\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 5.2714e-04 - val_loss: 5.4455e-04\n",
      "Epoch 134/200\n",
      "12800/12800 [==============================] - 1s 99us/step - loss: 5.2706e-04 - val_loss: 5.4458e-04\n",
      "Epoch 135/200\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 5.2704e-04 - val_loss: 5.4460e-04\n",
      "Epoch 136/200\n",
      "12800/12800 [==============================] - 1s 96us/step - loss: 5.2690e-04 - val_loss: 5.4447e-04\n",
      "Epoch 137/200\n",
      "12800/12800 [==============================] - 1s 92us/step - loss: 5.2691e-04 - val_loss: 5.4439e-04\n",
      "Epoch 138/200\n",
      "12800/12800 [==============================] - 1s 94us/step - loss: 5.2683e-04 - val_loss: 5.4449e-04\n",
      "Epoch 139/200\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 5.2682e-04 - val_loss: 5.4422e-04\n",
      "Epoch 140/200\n",
      "12800/12800 [==============================] - 1s 95us/step - loss: 5.2675e-04 - val_loss: 5.4434e-04\n",
      "Epoch 141/200\n",
      "12800/12800 [==============================] - 1s 94us/step - loss: 5.2670e-04 - val_loss: 5.4416e-04\n",
      "Epoch 142/200\n",
      "12800/12800 [==============================] - 1s 95us/step - loss: 5.2663e-04 - val_loss: 5.4412e-04\n",
      "Epoch 143/200\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 5.2657e-04 - val_loss: 5.4399e-04\n",
      "Epoch 144/200\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 5.2650e-04 - val_loss: 5.4401e-04\n",
      "Epoch 145/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 1s 79us/step - loss: 5.2648e-04 - val_loss: 5.4400e-04\n",
      "Epoch 146/200\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 5.2643e-04 - val_loss: 5.4392e-04\n",
      "Epoch 147/200\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 5.2636e-04 - val_loss: 5.4394e-04\n",
      "Epoch 148/200\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 5.2629e-04 - val_loss: 5.4384e-04\n",
      "Epoch 149/200\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 5.2628e-04 - val_loss: 5.4385e-04\n",
      "Epoch 150/200\n",
      "12800/12800 [==============================] - 1s 68us/step - loss: 5.2625e-04 - val_loss: 5.4376e-04\n",
      "Epoch 151/200\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 5.2621e-04 - val_loss: 5.4379e-04\n",
      "Epoch 152/200\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 5.2614e-04 - val_loss: 5.4354e-04\n",
      "Epoch 153/200\n",
      "12800/12800 [==============================] - 1s 69us/step - loss: 5.2611e-04 - val_loss: 5.4366e-04\n",
      "Epoch 154/200\n",
      "12800/12800 [==============================] - 1s 69us/step - loss: 5.2610e-04 - val_loss: 5.4350e-04\n",
      "Epoch 155/200\n",
      "12800/12800 [==============================] - 1s 72us/step - loss: 5.2602e-04 - val_loss: 5.4340e-04\n",
      "Epoch 156/200\n",
      "12800/12800 [==============================] - 1s 69us/step - loss: 5.2595e-04 - val_loss: 5.4347e-04\n",
      "Epoch 157/200\n",
      "12800/12800 [==============================] - 1s 67us/step - loss: 5.2590e-04 - val_loss: 5.4352e-04\n",
      "Epoch 158/200\n",
      "12800/12800 [==============================] - 1s 68us/step - loss: 5.2585e-04 - val_loss: 5.4341e-04\n",
      "Epoch 159/200\n",
      "12800/12800 [==============================] - 1s 68us/step - loss: 5.2584e-04 - val_loss: 5.4335e-04\n",
      "Epoch 160/200\n",
      "12800/12800 [==============================] - 1s 70us/step - loss: 5.2580e-04 - val_loss: 5.4314e-04\n",
      "Epoch 161/200\n",
      "12800/12800 [==============================] - 1s 68us/step - loss: 5.2576e-04 - val_loss: 5.4327e-04\n",
      "Epoch 162/200\n",
      "12800/12800 [==============================] - 1s 68us/step - loss: 5.2569e-04 - val_loss: 5.4329e-04\n",
      "Epoch 163/200\n",
      "12800/12800 [==============================] - 1s 90us/step - loss: 5.2570e-04 - val_loss: 5.4304e-04\n",
      "Epoch 164/200\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 5.2562e-04 - val_loss: 5.4321e-04\n",
      "Epoch 165/200\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 5.2558e-04 - val_loss: 5.4303e-04\n",
      "Epoch 166/200\n",
      "12800/12800 [==============================] - 1s 89us/step - loss: 5.2556e-04 - val_loss: 5.4295e-04\n",
      "Epoch 167/200\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 5.2556e-04 - val_loss: 5.4288e-04\n",
      "Epoch 168/200\n",
      "12800/12800 [==============================] - 1s 71us/step - loss: 5.2547e-04 - val_loss: 5.4281e-04\n",
      "Epoch 169/200\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 5.2545e-04 - val_loss: 5.4292e-04\n",
      "Epoch 170/200\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 5.2543e-04 - val_loss: 5.4291e-04\n",
      "Epoch 171/200\n",
      "12800/12800 [==============================] - 1s 79us/step - loss: 5.2537e-04 - val_loss: 5.4304e-04\n",
      "Epoch 172/200\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 5.2535e-04 - val_loss: 5.4288e-04\n",
      "Epoch 173/200\n",
      "12800/12800 [==============================] - 1s 78us/step - loss: 5.2533e-04 - val_loss: 5.4271e-04\n",
      "Epoch 174/200\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 5.2529e-04 - val_loss: 5.4269e-04\n",
      "Epoch 175/200\n",
      "12800/12800 [==============================] - 1s 94us/step - loss: 5.2524e-04 - val_loss: 5.4258e-04\n",
      "Epoch 176/200\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 5.2521e-04 - val_loss: 5.4255e-04\n",
      "Epoch 177/200\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 5.2515e-04 - val_loss: 5.4260e-04\n",
      "Epoch 178/200\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 5.2518e-04 - val_loss: 5.4260e-04\n",
      "Epoch 179/200\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 5.2513e-04 - val_loss: 5.4268e-04\n",
      "Epoch 180/200\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 5.2509e-04 - val_loss: 5.4247e-04\n",
      "Epoch 181/200\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 5.2504e-04 - val_loss: 5.4249e-04\n",
      "Epoch 182/200\n",
      "12800/12800 [==============================] - 1s 81us/step - loss: 5.2506e-04 - val_loss: 5.4269e-04\n",
      "Epoch 183/200\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 5.2503e-04 - val_loss: 5.4251e-04\n",
      "Epoch 184/200\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 5.2500e-04 - val_loss: 5.4243e-04\n",
      "Epoch 185/200\n",
      "12800/12800 [==============================] - 1s 91us/step - loss: 5.2495e-04 - val_loss: 5.4243e-04\n",
      "Epoch 186/200\n",
      "12800/12800 [==============================] - 1s 87us/step - loss: 5.2492e-04 - val_loss: 5.4231e-04\n",
      "Epoch 187/200\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 5.2489e-04 - val_loss: 5.4227e-04\n",
      "Epoch 188/200\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 5.2486e-04 - val_loss: 5.4240e-04\n",
      "Epoch 189/200\n",
      "12800/12800 [==============================] - 1s 83us/step - loss: 5.2483e-04 - val_loss: 5.4235e-04\n",
      "Epoch 190/200\n",
      "12800/12800 [==============================] - 1s 82us/step - loss: 5.2480e-04 - val_loss: 5.4224e-04\n",
      "Epoch 191/200\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 5.2480e-04 - val_loss: 5.4210e-04\n",
      "Epoch 192/200\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 5.2477e-04 - val_loss: 5.4226e-04\n",
      "Epoch 193/200\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 5.2478e-04 - val_loss: 5.4211e-04\n",
      "Epoch 194/200\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 5.2470e-04 - val_loss: 5.4208e-04\n",
      "Epoch 195/200\n",
      "12800/12800 [==============================] - 1s 99us/step - loss: 5.2468e-04 - val_loss: 5.4216e-04\n",
      "Epoch 196/200\n",
      "12800/12800 [==============================] - 1s 84us/step - loss: 5.2470e-04 - val_loss: 5.4203e-04\n",
      "Epoch 197/200\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 5.2465e-04 - val_loss: 5.4195e-04\n",
      "Epoch 198/200\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 5.2463e-04 - val_loss: 5.4200e-04\n",
      "Epoch 199/200\n",
      "12800/12800 [==============================] - 1s 85us/step - loss: 5.2461e-04 - val_loss: 5.4216e-04\n",
      "Epoch 200/200\n",
      "12800/12800 [==============================] - 1s 86us/step - loss: 5.2459e-04 - val_loss: 5.4191e-04\n",
      "Test MSE: 0.00052013126\n",
      "Total MSE: 0.00052589775\n"
     ]
    }
   ],
   "source": [
    "z_epochs = {}\n",
    "z_epochs_deep = {}\n",
    "for z in range(2, 11):\n",
    "    z_epochs[z] = process_image('a.jpg', z, 5e-10)\n",
    "    z_epochs[z] = process_image_deeply('a.jpg', z, 5e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x468 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y1 = [8.1040552e-06, 2.5589075e-05, 6.6116405e-05, 0.00012816954, 0.00020491519, 0.00030546259, 0.00038618364, 0.00046135308, 0.00052730165, ]\n",
    "y2 = [6.7852651e-05, 4.0898936e-05, 7.0802955e-05, 0.000129652, 0.0002066324, 0.00030601932, 0.00038610698,  0.000464026, 0.00052589775]\n",
    "\n",
    "plt.rcParams['font.size'] = 15\n",
    "plt.rcParams['font.weight'] = 100\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(15, 6.5)\n",
    "\n",
    "ax.set(xlabel='Comression rate z', ylabel='Error e')\n",
    "ax.plot(z_epochs.keys(), y1, '-o', label='Simple autoencoder', linewidth=2, markersize=8)\n",
    "ax.plot(z_epochs.keys(), y2, '-^', label='Deep autoencoder', linewidth=2, markersize=10)\n",
    "\n",
    "\n",
    "ax.grid()\n",
    "ax.legend(loc='upper center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.958291200135136e-05, 4.026132251965464e-05, 6.863429949589772e-05, 0.00013227012619609012, 0.00021593072116957046, 0.0003149186486552935, 0.0003856623097090051, 0.0004887630377197639, 0.0005419116604025476]\n"
     ]
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
